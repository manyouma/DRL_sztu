{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72791f90",
   "metadata": {},
   "source": [
    "## 🧊 Lab 1: FrozenLake Intro\n",
    "\n",
    "In this lab, we will explore **FrozenLake**, a classic reinforcement learning environment provided by [Gymnasium](https://gymnasium.farama.org/environments/toy_text/frozen_lake/).  \n",
    "FrozenLake is a simple **grid world** where an agent must navigate from the **start tile (S)** to the **goal tile (G)** without falling into any **holes (H)**.  \n",
    "Each step moves the agent **up, down, left, or right**, but the surface can be slippery — meaning your chosen action may not always lead to the intended direction.\n",
    "\n",
    "- **State Space:** Each grid cell is a discrete state (4×4 = 16 states by default).  \n",
    "- **Action Space:** 4 actions – `LEFT (0)`, `DOWN (1)`, `RIGHT (2)`, `UP (3)`.  \n",
    "- **Reward Function:** +1 for reaching the goal, 0 otherwise.  \n",
    "- **Episode Termination:** Episode ends when the agent falls into a hole or reaches the goal.\n",
    "\n",
    "FrozenLake is a great starting point for RL experiments because it is:\n",
    "- **Simple to visualize**, helping you understand the interaction loop (`reset → step → render`).\n",
    "- **Small and discrete**, perfect for testing value iteration, policy iteration, and basic RL algorithms.\n",
    "- **Customizable**, letting you adjust map size and stochasticity (`is_slippery=True/False`).\n",
    "\n",
    "Today, we will run a **random agent** to get familiar with the API and visualize how the environment behaves before we move on to smarter policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f388c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5cfc7f",
   "metadata": {},
   "source": [
    "### 🔧 Setting Up FrozenLake (4×4)\n",
    "\n",
    "Let's create the **4×4 FrozenLake environment** in deterministic mode (`is_slippery=False`) so we can step through it without random slips.\n",
    "\n",
    "We'll:\n",
    "1. Initialize the environment.\n",
    "2. Inspect the **state space** (number of discrete states).\n",
    "3. Inspect the **action space** (number of available actions).\n",
    "4. Print the **reward map** to see where the goal and holes are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create FrozenLake environment (deterministic so it's easy to follow)\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"ansi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b2852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Show state and action space\n",
    "print(f\"State space: {env.observation_space}  -> {env.observation_space.n} states\")\n",
    "print(f\"Action space: {env.action_space}  -> {env.action_space.n} actions (0=LEFT, 1=DOWN, 2=RIGHT, 3=UP)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a61d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Render initial state\n",
    "obs, info = env.reset(seed=42)\n",
    "print(\"\\nInitial Grid:\")\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b18452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Show reward map using env.unwrapped.P\n",
    "reward_map = np.zeros(env.observation_space.n)\n",
    "P = env.unwrapped.P  # <-- Access underlying FrozenLakeEnv\n",
    "\n",
    "for state, transitions in P.items():\n",
    "    for action, outcomes in transitions.items():\n",
    "        for prob, next_state, reward, done in outcomes:\n",
    "            reward_map[next_state] = max(reward_map[next_state], reward)\n",
    "\n",
    "print(\"Reward map (reshaped to 4x4):\")\n",
    "print(reward_map.reshape(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285c529",
   "metadata": {},
   "source": [
    "### 🔑 Core Gym API: `reset()` and `step()`\n",
    "\n",
    "Every Gym environment follows the same basic pattern:  \n",
    "you **reset** to start an episode, then **step** through the environment until it ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d22c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "obs, info = env.reset(seed=0)\n",
    "print(obs, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b234ad0e",
   "metadata": {},
   "source": [
    "- **`obs`** → the initial state (an integer for FrozenLake, `0–15` for a 4×4 grid).\n",
    "- **`info`** → extra diagnostic info (not used much here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf7475",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "print(action, obs, reward, terminated, truncated, info  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0589e30",
   "metadata": {},
   "source": [
    "#### 🟡 `env.step(action)`\n",
    "\n",
    "| Variable       | Meaning                                                                 |\n",
    "|---------------|-------------------------------------------------------------------------|\n",
    "| **`action`**      | Integer `0–3`: the action you chose (see table below).                  |\n",
    "| **`obs`**         | The **next state** (an integer from `0` to `15` for a 4×4 grid).        |\n",
    "| **`reward`**      | Scalar reward for this step (`+1` at goal, `0` elsewhere).              |\n",
    "| **`terminated`**  | `True` if the episode ended because you reached a **goal** or fell in a **hole**. |\n",
    "| **`truncated`**   | `True` if the episode ended because you hit a **time limit** (rare in FrozenLake). |\n",
    "| **`info`**        | Extra information (rarely needed for FrozenLake).                       |\n",
    "\n",
    "#### 🧭 Action Mapping (for FrozenLake)\n",
    "\n",
    "| Action Number | Direction | Symbol |\n",
    "|--------------|-----------|--------|\n",
    "| `0` | **LEFT**  | ⬅️ |\n",
    "| `1` | **DOWN**  | ⬇️ |\n",
    "| `2` | **RIGHT** | ➡️ |\n",
    "| `3` | **UP**    | ⬆️ |\n",
    "\n",
    "---\n",
    "\n",
    "You repeat `env.step(action)` until **`terminated or truncated`** becomes `True`,  \n",
    "then call **`env.reset()`** to start a new episode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581c0941",
   "metadata": {},
   "source": [
    "### 🏃 Exercise: Implement `run_random_trajectory`\n",
    "\n",
    "Write a function called **`run_random_trajectory`** that:\n",
    "\n",
    "1. **Resets the environment** to get the initial state.\n",
    "2. Prints the **initial grid** using `env.render()`.\n",
    "3. Loops for up to `max_steps`:\n",
    "   - Samples a **random action** with `env.action_space.sample()`.\n",
    "   - Calls `env.step(action)` and unpacks the result into  \n",
    "     `obs, reward, terminated, truncated, info`.\n",
    "   - Prints the **step number**, chosen action, reward, and the grid.\n",
    "   - **Breaks the loop** if `terminated` or `truncated` is `True`.\n",
    "4. At the end, prints the **total reward** collected in this episode.\n",
    "\n",
    "When you finish, run your function for one or more trajectories with different seeds:\n",
    "\n",
    "```python\n",
    "# Example usage (after you implement the function)\n",
    "for i in range(3):\n",
    "    print(f\"=== Trajectory {i+1} ===\")\n",
    "    run_random_trajectory(env, max_steps=20, seed=100 + i)\n",
    "```\n",
    "\n",
    "#### 💡 Tips\n",
    "- Remember to call `env.reset(seed=...)` at the start of your function.\n",
    "- Use a loop like `for step in range(max_steps):`.\n",
    "- Use f-strings to format the output clearly (e.g., `print(f\"Step {step+1}: Action={action} -> Reward={reward}\")`).\n",
    "- Stop the loop early when `terminated or truncated` is `True`.\n",
    "\n",
    "> **Goal:** You should see the agent move step by step and eventually fall into a hole or reach the goal. Your printout should look similar to the instructor’s solution but doesn’t have to match exactly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94979eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn to work on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f95fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
