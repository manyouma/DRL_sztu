{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839c9bab",
   "metadata": {},
   "source": [
    "### ðŸ§Š Lab 4 â€” Value Iteration and Policy Iteration for Frozen Lake\n",
    "In this lab we will continue our exploration of Markov Decision Processes (MDPs) \n",
    "using the simplified **FrozenLake** environment. Building on Lab 3, we will \n",
    "practice two key ideas:\n",
    "\n",
    "1. **Finding the Optimal Policy (Value Iteration)**  We apply value iteration to compute the optimal state values and extract the corresponding optimal policy that maximizes long-term return.\n",
    "\n",
    "2. **Finding the Optimal Policy (Policy Iteration)** We apply policy iteration, which alternates between evaluating the current policy and improving it until convergence to the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b7dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "import random\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3214c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFF\n",
      "FHF\n",
      "FFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a smaller 3x3 map\n",
    "DESC_3x3 = [\n",
    "    \"SFF\",\n",
    "    \"FHF\",\n",
    "    \"FFG\",\n",
    "]\n",
    "env = gym.make(\"FrozenLake-v1\",desc=DESC_3x3,is_slippery=True, render_mode=\"ansi\")\n",
    "obs, info = env.reset(seed=42)\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "324c6a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General transition builder for FrozenLake-style maps (works for any rectangular map).\n",
    "# Compatible with Gymnasium's action order: LEFT=0, DOWN=1, RIGHT=2, UP=3\n",
    "\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "ACTIONS = [LEFT, DOWN, RIGHT, UP]\n",
    "DIRS = {\n",
    "    LEFT:  (0, -1),\n",
    "    DOWN:  (1, 0),\n",
    "    RIGHT: (0, 1),\n",
    "    UP:    (-1, 0),\n",
    "}\n",
    "\n",
    "def _grid_to_idx(r: int, c: int, ncols: int) -> int:\n",
    "    return r * ncols + c\n",
    "\n",
    "def _idx_to_grid(s: int, ncols: int) -> Tuple[int, int]:\n",
    "    return divmod(s, ncols)\n",
    "\n",
    "def _clip_move(r: int, c: int, dr: int, dc: int, nrows: int, ncols: int) -> Tuple[int, int]:\n",
    "    rr, cc = r + dr, c + dc\n",
    "    rr = min(max(rr, 0), nrows - 1)\n",
    "    cc = min(max(cc, 0), ncols - 1)\n",
    "    return rr, cc\n",
    "\n",
    "def build_frozenlake_transitions(desc: List[str], is_slippery: bool = True):\n",
    "    \"\"\"\n",
    "    Build transition probabilities for a FrozenLake-like grid.\n",
    "\n",
    "    Args:\n",
    "        desc: list of strings (rows), made of {'S','F','H','G'}.\n",
    "        is_slippery: if True -> stochastic: {left, forward, right} each with prob 1/3.\n",
    "                     if False -> deterministic in the intended direction.\n",
    "\n",
    "    Returns:\n",
    "        P: np.ndarray (S, A, S), transition probabilities\n",
    "        R: np.ndarray (S, A, S), rewards (1 on entering 'G', else 0)\n",
    "        absorbing: np.ndarray (S,), True for 'H' or 'G' (absorbing/self-loop)\n",
    "        shape_2d: (nrows, ncols)\n",
    "        flatten_map: np.ndarray (S,), identity map (r,c)->s ordering\n",
    "    \"\"\"\n",
    "    nrows = len(desc)\n",
    "    ncols = len(desc[0])\n",
    "    S = nrows * ncols\n",
    "    A = 4\n",
    "\n",
    "    grid = np.array([list(row) for row in desc])\n",
    "    is_hole = (grid == 'H')\n",
    "    is_goal = (grid == 'G')\n",
    "    absorbing = (is_hole | is_goal).reshape(-1)\n",
    "\n",
    "    P = np.zeros((S, A, S), dtype=float)\n",
    "    R = np.zeros((S, A, S), dtype=float)\n",
    "\n",
    "    def step_from_state(s: int, a: int) -> int:\n",
    "        r, c = _idx_to_grid(s, ncols)\n",
    "        dr, dc = DIRS[a]\n",
    "        rr, cc = _clip_move(r, c, dr, dc, nrows, ncols)\n",
    "        return _grid_to_idx(rr, cc, ncols)\n",
    "\n",
    "    for s in range(S):\n",
    "        if absorbing[s]:\n",
    "            # Absorbing states self-loop for all actions\n",
    "            for a in ACTIONS:\n",
    "                P[s, a, s] = 1.0\n",
    "            continue\n",
    "\n",
    "        for a in ACTIONS:\n",
    "            if is_slippery:\n",
    "                left = (a - 1) % 4\n",
    "                right = (a + 1) % 4\n",
    "                for aa in [left, a, right]:\n",
    "                    s2 = step_from_state(s, aa)\n",
    "                    P[s, a, s2] += 1.0/3.0\n",
    "            else:\n",
    "                s2 = step_from_state(s, a)\n",
    "                P[s, a, s2] = 1.0\n",
    "\n",
    "            # Reward for ARRIVING at goal\n",
    "            for s2 in range(S):\n",
    "                if P[s, a, s2] > 0:\n",
    "                    rr, cc = _idx_to_grid(s2, ncols)\n",
    "                    if grid[rr, cc] == 'G':\n",
    "                        R[s, a, s2] = 1.0\n",
    "\n",
    "    flatten_map = np.arange(S, dtype=int)\n",
    "    return P, R, absorbing, (nrows, ncols), flatten_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5402244",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R, absorbing, shape2d, flatmap = build_frozenlake_transitions(DESC_3x3, is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "232f50ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid shape: (3, 3)   States: 9   Actions: 4\n",
      "Absorbing states (H/G):\n",
      "[[False False False]\n",
      " [False  True False]\n",
      " [False False  True]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Grid shape:\", shape2d, \"  States:\", P.shape[0], \"  Actions:\", P.shape[1])\n",
    "print(\"Absorbing states (H/G):\")\n",
    "print(absorbing.reshape(shape2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92cf4e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-action SxS matrices\n",
    "T_per_action = [P[:, a, :] for a in range(4)]  # list of 4 matrices, each (S,S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "320f91af",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_SIZE = 3\n",
    "map_loc = np.zeros((MAP_SIZE, MAP_SIZE))\n",
    "\n",
    "# First construct the matrix for moving left\n",
    "for i_x in np.arange(MAP_SIZE):\n",
    "    for i_y in np.arange(MAP_SIZE):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9dc459a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_LEFT = [\n",
    "    [2, 1, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "    [1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 0, 3, 1, 0, 1, 0],\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 1, 0, 0, 2, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
    "]\n",
    "\n",
    "P_DOWN = [\n",
    "    [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 1, 3, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0, 2, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 1, 2],\n",
    "]\n",
    "\n",
    "P_RIGHT = [\n",
    "    [1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 2, 0, 0, 1, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 3, 0, 0, 1, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
    "    [0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 1, 2],\n",
    "]\n",
    "\n",
    "P_UP = [\n",
    "    [2, 1, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 2, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 1, 3, 1, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
    "]\n",
    "P_all = np.array([P_LEFT, P_DOWN, P_RIGHT, P_UP]).transpose(0, 2, 1)/3\n",
    "POLICY = [\n",
    "    1,  # state 0 (top-left)\n",
    "    2,  # state 1\n",
    "    1,  # state 2\n",
    "    1,  # state 3\n",
    "    1,  # state 4 (center, possibly hole)\n",
    "    1,  # state 5\n",
    "    2,  # state 6\n",
    "    2,  # state 7\n",
    "    2,  # state 8 (goal state)\n",
    "]\n",
    "\n",
    "n_states = len(POLICY)\n",
    "P_pi = np.zeros((n_states, n_states))\n",
    "for s in range(n_states):\n",
    "    a = POLICY[s]             # action chosen at state s\n",
    "    P_pi[s, :] = P_all[a, s]  # copy the probabilities for that action\n",
    "\n",
    "Reward = [\n",
    "0,  # state 0 (top-left)\n",
    "0,  # state 1\n",
    "0,  # state 2\n",
    "0,  # state 3\n",
    "0,  # state 4 (center, possibly hole)\n",
    "0,  # state 5\n",
    "0,  # state 6\n",
    "0,  # state 7\n",
    "1,  # state 8 (goal state)\n",
    "]\n",
    "Reward = np.array(Reward)\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fbcca0",
   "metadata": {},
   "source": [
    "## Part 1: Finding the Optimal Policy (Value Iteration)\n",
    "\n",
    "So far, we have focused on **policy evaluation** â€” estimating the value of a \n",
    "given policy. In this part, we move to **control**, where the goal is to find \n",
    "the **optimal policy** that maximizes long-term returns.\n",
    "\n",
    "The key idea is to apply **value iteration**, which combines evaluation and \n",
    "improvement into a single update rule:\n",
    "\n",
    "$$\n",
    "V_{k+1}(s) \\;=\\; \\max_a \\Big[ R(s) + \\gamma \\sum_{s'} P(s'|s,a) \\, V_k(s') \\Big].\n",
    "$$\n",
    "\n",
    "Once the value function converges, we extract the optimal policy by choosing, \n",
    "in each state, the action that achieves the maximum:\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\arg\\max_a \\Big[ R(s) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s') \\Big].\n",
    "$$\n",
    "\n",
    "### Steps\n",
    "1. Initialize the value function $V_0(s)$ arbitrarily (e.g., all zeros).  \n",
    "2. Repeatedly update each stateâ€™s value using the **max over actions** rule.  \n",
    "3. Stop when values converge (the updates change very little).  \n",
    "4. Derive the optimal policy $\\pi^*$ by picking the greedy action at each state.  \n",
    "\n",
    "### What to do\n",
    "- Implement value iteration with your transition model `P_all_prob` and rewards.  \n",
    "- Print the **optimal state values**.  \n",
    "- Print the **optimal policy** as action indices (0 = LEFT, 1 = DOWN, 2 = RIGHT, 3 = UP).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97536865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0ef55ef",
   "metadata": {},
   "source": [
    "## Part 2: Finding the Optimal Policy (Truncated Policy Iteration)\n",
    "So far, we have seen two extremes:  \n",
    "- **Value Iteration**, which maximizes at every update, and  \n",
    "- **Policy Iteration**, which fully evaluates a policy before improving it.  \n",
    "\n",
    "**Truncated (Modified) Policy Iteration** strikes a balance by performing only a **few** policy-evaluation sweeps before each policy-improvement step.\n",
    "\n",
    "Formally, we alternate between **partial policy evaluation** and **policy improvement**:\n",
    "\n",
    "$$\n",
    "V^{(j+1)}(s) \\;=\\; R(s) \\;+\\; \\gamma \\sum_{s'} P(s' \\mid s, \\pi_k(s))\\, V^{(j)}(s'),\n",
    "\\quad j = 0,1,\\dots,m-1,\n",
    "$$\n",
    "\n",
    "followed by:\n",
    "\n",
    "$$\n",
    "\\pi_{k+1}(s) \\;=\\; \\arg\\max_{a} \\Big[ \\, R(s) \\;+\\; \\gamma \\sum_{s'} P(s' \\mid s, a)\\, V^{(m)}(s') \\, \\Big].\n",
    "$$\n",
    "\n",
    "### Steps\n",
    "1. Initialize the value function $ V_0(s) $ (e.g., all zeros) and an initial policy $ \\pi_0 $.  \n",
    "2. **Partial Evaluation**: With $ \\pi_k$ fixed, perform only $ m $ Bellman **expectation** sweeps to update $ V $.  \n",
    "3. **Policy Improvement**: Update the policy by choosing the action that maximizes the expected return with the updated $V $.  \n",
    "4. Repeat Steps 2â€“3 until the policy no longer changes (or value/policy changes are below a small threshold).\n",
    "\n",
    "### What to do\n",
    "- Implement truncated policy iteration using your transition model `P_all_prob` and rewards `R`.  \n",
    "- Choose a small number of evaluation sweeps \\( m \\) (e.g., 1â€“3) **or** use a tolerance \\( \\theta \\) to stop early within each evaluation phase.  \n",
    "- Print the **final state values** and the **resulting policy** as action indices  \n",
    "  `(0 = LEFT, 1 = DOWN, 2 = RIGHT, 3 = UP)`.  \n",
    "- (Optional) Compare iterations/time vs. **full policy iteration** and **value iteration** on your map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c2ed431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your time to work on it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee03dd",
   "metadata": {},
   "source": [
    "## Comparison: Reproduce the Convergence Plot (PI vs. VI vs. Truncated PI)\n",
    "\n",
    "Goal: make a single figure showing how **Value Iteration (VI)**, **Policy Iteration (PI)**, and\n",
    "**Truncated Policy Iteration (TPI)** converge toward the **optimal state value** \\(V^\\*\\) for the\n",
    "same MDP (e.g., your FrozenLake map).\n",
    "\n",
    "### Setup\n",
    "- Use the **same environment**, discount factor $\\gamma$, and reward convention as in Parts 1â€“4.\n",
    "- Fix a **representative state** $s_\\text{ref}$ (recommended: the start state `S`).  \n",
    "  The y-axis will be $V_k(s_\\text{ref})$.\n",
    "- Use **synchronous** updates for VI/TPI to keep curves smooth (one full sweep = 1 iteration).\n",
    "\n",
    "### Plotting instructions\n",
    "- x-axis: iteration index $k$.\n",
    "- y-axis: $V_k(s_\\text{ref})$.\n",
    "- Draw:\n",
    "  - Magenta line with diamond markers for **Value Iteration** (`vals_vi`).\n",
    "  - Blue line with circular markers for **Policy Iteration** (`vals_pi`).\n",
    "  - Black line with square markers for **Truncated PI** (`vals_tpi`).\n",
    "  - Red **horizontal** line at $y = V^*(s_\\text{ref})$ labeled **Optimal state value**.\n",
    "- Add legend:\n",
    "  - Policy iteration\n",
    "  - Value iteration\n",
    "  - Truncated policy iteration\n",
    "  - Optimal state value\n",
    "- Label axes: `k` (iterations) and `V` (state value).\n",
    "- Add light grid lines for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb3049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your time to work on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4450a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
