{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839c9bab",
   "metadata": {},
   "source": [
    "# üßä Lab 2 ‚Äî Stochastic FrozenLake & Transition Probabilities\n",
    "\n",
    "In this lab, we will:\n",
    "1. **Visualize the slippery surface** to build intuition about stochastic transitions.\n",
    "2. **Model FrozenLake as an MDP**, explicitly defining $P(s' \\mid s,a)$.\n",
    "3. **Implement and verify** our own transition probability table and compare it to Gym's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a04647",
   "metadata": {},
   "source": [
    "### üßä Demonstration: Slippery vs. Deterministic FrozenLake\n",
    "\n",
    "Let's compare **deterministic** (`is_slippery=False`) and **stochastic** (`is_slippery=True`) FrozenLake.\n",
    "\n",
    "1. First, create the environment with `is_slippery=False` and run a trajectory.\n",
    "2. Then recreate the environment with `is_slippery=True` and run the **same function** again.\n",
    "3. Notice how the agent may slip into unintended directions when the surface is slippery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea60ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d2bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic environment (no slipping)\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"ansi\")\n",
    "print(\"=== Deterministic FrozenLake ===\")\n",
    "obs, info = env.reset(seed=42)\n",
    "print(env.render())\n",
    "# Take 4 fixed actions: DOWN, DOWN, RIGHT, RIGHT (1, 1, 2, 2)\n",
    "for action in [1, 1, 2, 2]:\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Action: {action}, State: {obs}, Reward: {reward}\")\n",
    "    print(env.render())\n",
    "    if terminated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99115d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now recreate environment with slippery dynamics\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"ansi\")\n",
    "print(\"\\n=== Slippery FrozenLake ===\")\n",
    "obs, info = env.reset(seed=42)\n",
    "print(env.render())\n",
    "\n",
    "# Take the same fixed action sequence again\n",
    "for action in [1, 1, 2, 2]:\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Action: {action}, State: {obs}, Reward: {reward}\")\n",
    "    print(env.render())\n",
    "    if terminated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b5a78",
   "metadata": {},
   "source": [
    "### üìä How Slippery Works in FrozenLake\n",
    "\n",
    "When `is_slippery=True`, the environment introduces **stochastic transitions**:\n",
    "\n",
    "- Each time you choose an action, the environment samples from:\n",
    "  - **Left turn**: with probability **1/3**\n",
    "  - **Forward (intended direction)**: with probability **1/3**\n",
    "  - **Right turn**: with probability **1/3**\n",
    "\n",
    "This means your chosen direction only succeeds about **33% of the time**.  \n",
    "If a slip would move you outside the grid, the agent simply stays in place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451f617",
   "metadata": {},
   "source": [
    "## üßÆGroup Exercise: Modeling FrozenLake as Four Transition Matrices\n",
    "\n",
    "In slippery mode, **each action has its own transition probability matrix**:\n",
    "\n",
    "- **`P_LEFT`** ‚Äî probabilities for taking action **LEFT**\n",
    "- **`P_DOWN`** ‚Äî probabilities for taking action **DOWN**\n",
    "- **`P_RIGHT`** ‚Äî probabilities for taking action **RIGHT**\n",
    "- **`P_UP`** ‚Äî probabilities for taking action **UP**\n",
    "\n",
    "Each matrix is **9√ó9** (one row per current state, one column per next state)  \n",
    "and satisfies the property that **each row sums to 1** (or 0 if the state is terminal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40519cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a smaller 3x3 map\n",
    "DESC_3x3 = [\n",
    "    \"SFF\",\n",
    "    \"FHF\",\n",
    "    \"FFG\",\n",
    "]\n",
    "env = gym.make(\"FrozenLake-v1\",desc=DESC_3x3,is_slippery=True, render_mode=\"ansi\")\n",
    "obs, info = env.reset(seed=42)\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8de090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9x9 placeholder matrices for students to fill manually\n",
    "\n",
    "P_LEFT = [\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "]\n",
    "\n",
    "P_DOWN = [\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "]\n",
    "\n",
    "P_RIGHT = [\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "]\n",
    "\n",
    "P_UP = [\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f771997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_all = np.array([P_LEFT, P_DOWN, P_RIGHT, P_UP])\n",
    "print(\"Shape of combined matrix:\", P_all.shape)  # (4, 9, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928cdb6a",
   "metadata": {},
   "source": [
    "### üéØ Overview: Sampling a Trajectory from `P_all`\n",
    "\n",
    "We now have `P_all`, a combined transition model with shape `(4, n_states, n_states)`.  \n",
    "This means we know the probability of reaching every possible next state `s'`  \n",
    "from any current state `s` for each action `a`.\n",
    "\n",
    "In this step, we will **simulate a trajectory** given:\n",
    "- A starting state (e.g., the state of `S`).\n",
    "- A fixed sequence of actions (e.g. `[0, 1, 3, 2]`).\n",
    "\n",
    "Instead of calling `env.step()`, we will:\n",
    "- Use `P_all[a, s, :]` to get the probability distribution of next states.\n",
    "- Sample the next state according to these probabilities.\n",
    "- Repeat until all actions are taken, building a list of visited states.\n",
    "\n",
    "This shows that once we have the full transition model,  \n",
    "we can generate experience **entirely from our MDP representation** ‚Äî  \n",
    "an essential idea behind **model-based reinforcement learning**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f08d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectory(P_all, start_state, actions):\n",
    "    s = start_state\n",
    "    trajectory = [s]\n",
    "    for a in actions:\n",
    "        probs = P_all[a, s]  # row of probabilities for next states\n",
    "        s_next = random.choices(range(len(probs)), weights=probs, k=1)[0]\n",
    "        trajectory.append(s_next)\n",
    "        s = s_next\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "obs = 0  # assume starting from state 0 (top-left S)\n",
    "actions = [0, 1, 3, 2]  # LEFT, DOWN, UP, RIGHT\n",
    "trajectory = sample_trajectory(P_all, obs, actions)\n",
    "print(\"Sampled trajectory of states:\", trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7706640",
   "metadata": {},
   "source": [
    "### üß≠ From Policy to State Transition Matrix\n",
    "\n",
    "Once we have defined a **policy** ‚Äî a list of actions, one for each state ‚Äî\n",
    "\n",
    "```python\n",
    "# Example: choose an action for each of the 9 states\n",
    "POLICY = [1, 1, 2, 0, 0, 2, 1, 1, 0]  # 0=LEFT, 1=DOWN, 2=RIGHT, 3=UP\n",
    "```\n",
    "\n",
    "we can use it to create a **policy-specific transition matrix** `P_pi`.\n",
    "\n",
    "---\n",
    "\n",
    "### üìù What We Are Doing\n",
    "\n",
    "- Start with `P_all`, which contains probabilities for **all actions**:\n",
    "  - Shape: `(4, 9, 9)` ‚Üí 4 actions √ó 9 states √ó 9 next states.\n",
    "- For each state `s`:\n",
    "  - Look up the action chosen by the policy: `a = POLICY[s]`.\n",
    "  - Copy the probability row `P_all[a, s, :]` into the corresponding row of `P_pi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62bce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY = [\n",
    "    0,  # state 0 (top-left)\n",
    "    0,  # state 1\n",
    "    0,  # state 2\n",
    "    0,  # state 3\n",
    "    0,  # state 4 (center, possibly hole)\n",
    "    0,  # state 5\n",
    "    0,  # state 6\n",
    "    0,  # state 7\n",
    "    0,  # state 8 (goal state)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a8817",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = len(POLICY)\n",
    "P_pi = np.zeros((n_states, n_states))\n",
    "\n",
    "for s in range(n_states):\n",
    "    a = POLICY[s]             # action chosen at state s\n",
    "    P_pi[s, :] = P_all[a, s]  # copy the probabilities for that action\n",
    "\n",
    "# Print the resulting matrix\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "print(\"Policy-specific transition matrix (P_pi):\")\n",
    "print(P_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d7449b",
   "metadata": {},
   "source": [
    "### üìà Policy Evaluation: Solving for State Values\n",
    "\n",
    "Now that we have:\n",
    "\n",
    "- **`P_pi`** ‚Üí the policy-specific state transition matrix (9√ó9)\n",
    "- **`R`** ‚Üí the reward vector for each state (or reward per state-action-next_state)\n",
    "\n",
    "we can compute the **value of each state under this policy**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1745616",
   "metadata": {},
   "source": [
    "### üìù What We Are Doing\n",
    "\n",
    "The state value function under a fixed policy satisfies:\n",
    "\n",
    "$$\n",
    "V = R + \\gamma P_\\pi V\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- **`V`** is a column vector of state values.\n",
    "- **`R`** is a column vector of expected immediate rewards per state.\n",
    "- **`Œ≥`** is the discount factor (e.g. 0.9).\n",
    "- **`P_pi`** is the transition matrix under the policy.\n",
    "\n",
    "We can rearrange this to solve for `V` directly:\n",
    "\n",
    "$$\n",
    "(I - \\gamma P_\\pi) V = R\n",
    "\\quad\\Rightarrow\\quad\n",
    "V = (I - \\gamma P_\\pi)^{-1} R\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Your Task\n",
    "\n",
    "1. Build a reward vector `R` of length 9 (1 for goal states, 0 otherwise).\n",
    "2. Choose a discount factor `gamma` (e.g. 0.9).\n",
    "3. Solve for `V` using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae52c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your time to work on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed850207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a40a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
