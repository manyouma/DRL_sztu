{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3db9f95-e915-4710-bffe-e6e47588452b",
   "metadata": {},
   "source": [
    "# ðŸ§© Lab 10: REINFORCE on CartPole \n",
    "\n",
    "In the previous labs, we solved the CartPole control task using a Monte Carlo approach to estimate the value function. We discretized the state space, collected full trajectories, computed returns, and used those returns to update a tabular estimate of \\(Q(s,a)\\).\n",
    "\n",
    "In this lab, we will revisit the CartPole environment, but instead of estimating a value function, we will directly learn a **parameterized policy** using a neural network. This approach is known as **policy gradient**. Rather than selecting actions based on a Q-table, the policy network outputs a probability distribution over actions, and we update its parameters so that actions leading to higher returns become more likely.\n",
    "\n",
    "Our goal is to implement **REINFORCE**, one of the simplest policyâ€gradient algorithms:\n",
    "- collect full episodes under the current policy,\n",
    "- compute Monte Carlo returns for each time step,\n",
    "- and adjust the policy parameters in the direction that increases the logâ€probability of good actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc3a20d8-5b0c-4dd3-b4e2-9c92e5919e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4520a062-b0e5-4c94-9c53-91b46ebe9a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")   # no need to discretize now\n",
    "obs, info = env.reset(seed=0)\n",
    "obs_dim = env.observation_space.shape[0]  # 4 for CartPole\n",
    "n_actions = env.action_space.n            # 2 for CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666fcc4f-a7a9-4856-ba23-2570f40400af",
   "metadata": {},
   "source": [
    "### Task 1: Define a Policy Network `PolicyNet`\n",
    "\n",
    "In this part, you will implement a small neural network that represents the policy  \n",
    "$\\pi_\\theta(a \\mid s)$ for CartPole.\n",
    "\n",
    "The observation space of `CartPole-v1` is a 4-dimensional vector:\n",
    "- cart position\n",
    "- cart velocity\n",
    "- pole angle\n",
    "- pole angular velocity\n",
    "\n",
    "The action space has 2 discrete actions:\n",
    "- `0`: push cart to the left  \n",
    "- `1`: push cart to the right  \n",
    "\n",
    "We will use a **multi-layer perceptron (MLP)** that:\n",
    "- takes the observation \\(s \\in \\mathbb{R}^4\\) as input,\n",
    "- outputs **logits** over the 2 actions (these will go into a `Categorical` distribution),\n",
    "- uses **two hidden layers** with ReLU activations.\n",
    "\n",
    "Hints for architecture:\n",
    "- Use `nn.Sequential` to stack layers.\n",
    "- A common choice for CartPole is:\n",
    "  - First hidden layer: around 100â€“150 units (e.g., `128`).\n",
    "  - Second hidden layer: smaller, e.g., about half of the first layer (e.g., `64`).\n",
    "- The final linear layer should map from the second hidden layer to `n_actions`\n",
    "  (no activation on the output layer; the `Categorical` distribution will handle the softmax internally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6934afe5-ca82-42ca-8229-ff11f170ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Policy network Ï€_Î¸(a | s) -----\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, obs_dim, n_actions):\n",
    "        super().__init__()\n",
    "        # Your time to work on it\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)  # shape: (batch, n_actions)\n",
    "        return torch.distributions.Categorical(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04832f45-a0ba-4fab-aa26-8acb97a4d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = PolicyNet(obs_dim, n_actions)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)\n",
    "gamma = 0.99\n",
    "num_episodes = 200000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e603189-ca5f-4a7a-8c1c-36a86d896a8a",
   "metadata": {},
   "source": [
    "### Task 2: Implement the REINFORCE Loss Function\n",
    "\n",
    "After collecting one full episode and computing the Monte Carlo returns $G_t$ for\n",
    "each time step, the final step is to update the policy parameters.  \n",
    "In REINFORCE, we adjust the policy in the direction that increases the\n",
    "log-probability of actions that resulted in high returns.\n",
    "\n",
    "Recall the update rule:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\, \\nabla_\\theta \n",
    "\\log \\pi_\\theta(a_t \\mid s_t) \\, G_t.\n",
    "$$\n",
    "\n",
    "In practice, instead of applying this update manually, we construct a **loss\n",
    "function** such that performing gradient descent on the loss produces the same\n",
    "update as gradient ascent on $J(\\theta)$.\n",
    "\n",
    "Your task:\n",
    "\n",
    "1. You have a list of `log_probs`, one for each action taken in the episode.\n",
    "2. You have a list of `returns`, containing the Monte Carlo return $G_t$ for each step.\n",
    "3. Combine them into a single scalar loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3008da15-bd0b-4000-8fe0-712609b76453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  100 | Return: 43.0 | Avg(50):  25.6\n",
      "Episode  200 | Return: 12.0 | Avg(50):  25.5\n",
      "Episode  300 | Return: 38.0 | Avg(50):  28.7\n",
      "Episode  400 | Return: 33.0 | Avg(50):  25.4\n",
      "Episode  500 | Return: 37.0 | Avg(50):  32.4\n",
      "Episode  600 | Return: 27.0 | Avg(50):  28.3\n",
      "Episode  700 | Return: 55.0 | Avg(50):  36.6\n",
      "Episode  800 | Return: 53.0 | Avg(50):  36.4\n",
      "Episode  900 | Return: 29.0 | Avg(50):  42.7\n",
      "Episode 1000 | Return: 59.0 | Avg(50):  43.9\n",
      "Episode 1100 | Return: 37.0 | Avg(50):  44.2\n",
      "Episode 1200 | Return: 51.0 | Avg(50):  51.1\n",
      "Episode 1300 | Return: 55.0 | Avg(50):  48.5\n",
      "Episode 1400 | Return: 92.0 | Avg(50):  55.7\n",
      "Episode 1500 | Return: 114.0 | Avg(50):  59.6\n",
      "Episode 1600 | Return: 136.0 | Avg(50):  59.1\n",
      "Episode 1700 | Return: 37.0 | Avg(50):  66.4\n",
      "Episode 1800 | Return: 65.0 | Avg(50):  79.0\n",
      "Episode 1900 | Return: 72.0 | Avg(50):  91.1\n",
      "Episode 2000 | Return: 183.0 | Avg(50): 122.3\n",
      "Episode 2100 | Return: 155.0 | Avg(50): 122.6\n",
      "Episode 2200 | Return: 131.0 | Avg(50): 182.5\n",
      "Episode 2300 | Return: 170.0 | Avg(50): 170.2\n",
      "Episode 2400 | Return: 424.0 | Avg(50): 217.0\n",
      "Episode 2500 | Return: 175.0 | Avg(50): 218.1\n",
      "Episode 2600 | Return: 350.0 | Avg(50): 248.7\n",
      "Episode 2700 | Return: 331.0 | Avg(50): 249.0\n",
      "Episode 2800 | Return: 334.0 | Avg(50): 263.1\n",
      "Episode 2900 | Return: 232.0 | Avg(50): 295.3\n",
      "Episode 3000 | Return: 127.0 | Avg(50): 290.1\n",
      "Episode 3100 | Return: 486.0 | Avg(50): 295.2\n",
      "Episode 3200 | Return: 347.0 | Avg(50): 305.4\n",
      "Episode 3300 | Return: 500.0 | Avg(50): 314.3\n",
      "Episode 3400 | Return: 274.0 | Avg(50): 311.6\n",
      "Episode 3500 | Return: 403.0 | Avg(50): 311.5\n",
      "Episode 3600 | Return: 481.0 | Avg(50): 334.3\n",
      "Episode 3700 | Return: 340.0 | Avg(50): 377.6\n",
      "Episode 3800 | Return: 252.0 | Avg(50): 362.3\n",
      "Episode 3900 | Return: 500.0 | Avg(50): 418.8\n",
      "Episode 4000 | Return: 416.0 | Avg(50): 392.5\n",
      "Episode 4100 | Return: 236.0 | Avg(50): 356.1\n",
      "Episode 4200 | Return: 345.0 | Avg(50): 402.0\n",
      "Episode 4300 | Return: 427.0 | Avg(50): 421.7\n",
      "Episode 4400 | Return: 434.0 | Avg(50): 403.4\n",
      "Episode 4500 | Return: 500.0 | Avg(50): 439.2\n",
      "Episode 4600 | Return: 407.0 | Avg(50): 402.8\n",
      "Episode 4700 | Return: 424.0 | Avg(50): 390.9\n",
      "Episode 4800 | Return: 344.0 | Avg(50): 420.0\n",
      "Episode 4900 | Return: 383.0 | Avg(50): 394.6\n",
      "Episode 5000 | Return: 148.0 | Avg(50): 413.6\n",
      "Episode 5100 | Return: 500.0 | Avg(50): 484.6\n",
      "Episode 5200 | Return: 500.0 | Avg(50): 429.9\n",
      "Episode 5300 | Return: 277.0 | Avg(50): 409.7\n",
      "Episode 5400 | Return: 327.0 | Avg(50): 408.9\n",
      "Episode 5500 | Return: 500.0 | Avg(50): 456.3\n",
      "Episode 5600 | Return: 407.0 | Avg(50): 449.4\n",
      "Episode 5700 | Return: 500.0 | Avg(50): 435.0\n",
      "Episode 5800 | Return: 273.0 | Avg(50): 284.0\n",
      "Episode 5900 | Return: 500.0 | Avg(50): 448.6\n",
      "Episode 6000 | Return: 500.0 | Avg(50): 423.4\n",
      "Episode 6100 | Return: 329.0 | Avg(50): 465.7\n",
      "Episode 6200 | Return: 500.0 | Avg(50): 483.8\n",
      "Episode 6300 | Return: 500.0 | Avg(50): 489.0\n",
      "Episode 6400 | Return: 490.0 | Avg(50): 374.4\n",
      "Episode 6500 | Return: 352.0 | Avg(50): 406.5\n",
      "Episode 6600 | Return: 500.0 | Avg(50): 466.1\n",
      "Episode 6700 | Return: 500.0 | Avg(50): 476.6\n",
      "Episode 6800 | Return: 500.0 | Avg(50): 466.3\n",
      "Episode 6900 | Return: 500.0 | Avg(50): 477.3\n",
      "Episode 7000 | Return: 300.0 | Avg(50): 450.4\n",
      "Episode 7100 | Return: 460.0 | Avg(50): 464.2\n",
      "Episode 7200 | Return: 385.0 | Avg(50): 469.5\n",
      "Episode 7300 | Return: 355.0 | Avg(50): 447.9\n",
      "Episode 7400 | Return: 500.0 | Avg(50): 465.2\n",
      "Episode 7500 | Return: 500.0 | Avg(50): 483.0\n",
      "Episode 7600 | Return: 500.0 | Avg(50): 466.7\n",
      "Episode 7700 | Return: 287.0 | Avg(50): 397.9\n",
      "Episode 7800 | Return: 355.0 | Avg(50): 370.2\n",
      "Episode 7900 | Return: 289.0 | Avg(50): 384.9\n",
      "Episode 8000 | Return: 500.0 | Avg(50): 382.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m dist \u001b[38;5;241m=\u001b[39m policy(s_t)                                          \u001b[38;5;66;03m# Ï€_Î¸(. | s_t)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m a_t \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()                                         \u001b[38;5;66;03m# sample action\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m log_prob_t \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_t\u001b[49m\u001b[43m)\u001b[49m                             \u001b[38;5;66;03m# log Ï€_Î¸(a_t | s_t)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m obs_next, r, term, trunc, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a_t\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     18\u001b[0m done \u001b[38;5;241m=\u001b[39m term \u001b[38;5;129;01mor\u001b[39;00m trunc\n",
      "File \u001b[1;32m~\\.conda\\envs\\atari\\lib\\site-packages\\torch\\distributions\\categorical.py:138\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[1;32m--> 138\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    140\u001b[0m     value, log_pmf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits)\n",
      "File \u001b[1;32m~\\.conda\\envs\\atari\\lib\\site-packages\\torch\\distributions\\distribution.py:314\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m support \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    313\u001b[0m valid \u001b[38;5;241m=\u001b[39m support\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m--> 314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mvalid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    316\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected value argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "returns_history = []\n",
    "\n",
    "for ep in range(1, num_episodes + 1):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    log_probs = []   # store log Ï€_Î¸(a_t | s_t)\n",
    "    rewards = []     # store r_t\n",
    "\n",
    "    # Generate an episode\n",
    "    while not done:\n",
    "        s_t = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)   # shape (1, obs_dim)\n",
    "        dist = policy(s_t)                                          # Ï€_Î¸(. | s_t)\n",
    "        a_t = dist.sample()                                         # sample action\n",
    "        log_prob_t = dist.log_prob(a_t)                             # log Ï€_Î¸(a_t | s_t)\n",
    "\n",
    "        obs_next, r, term, trunc, _ = env.step(a_t.item())\n",
    "        done = term or trunc\n",
    "\n",
    "        log_probs.append(log_prob_t)\n",
    "        rewards.append(r)\n",
    "\n",
    "        obs = obs_next\n",
    "\n",
    "    #  Value update\n",
    "    T = len(rewards)\n",
    "    G = 0.0\n",
    "    returns = []\n",
    "\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "   \n",
    "\n",
    "    log_probs = torch.stack(log_probs)      # shape (T,)\n",
    "\n",
    "    # Your time to work on it\n",
    "    loss = -(log_probs * returns).sum()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    ep_return = sum(rewards)\n",
    "    returns_history.append(ep_return)\n",
    "\n",
    "    if ep % 100 == 0:\n",
    "        avg = np.mean(returns_history[-50:])\n",
    "        print(f\"Episode {ep:4d} | Return: {ep_return:4.1f} | \"\n",
    "              f\"Avg(50): {avg:5.1f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7addcab-5c31-4644-af1a-bc01dc957a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86683be-8318-4650-864d-553512bbc8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5640ae4f-fa4a-4938-97ac-a12b946a7122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
