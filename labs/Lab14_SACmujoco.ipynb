{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a971857-7f18-444a-af38-af4d0bab0430",
   "metadata": {},
   "source": [
    "# Lab 14: Soft Actor-Critic (SAC) on HalfCheetah-v4\n",
    "\n",
    "In this lab, we will move from **TD3 (deterministic policy)** to **Soft Actor-Critic (SAC)**, which is one of the most important **modern off-policy stochastic actor-critic algorithms**.\n",
    "\n",
    "Unlike TD3, SAC does **not** learn a single deterministic action. Instead, it learns a **probability distribution over actions**, and explicitly encourages **exploration via entropy maximization**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c615dd-8837-4a8b-bb35-14a694aad828",
   "metadata": {},
   "source": [
    "## Key Idea of SAC\n",
    "\n",
    "SAC optimizes the following objective:\n",
    "\n",
    "$$\n",
    "\\max_\\pi \\; \\mathbb{E}\\left[ \\sum_t r(s_t, a_t) + \\alpha \\mathcal{H}(\\pi(\\cdot | s_t)) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ r(s, a)$: task reward  \n",
    "- $ \\mathcal{H}(\\pi) $: entropy of the policy  \n",
    "- $ \\alpha $: temperature coefficient (controls exploration strength)  \n",
    "\n",
    "✅ High entropy ⇒ more exploration  \n",
    "✅ Low entropy ⇒ more exploitation  \n",
    "\n",
    "---\n",
    "\n",
    "### From the Maximum-Entropy Objective to the Actor Loss\n",
    "\n",
    "The entropy term can be written as:\n",
    "\n",
    "$$\n",
    "\\mathcal{H}(\\pi(\\cdot|s)) \n",
    "= -\\mathbb{E}_{a \\sim \\pi}[\\log \\pi(a|s)]\n",
    "$$\n",
    "\n",
    "Substituting it into the objective leads to the following policy optimization problem:\n",
    "\n",
    "$$\n",
    "\\max_\\pi \\;\n",
    "\\mathbb{E}_{s \\sim D,\\, a \\sim \\pi}\n",
    "\\left[\n",
    "Q(s,a) - \\alpha \\log \\pi(a|s)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "For gradient-based optimization, this is implemented in its **minimization form**, which gives the **actor loss used in SAC**:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\mathcal{L}_{\\text{actor}}\n",
    "=\n",
    "\\mathbb{E}\n",
    "\\left[\n",
    "\\alpha \\log \\pi(a|s) - Q(s,a)\n",
    "\\right]\n",
    "}\n",
    "$$\n",
    "\n",
    "This loss explicitly shows the **trade-off between**:\n",
    "- maximizing long-term reward $Q(s,a)$, and  \n",
    "- maintaining sufficient exploration through the entropy term $\\alpha \\log \\pi(a|s)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed2e19-1d92-4cf0-abb5-6bf637c14903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bca5de1-1caa-47bd-8638-6b270d673ae0",
   "metadata": {},
   "source": [
    "## 1. Gaussian Policy in Soft Actor-Critic (SAC)\n",
    "\n",
    "Unlike TD3, which uses a **deterministic policy**  \n",
    "$$\n",
    "a = \\pi_\\theta(s)\n",
    "$$\n",
    "SAC uses a **stochastic policy**, where actions are sampled from a **Gaussian distribution**:\n",
    "\n",
    "$$\n",
    "a \\sim \\mathcal{N}(\\mu_\\theta(s), \\sigma_\\theta(s))\n",
    "$$\n",
    "\n",
    "This means the policy does not output a single fixed action, but a **probability distribution over actions**.\n",
    "\n",
    "For each state $s$, the policy network outputs two vectors:\n",
    "\n",
    "- **Mean**: $\\mu_\\theta(s)$\n",
    "- **Log standard deviation**: $\\log \\sigma_\\theta(s) $\n",
    "\n",
    "So the policy represents the distribution:\n",
    "\n",
    "$$\n",
    "\\pi(a|s) = \\mathcal{N}(\\mu_\\theta(s), \\sigma_\\theta(s))\n",
    "$$\n",
    "\n",
    "The log standard deviation is clipped to avoid:\n",
    "- extremely large variance\n",
    "- numerical instability\n",
    "- exploding gradients\n",
    "\n",
    "To enable backpropagation through a random sample, SAC uses the **reparameterization trick**:\n",
    "\n",
    "$$\n",
    "u = \\mu_\\theta(s) + \\sigma_\\theta(s) \\cdot \\epsilon,\n",
    "\\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "This transforms a random sampling process into a **deterministic function with random noise**, allowing gradients to flow through the policy parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c03c0e-e0e2-45d6-b5f2-167341557631",
   "metadata": {},
   "source": [
    "Most continuous-control environments require bounded actions:\n",
    "\n",
    "$$\n",
    "a \\in [-1, 1]\n",
    "$$\n",
    "\n",
    "So the sampled value $ u $ is passed through:\n",
    "\n",
    "$$\n",
    "a = \\tanh(u)\n",
    "$$\n",
    "\n",
    "and then rescaled:\n",
    "\n",
    "$$\n",
    "a_{\\text{env}} = a \\cdot a_{\\max}\n",
    "$$\n",
    "\n",
    "This guarantees that all actions sent to the environment are valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f85bfa5-7499-4f08-97d7-92e15ead7646",
   "metadata": {},
   "source": [
    "Because the action is transformed using `tanh`, the log-probability must be corrected using the **change-of-variables rule**:\n",
    "\n",
    "$$\n",
    "\\log \\pi(a|s)\n",
    "=\n",
    "\\log \\mathcal{N}(u; \\mu, \\sigma)\n",
    "-\n",
    "\\log(1 - \\tanh^2(u))\n",
    "$$\n",
    "\n",
    "This corrected log-probability is **essential** for:\n",
    "\n",
    "- The critic target update\n",
    "- The actor (policy) update\n",
    "- The temperature parameter $ \\alpha $ update\n",
    "\n",
    "Without this correction, the entropy estimate would be wrong and training would become unstable.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "During evaluation, SAC does **not use random sampling**.  \n",
    "Instead, it uses the **mean action**:\n",
    "\n",
    "$$\n",
    "a_{\\text{eval}} = \\tanh(\\mu_\\theta(s)) \\cdot a_{\\max}\n",
    "$$\n",
    "\n",
    "This ensures:\n",
    "- stable test performance\n",
    "- reproducible results\n",
    "- no randomness during evaluation\n",
    "\n",
    "---\n",
    "\n",
    "####  What the Gaussian Policy Produces\n",
    "\n",
    "For each input state \\( s \\), the Gaussian policy provides:\n",
    "\n",
    "| Quantity | Mathematical Meaning | Purpose |\n",
    "|----------|------------------------|---------|\n",
    "| $ a $ | sampled action | environment interaction |\n",
    "| $ \\log \\pi(a|s) $ | log-probability | critic target, actor loss, α update |\n",
    "| $ \\tanh(\\mu) $ | mean (deterministic) action | evaluation |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b764a9-8828-4b30-aa6f-de8a5f161149",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int, max_action: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "\n",
    "        self.mean_layer = nn.Linear(256, action_dim)\n",
    "        self.log_std_layer = nn.Linear(256, action_dim)\n",
    "\n",
    "        # For clamping log_std\n",
    "        self.LOG_STD_MIN = -20\n",
    "        self.LOG_STD_MAX = 2\n",
    "\n",
    "    def forward(self, state: torch.Tensor):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        mean = self.mean_layer(x)\n",
    "        log_std = self.log_std_layer(x)\n",
    "        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, state: torch.Tensor):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        normal = Normal(mean, std)\n",
    "        u = normal.rsample()\n",
    "\n",
    "        tanh_u = torch.tanh(u)\n",
    "        action = tanh_u * self.max_action\n",
    "\n",
    "        log_prob = normal.log_prob(u)\n",
    "        \n",
    "        epsilon = 1e-6\n",
    "        log_prob -= torch.log(1 - tanh_u.pow(2) + epsilon)\n",
    "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        mean_action = torch.tanh(mean) * self.max_action\n",
    "\n",
    "        return action, log_prob, mean_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6553b106-21cb-4fb4-a65a-40f640fc6f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.q1_fc1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.q1_fc2 = nn.Linear(256, 256)\n",
    "        self.q1_out = nn.Linear(256, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.q2_fc1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.q2_fc2 = nn.Linear(256, 256)\n",
    "        self.q2_out = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor):\n",
    "        xu = torch.cat([state, action], dim=-1)  # (batch, state_dim + action_dim)\n",
    "\n",
    "        # Q1\n",
    "        x1 = F.relu(self.q1_fc1(xu))\n",
    "        x1 = F.relu(self.q1_fc2(x1))\n",
    "        q1 = self.q1_out(x1)\n",
    "\n",
    "        # Q2\n",
    "        x2 = F.relu(self.q2_fc1(xu))\n",
    "        x2 = F.relu(self.q2_fc2(x2))\n",
    "        q2 = self.q2_out(x2)\n",
    "\n",
    "        return q1, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c372990-5790-4fbe-8333-a1bfdefcbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, size):\n",
    "        self.size = size\n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "\n",
    "        self.states = np.zeros((size, state_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros((size, action_dim), dtype=np.float32)\n",
    "        self.rewards = np.zeros((size,), dtype=np.float32)\n",
    "        self.next_states = np.zeros((size, state_dim), dtype=np.float32)\n",
    "        self.dones = np.zeros((size,), dtype=np.float32)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.states[self.ptr] = state\n",
    "        self.actions[self.ptr] = action\n",
    "        self.rewards[self.ptr] = reward\n",
    "        self.next_states[self.ptr] = next_state\n",
    "        self.dones[self.ptr] = done\n",
    "\n",
    "        self.ptr += 1\n",
    "        if self.ptr >= self.size:\n",
    "            self.ptr = 0\n",
    "            self.full = True\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size if self.full else self.ptr\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        max_size = self.size if self.full else self.ptr\n",
    "        idx = np.random.randint(0, max_size, size=batch_size)\n",
    "\n",
    "        states = torch.as_tensor(self.states[idx], dtype=torch.float32, device=device)\n",
    "        actions = torch.as_tensor(self.actions[idx], dtype=torch.float32, device=device)\n",
    "        rewards = torch.as_tensor(self.rewards[idx], dtype=torch.float32, device=device).unsqueeze(-1)\n",
    "        next_states = torch.as_tensor(self.next_states[idx], dtype=torch.float32, device=device)\n",
    "        dones = torch.as_tensor(self.dones[idx], dtype=torch.float32, device=device).unsqueeze(-1)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "\n",
    "def soft_update(source: nn.Module, target: nn.Module, tau: float):\n",
    "    with torch.no_grad():\n",
    "        for param, target_param in zip(source.parameters(), target.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32130f34-10ca-41f2-a655-e83c87b8a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"HalfCheetah-v4\"\n",
    "seed = 0\n",
    "\n",
    "# ==========================\n",
    "#  Hyperparameters\n",
    "# ==========================\n",
    "replay_size = int(1e6)\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "lr = 3e-4\n",
    "batch_size = 256\n",
    "\n",
    "# Number of steps purely using random actions at the beginning\n",
    "start_steps = 10_000\n",
    "# Total environment interaction steps\n",
    "max_steps = 300_000\n",
    "# Start learning only after some initial experience\n",
    "update_after = 1_000\n",
    "# Number of gradient updates per environment step\n",
    "update_every = 1\n",
    "# Interval to run evaluation\n",
    "eval_interval = 5_000\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9626cdf3-5a2d-4cc1-a515-0f2ba1b584f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.reset(seed=seed)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "print(\"State dim:\", state_dim)\n",
    "print(\"Action dim:\", action_dim)\n",
    "print(\"Max action:\", max_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c60005c-ef27-4a5f-badf-1bb762c3a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = GaussianPolicy(state_dim, action_dim, max_action).to(device)\n",
    "critic = Critic(state_dim, action_dim).to(device)\n",
    "critic_target = Critic(state_dim, action_dim).to(device)\n",
    "\n",
    "critic_target.load_state_dict(critic.state_dict())\n",
    "\n",
    "policy_optimizer = torch.optim.Adam(policy.parameters(), lr=lr)\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=lr)\n",
    "\n",
    "log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "alpha_optimizer = torch.optim.Adam([log_alpha], lr=lr)\n",
    "\n",
    "target_entropy = -float(action_dim)\n",
    "\n",
    "replay_buffer = ReplayBuffer(state_dim, action_dim, replay_size)\n",
    "\n",
    "state, _ = env.reset(seed=seed)\n",
    "episode_return = 0.0\n",
    "episode_length = 0\n",
    "episode_idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf2b9d-a184-406e-9908-5f88247823fc",
   "metadata": {},
   "source": [
    "## Hints for the SAC Update Block \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Sampling the Next Action from the Current Policy  \n",
    "(used for the critic target)\n",
    "\n",
    "Purpose:\n",
    "- Generate the next action according to the **current stochastic policy**\n",
    "- Also obtain its **log-probability**, which is required by the entropy term\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "a_{t+1} \\sim \\pi(\\cdot | s_{t+1}), \n",
    "\\quad \\log \\pi(a_{t+1} | s_{t+1})\n",
    "$$\n",
    "\n",
    "This corresponds to the line:\n",
    "- `next_actions, next_log_pi, _ = ?`\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### Evaluating the Target Critic on the Next State–Action Pair\n",
    "\n",
    "Purpose:\n",
    "- Evaluate the value of the **next state and next action**\n",
    "- Use the **target critic networks**, not the online critics\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "Q_1'(s_{t+1}, a_{t+1}), \\quad Q_2'(s_{t+1}, a_{t+1})\n",
    "$$\n",
    "\n",
    "This corresponds to the line:\n",
    "- `q1_next_target, q2_next_target = ?`\n",
    "\n",
    "\n",
    "#### Taking the Minimum of the Twin Target Q-Values\n",
    "\n",
    "Purpose:\n",
    "- Prevent overestimation bias\n",
    "- Follow the TD3-style minimum trick\n",
    "\n",
    "$$\n",
    "Q_{\\text{min}}'(s_{t+1}, a_{t+1})\n",
    "=\n",
    "\\min(Q_1', Q_2')\n",
    "$$\n",
    "\n",
    "This corresponds to the line:\n",
    "- `q_next_target = ?`\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### Computing the Soft Bellman Target for the Critic\n",
    "\n",
    "Purpose:\n",
    "- Build the **entropy-regularized TD target**\n",
    "\n",
    "$$\n",
    "y_t\n",
    "=\n",
    "r_t\n",
    "+\n",
    "(1 - d_t)\\,\\gamma\n",
    "\\left(\n",
    "Q_{\\text{min}}'(s_{t+1}, a_{t+1})\n",
    "-\n",
    "\\alpha \\log \\pi(a_{t+1} | s_{t+1})\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Key components:\n",
    "- Immediate reward: $ r_t $\n",
    "- Discount factor: $ \\gamma $\n",
    "- Done mask: $ (1 - d_t) $\n",
    "- Entropy penalty: $\\alpha \\log \\pi $\n",
    "\n",
    "This corresponds to the line:\n",
    "- `target_q = ?`\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### Computing the Current Critic Predictions\n",
    "\n",
    "Purpose:\n",
    "- Evaluate the current critic on **replay-buffer actions**\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "Q_1(s_t, a_t), \\quad Q_2(s_t, a_t)\n",
    "$$\n",
    "\n",
    "These are used to regress toward the TD target.\n",
    "\n",
    "This corresponds to the line:\n",
    "- `q1_pred, q2_pred = ?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e090ef-cae3-40c1-96c3-40e04fab9c3e",
   "metadata": {},
   "source": [
    "#### Critic Loss: Mean Squared Bellman Error\n",
    "\n",
    "Purpose:\n",
    "- Force both critics to match the entropy-regularized TD target\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{critic}}\n",
    "=\n",
    "\\| Q_1(s,a) - y \\|^2\n",
    "+\n",
    "\\| Q_2(s,a) - y \\|^2\n",
    "$$\n",
    "\n",
    "This corresponds to the line:\n",
    "- `critic_loss = ?`\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### Evaluating the Critic on Newly Sampled Actions (Actor Update)\n",
    "\n",
    "Purpose:\n",
    "- Evaluate **how good the current policy’s own actions are**\n",
    "- These actions are **not from the replay buffer**\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "a \\sim \\pi(\\cdot | s)\n",
    "$$\n",
    "$$\n",
    "Q_1(s, a), \\quad Q_2(s, a)\n",
    "$$\n",
    "\n",
    "This corresponds to the line:\n",
    "- `q1_new, q2_new = ?`\n",
    "\n",
    "---\n",
    "\n",
    "#### Taking the Minimum Q-Value for Policy Optimization\n",
    "\n",
    "Purpose:\n",
    "- Follow the conservative evaluation principle\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "Q_{\\text{min}}(s, a) = \\min(Q_1, Q_2)\n",
    "$$\n",
    "\n",
    "This corresponds to the line:\n",
    "- `q_new = ?`\n",
    "\n",
    "---\n",
    "\n",
    "#### Actor Loss: Maximum-Entropy Policy Objective\n",
    "\n",
    "Purpose:\n",
    "- Optimize the policy for **both high reward and high entropy**\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{actor}}\n",
    "=\n",
    "\\mathbb{E}\n",
    "\\left[\n",
    "\\alpha \\log \\pi(a|s) - Q_{\\text{min}}(s,a)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "- The $ -Q $ term encourages **high-value actions**\n",
    "- The $ \\alpha \\log \\pi $ term encourages **stochastic exploration**\n",
    "\n",
    "This corresponds to the line:\n",
    "- `actor_loss = ?`\n",
    "\n",
    "---\n",
    "\n",
    "#### Alpha Loss: Automatic Temperature Adjustment\n",
    "\n",
    "Purpose:\n",
    "- Automatically adjust the **exploration strength** so that:\n",
    "$$\n",
    "\\mathcal{H}(\\pi) \\approx \\mathcal{H}_{\\text{target}}\n",
    "$$\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "\\mathcal{L}_{\\alpha}\n",
    "=\n",
    "-\\log \\alpha \\cdot\n",
    "\\left(\n",
    "\\log \\pi(a|s) + \\mathcal{H}_{\\text{target}}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Key idea:\n",
    "- If the policy is **too deterministic**, α should increase\n",
    "- If the policy is **too random**, α should decrease\n",
    "\n",
    "This corresponds to the line:\n",
    "- `alpha_loss = ?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8d31c-dd95-4fe1-9cb1-1dd59c374929",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(1, max_steps + 1):\n",
    "\n",
    "    if t < start_steps:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        state_tensor = torch.as_tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            sampled_action, _, _ = policy.sample(state_tensor)\n",
    "        action = sampled_action.cpu().numpy()[0]\n",
    "\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    episode_return += reward\n",
    "    episode_length += 1\n",
    "\n",
    "    replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        print(\n",
    "            f\"Step {t} | \"\n",
    "            f\"Episode {episode_idx} | \"\n",
    "            f\"Return: {episode_return:.1f} | \"\n",
    "            f\"Length: {episode_length}\"\n",
    "        )\n",
    "        state, _ = env.reset()\n",
    "        episode_return = 0.0\n",
    "        episode_length = 0\n",
    "        episode_idx += 1\n",
    "\n",
    "\n",
    "    if t >= update_after and len(replay_buffer) >= batch_size:\n",
    "        for _ in range(update_every):\n",
    "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size, device)\n",
    "            alpha = log_alpha.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                next_actions, next_log_pi, _ = #\n",
    "                q1_next_target, q2_next_target = #\n",
    "                q_next_target = #\n",
    "                target_q = #\n",
    "                \n",
    "            q1_pred, q2_pred = #\n",
    "            critic_loss = #\n",
    "\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            # pick action again and optimize \n",
    "            new_actions, log_pi, _ = policy.sample(states)\n",
    "            q1_new, q2_new = #\n",
    "            q_new = #\n",
    "            actor_loss = #\n",
    "            \n",
    "            policy_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "\n",
    "            alpha_loss = #\n",
    "\n",
    "            alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            alpha_optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for param, target_param in zip(critic.parameters(), critic_target.parameters()):\n",
    "                    target_param.data.copy_(\n",
    "                        tau * param.data + (1.0 - tau) * target_param.data\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca8cad3-9426-40d4-a27f-f97ec772ebd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e89ce-698f-4927-a9c0-ad80f0aae48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5088d86d-95b7-431d-bcd2-b95b07ad514f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
