{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1002a6a-1182-4d5b-9c4e-b2ae42303cbf",
   "metadata": {},
   "source": [
    "# Lab 11: A2C on CartPole with n-Step Bootstrapping\n",
    "\n",
    "In this experiment, we implement a minimal yet fully functional version of the **Advantage Actor‚ÄìCritic (A2C)** algorithm and apply it to the classic **CartPole-v1** control task.  \n",
    "A2C is one of the core policy-gradient methods where a learned value function is used as a **baseline** to reduce variance, while the actor learns a stochastic policy that maximizes expected cumulative rewards.\n",
    "\n",
    "Unlike simple Monte-Carlo policy gradient, A2C supports **bootstrapped n-step returns**, which combine short-horizon rewards with value estimates of future states. This makes training more stable and significantly improves learning efficiency. In this lab, you will:\n",
    "\n",
    "- Implement an Actor‚ÄìCritic network with shared representation.\n",
    "- Collect trajectories from the CartPole environment.\n",
    "- Compute **n-step bootstrapped targets** $R_t^{(n)}$.\n",
    "- Compute the **advantage** $A_t = R_t^{(n)} - V(s_t)$.\n",
    "- Update both policy and value networks using stochastic gradient descent.\n",
    "\n",
    "This version of A2C is intentionally kept small and readable so that you can fully understand how policy gradients and value baselines work together. Once you are familiar with the single-environment version, you can extend it to multi-environment parallel A2C, GAE(Œª), or even PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c210687d-60fa-473a-bd0d-34571b6bc7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import gymnasium as gym "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30ebab-eaf6-4f45-8973-e01d3bbc5d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LR = 1e-3\n",
    "MAX_EPISODES = 1000\n",
    "PRINT_INTERVAL = 10\n",
    "N_STEPS = 20  \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1217f7ac-241c-4fa1-94ce-c52043a5276a",
   "metadata": {},
   "source": [
    "## üß† Step 1: The Actor‚ÄìCritic Neural Network\n",
    "\n",
    "In A2C, the policy (actor) and the value function (critic) are implemented using a **single neural network** that shares its initial layers.  \n",
    "This design is simple, efficient, and helps both components learn better features.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1  Shared Feature Representation\n",
    "\n",
    "The first part of the network is a shared feed-forward module that takes in the state \\( s_t \\) and produces a latent feature vector.  \n",
    "This shared representation serves two purposes:\n",
    "\n",
    "- It reduces the total number of parameters  \n",
    "- It allows both actor and critic to learn from the same extracted features  \n",
    "- It stabilizes training, because both heads learn consistent representations of the environment\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2  Policy Head (Actor)\n",
    "\n",
    "The policy head maps the shared features to a vector of **action logits**, which represent the unnormalized preferences for each action.\n",
    "\n",
    "From these logits, a categorical policy \\( \\pi(a|s) \\) is formed.\n",
    "\n",
    "The actor‚Äôs job:\n",
    "\n",
    "- Output a probability distribution over actions  \n",
    "- Encourage exploration through stochastic sampling  \n",
    "- Adjust action probabilities based on advantage values during training  \n",
    "\n",
    "The outputs of the actor determine which action the agent will take at each step.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.3  Value Head (Critic)\n",
    "\n",
    "The critic head maps the shared features to a single scalar value \\( V(s_t) \\).\n",
    "\n",
    "The critic‚Äôs role:\n",
    "\n",
    "- Estimate the expected discounted return from the current state  \n",
    "- Provide the **baseline** used to compute advantages  \n",
    "- Stabilize the actor updates by reducing variance  \n",
    "\n",
    "A good critic makes the policy gradient updates far more sample-efficient.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.4  Why Use a Shared Network?\n",
    "\n",
    "Sharing the lower layers of the network is beneficial because:\n",
    "\n",
    "- The actor and critic often rely on similar state features  \n",
    "- It reduces computational cost and parameter count  \n",
    "- It speeds up training  \n",
    "- It encourages the two components to learn a consistent internal representation  \n",
    "\n",
    "In practice, this shared architecture is widely used in A2C, PPO, IMPALA, and other modern policy-gradient algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa515d-f2b8-4cd0-ae85-b7ea018eb565",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, n_actions):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy_head = nn.Linear(128, n_actions)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, obs_dim)\n",
    "        x = self.shared(x)\n",
    "        logits = self.policy_head(x)           # (batch, n_actions)\n",
    "        value = self.value_head(x).squeeze(-1) # (batch,)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs=probs)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2190f1-aa62-43e5-a696-dcda9a54eb35",
   "metadata": {},
   "source": [
    "# üîç Step 4: Computing n-step Targets, Advantages, and Loss Functions\n",
    "\n",
    "In this section, you will complete the core learning logic of the A2C algorithm.  \n",
    "After collecting **N steps** of rollout data, your task is to compute the necessary learning signals and perform a single update of both the actor and the critic.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1  Compute n-step Bootstrapped Returns\n",
    "\n",
    "You need to compute the n-step return for each timestep in the rollout.  \n",
    "This return should:\n",
    "\n",
    "- Start from the immediate reward,\n",
    "- Discount future rewards by Œ≥,\n",
    "- And optionally bootstrap from the value function of the next state (unless the episode has ended).\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$\n",
    "G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^n V(s_{t+n})\n",
    "$$\n",
    "\n",
    "If the environment ends within the rollout, the bootstrap value should be zero.\n",
    "\n",
    "You should compute these returns **backwards from the last transition**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2  Compute Advantages\n",
    "\n",
    "Once you have the n-step targets, compute the advantage:\n",
    "\n",
    "$$\n",
    "A_t = G_t - V(s_t)\n",
    "$$\n",
    "\n",
    "The value $ V(s_t) $ should come from your critic network, but you should **detach** it to avoid backpropagating through the advantage term.\n",
    "\n",
    "Advantages act as the learning signal for the actor.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3  Actor Loss (Policy Gradient with Baseline)\n",
    "\n",
    "The actor should maximize:\n",
    "\n",
    "$$\n",
    "J_{\\text{actor}} = \\mathbb{E}[ A_t \\log \\pi(a_t|s_t) ]\n",
    "$$\n",
    "\n",
    "In practice, you will compute the negative of this expectation because optimizers perform gradient descent.\n",
    "\n",
    "Your loss should ensure:\n",
    "- Actions with **positive advantage** become more likely,\n",
    "- Actions with **negative advantage** become less likely.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4  Critic Loss (Value Function Regression)\n",
    "\n",
    "The critic should predict the n-step return.  \n",
    "Optimize it by minimizing the squared error:\n",
    "\n",
    "$$\n",
    "J_{\\text{critic}} = (V(s_t) - G_t)^2\n",
    "$$\n",
    "\n",
    "This gives the value network a stable regression target.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.5  Combine Losses and Update the Network\n",
    "\n",
    "Your final loss is a combination of actor and critic losses:\n",
    "\n",
    "$$\n",
    "J = J_{\\text{actor}} + 0.5 \\, J_{\\text{critic}}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "1. Zero out previous gradients  \n",
    "2. Backpropagate through the combined loss  \n",
    "3. Update the model parameters once  \n",
    "4. Clear the rollout buffer  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary of What You Must Implement\n",
    "\n",
    "- Compute **backward n-step targets** with discounting and bootstrap  \n",
    "- Compute **advantages**  \n",
    "- Construct the **actor loss** using policy gradient + baseline  \n",
    "- Construct the **critic loss** using MSE  \n",
    "- Perform a **single gradient update**  \n",
    "- Reset t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd04ae7-d9cb-4e2f-acfa-875476492349",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "ac = ActorCritic(obs_dim, n_actions).to(device)\n",
    "optimizer = optim.Adam(ac.parameters(), lr=LR)\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(1, MAX_EPISODES + 1):\n",
    "\n",
    "    episode_length = 1\n",
    "    \n",
    "    obs, info = env.reset()  # ËÄÅ gym: obs = env.reset()\n",
    "    obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "\n",
    "    obs_buf = []\n",
    "    logprob_buf = []\n",
    "    value_buf = []\n",
    "    reward_buf = []\n",
    "    done_buf = []\n",
    "\n",
    "    while not done:\n",
    "        episode_length+=1\n",
    "        dist, value = ac(obs.unsqueeze(0))  # (1, obs_dim)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = env.step(\n",
    "            int(action.item())\n",
    "        ) \n",
    "        done = terminated or truncated\n",
    "\n",
    "        obs_buf.append(obs)\n",
    "        logprob_buf.append(log_prob)\n",
    "        value_buf.append(value.squeeze(0))\n",
    "        reward_buf.append(reward)\n",
    "        done_buf.append(float(done))  \n",
    "\n",
    "        total_reward += reward\n",
    "        obs = torch.tensor(next_obs, dtype=torch.float32, device=device)\n",
    "\n",
    "        if len(reward_buf) == N_STEPS or done:\n",
    "            with torch.no_grad():\n",
    "                if done:\n",
    "                    next_value = torch.tensor(0.0, device=device)\n",
    "                else:\n",
    "                    _, nv = ac(obs.unsqueeze(0))\n",
    "                    next_value = nv.squeeze(0)\n",
    "\n",
    "            values = torch.stack(value_buf)  # (T,)\n",
    "            log_probs = torch.stack(logprob_buf)  # (T,)\n",
    "            rewards = torch.tensor(\n",
    "                reward_buf, dtype=torch.float32, device=device\n",
    "            )  \n",
    "            dones = torch.tensor(\n",
    "                done_buf, dtype=torch.float32, device=device\n",
    "            ) \n",
    "\n",
    "\n",
    "            '''\n",
    "            Your time to work on it\n",
    "            '''\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            obs_buf.clear()\n",
    "            logprob_buf.clear()\n",
    "            value_buf.clear()\n",
    "            reward_buf.clear()\n",
    "            done_buf.clear()\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "    if episode % PRINT_INTERVAL == 0:\n",
    "        avg_r = np.mean(episode_rewards[-PRINT_INTERVAL:])\n",
    "        print(\n",
    "            f\"Episode {episode:4d} | \"\n",
    "            f\"avg_reward (last {PRINT_INTERVAL}) = {avg_r:.1f} | \"\n",
    "            f\"episode length = {episode_length}\"\n",
    "        )\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8708324f-c325-44b7-ba7f-481a5b7ec6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f5f0df-e1fa-4d1e-98da-c9263ba5b7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71bb08a-a95d-4e93-bf7e-4ea86de7652b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
