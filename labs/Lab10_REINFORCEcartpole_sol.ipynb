{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3db9f95-e915-4710-bffe-e6e47588452b",
   "metadata": {},
   "source": [
    "# üß© Lab 10: REINFORCE on CartPole \n",
    "\n",
    "In the previous labs, we solved the CartPole control task using a Monte Carlo approach to estimate the value function. We discretized the state space, collected full trajectories, computed returns, and used those returns to update a tabular estimate of \\(Q(s,a)\\).\n",
    "\n",
    "In this lab, we will revisit the CartPole environment, but instead of estimating a value function, we will directly learn a **parameterized policy** using a neural network. This approach is known as **policy gradient**. Rather than selecting actions based on a Q-table, the policy network outputs a probability distribution over actions, and we update its parameters so that actions leading to higher returns become more likely.\n",
    "\n",
    "Our goal is to implement **REINFORCE**, one of the simplest policy‚Äêgradient algorithms:\n",
    "- collect full episodes under the current policy,\n",
    "- compute Monte Carlo returns for each time step,\n",
    "- and adjust the policy parameters in the direction that increases the log‚Äêprobability of good actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc3a20d8-5b0c-4dd3-b4e2-9c92e5919e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4520a062-b0e5-4c94-9c53-91b46ebe9a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")   # no need to discretize now\n",
    "obs, info = env.reset(seed=0)\n",
    "obs_dim = env.observation_space.shape[0]  # 4 for CartPole\n",
    "n_actions = env.action_space.n            # 2 for CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666fcc4f-a7a9-4856-ba23-2570f40400af",
   "metadata": {},
   "source": [
    "### Task 1: Define a Policy Network `PolicyNet`\n",
    "\n",
    "In this part, you will implement a small neural network that represents the policy  \n",
    "$\\pi_\\theta(a \\mid s)$ for CartPole.\n",
    "\n",
    "The observation space of `CartPole-v1` is a 4-dimensional vector:\n",
    "- cart position\n",
    "- cart velocity\n",
    "- pole angle\n",
    "- pole angular velocity\n",
    "\n",
    "The action space has 2 discrete actions:\n",
    "- `0`: push cart to the left  \n",
    "- `1`: push cart to the right  \n",
    "\n",
    "We will use a **multi-layer perceptron (MLP)** that:\n",
    "- takes the observation \\(s \\in \\mathbb{R}^4\\) as input,\n",
    "- outputs **logits** over the 2 actions (these will go into a `Categorical` distribution),\n",
    "- uses **two hidden layers** with ReLU activations.\n",
    "\n",
    "Hints for architecture:\n",
    "- Use `nn.Sequential` to stack layers.\n",
    "- A common choice for CartPole is:\n",
    "  - First hidden layer: around 100‚Äì150 units (e.g., `128`).\n",
    "  - Second hidden layer: smaller, e.g., about half of the first layer (e.g., `64`).\n",
    "- The final linear layer should map from the second hidden layer to `n_actions`\n",
    "  (no activation on the output layer; the `Categorical` distribution will handle the softmax internally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6934afe5-ca82-42ca-8229-ff11f170ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Policy network œÄ_Œ∏(a | s) -----\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, obs_dim, n_actions):\n",
    "        super().__init__()\n",
    "        # Your time to work on it\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)  # shape: (batch, n_actions)\n",
    "        return torch.distributions.Categorical(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04832f45-a0ba-4fab-aa26-8acb97a4d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = PolicyNet(obs_dim, n_actions)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)\n",
    "gamma = 0.99\n",
    "num_episodes = 200000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e603189-ca5f-4a7a-8c1c-36a86d896a8a",
   "metadata": {},
   "source": [
    "### Task 2: Implement the REINFORCE Loss Function\n",
    "\n",
    "After collecting one full episode and computing the Monte Carlo returns $G_t$ for\n",
    "each time step, the final step is to update the policy parameters.  \n",
    "In REINFORCE, we adjust the policy in the direction that increases the\n",
    "log-probability of actions that resulted in high returns.\n",
    "\n",
    "Recall the update rule:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\, \\nabla_\\theta \n",
    "\\log \\pi_\\theta(a_t \\mid s_t) \\, G_t.\n",
    "$$\n",
    "\n",
    "In practice, instead of applying this update manually, we construct a **loss\n",
    "function** such that performing gradient descent on the loss produces the same\n",
    "update as gradient ascent on $J(\\theta)$.\n",
    "\n",
    "Your task:\n",
    "\n",
    "1. You have a list of `log_probs`, one for each action taken in the episode.\n",
    "2. You have a list of `returns`, containing the Monte Carlo return $G_t$ for each step.\n",
    "3. Combine them into a single scalar loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008da15-bd0b-4000-8fe0-712609b76453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  100 | Return: 22.0 | Avg(50):  22.8\n",
      "Episode  200 | Return: 24.0 | Avg(50):  24.1\n",
      "Episode  300 | Return: 14.0 | Avg(50):  25.2\n",
      "Episode  400 | Return: 73.0 | Avg(50):  28.0\n",
      "Episode  500 | Return: 18.0 | Avg(50):  29.8\n",
      "Episode  600 | Return: 16.0 | Avg(50):  23.1\n",
      "Episode  700 | Return: 59.0 | Avg(50):  28.4\n",
      "Episode  800 | Return: 20.0 | Avg(50):  32.6\n",
      "Episode  900 | Return: 55.0 | Avg(50):  36.9\n",
      "Episode 1000 | Return: 73.0 | Avg(50):  50.6\n",
      "Episode 1100 | Return: 34.0 | Avg(50):  44.9\n",
      "Episode 1200 | Return: 19.0 | Avg(50):  50.3\n",
      "Episode 1300 | Return: 48.0 | Avg(50):  61.4\n",
      "Episode 1400 | Return: 145.0 | Avg(50):  65.4\n",
      "Episode 1500 | Return: 35.0 | Avg(50):  73.8\n",
      "Episode 1600 | Return: 107.0 | Avg(50):  81.1\n",
      "Episode 1700 | Return: 59.0 | Avg(50):  79.8\n",
      "Episode 1800 | Return: 80.0 | Avg(50):  94.8\n",
      "Episode 1900 | Return: 75.0 | Avg(50): 127.5\n",
      "Episode 2000 | Return: 148.0 | Avg(50): 140.6\n",
      "Episode 2100 | Return: 166.0 | Avg(50): 157.1\n",
      "Episode 2200 | Return: 183.0 | Avg(50): 154.4\n",
      "Episode 2300 | Return: 386.0 | Avg(50): 206.0\n",
      "Episode 2400 | Return: 248.0 | Avg(50): 231.4\n",
      "Episode 2500 | Return: 282.0 | Avg(50): 199.8\n",
      "Episode 2600 | Return: 158.0 | Avg(50): 211.2\n",
      "Episode 2700 | Return: 136.0 | Avg(50): 258.3\n",
      "Episode 2800 | Return: 366.0 | Avg(50): 261.0\n",
      "Episode 2900 | Return: 500.0 | Avg(50): 316.8\n",
      "Episode 3000 | Return: 281.0 | Avg(50): 343.3\n",
      "Episode 3100 | Return: 482.0 | Avg(50): 383.7\n",
      "Episode 3200 | Return: 500.0 | Avg(50): 363.5\n",
      "Episode 3300 | Return: 265.0 | Avg(50): 333.9\n",
      "Episode 3400 | Return: 500.0 | Avg(50): 381.7\n",
      "Episode 3500 | Return: 500.0 | Avg(50): 417.9\n",
      "Episode 3600 | Return: 301.0 | Avg(50): 409.7\n",
      "Episode 3700 | Return: 500.0 | Avg(50): 392.2\n",
      "Episode 3800 | Return: 500.0 | Avg(50): 421.3\n",
      "Episode 3900 | Return: 500.0 | Avg(50): 430.8\n",
      "Episode 4000 | Return: 248.0 | Avg(50): 408.8\n",
      "Episode 4100 | Return: 365.0 | Avg(50): 369.8\n",
      "Episode 4200 | Return: 471.0 | Avg(50): 454.0\n",
      "Episode 4300 | Return: 500.0 | Avg(50): 424.3\n",
      "Episode 4400 | Return: 490.0 | Avg(50): 332.5\n",
      "Episode 4500 | Return: 457.0 | Avg(50): 363.2\n",
      "Episode 4600 | Return: 500.0 | Avg(50): 459.2\n",
      "Episode 4700 | Return: 455.0 | Avg(50): 413.6\n",
      "Episode 4800 | Return: 500.0 | Avg(50): 423.7\n",
      "Episode 4900 | Return: 500.0 | Avg(50): 441.2\n",
      "Episode 5000 | Return: 461.0 | Avg(50): 407.3\n",
      "Episode 5100 | Return: 500.0 | Avg(50): 464.4\n",
      "Episode 5200 | Return: 190.0 | Avg(50): 456.7\n",
      "Episode 5300 | Return: 500.0 | Avg(50): 449.6\n",
      "Episode 5400 | Return: 500.0 | Avg(50): 455.8\n",
      "Episode 5500 | Return: 500.0 | Avg(50): 463.3\n",
      "Episode 5600 | Return: 500.0 | Avg(50): 455.3\n",
      "Episode 5700 | Return: 500.0 | Avg(50): 490.4\n",
      "Episode 5800 | Return: 500.0 | Avg(50): 488.8\n",
      "Episode 5900 | Return: 500.0 | Avg(50): 464.1\n",
      "Episode 6000 | Return: 500.0 | Avg(50): 482.0\n",
      "Episode 6100 | Return: 335.0 | Avg(50): 482.2\n",
      "Episode 6200 | Return: 414.0 | Avg(50): 469.8\n",
      "Episode 6300 | Return: 153.0 | Avg(50): 455.9\n",
      "Episode 6400 | Return: 500.0 | Avg(50): 475.6\n",
      "Episode 6500 | Return: 372.0 | Avg(50): 452.5\n",
      "Episode 6600 | Return: 500.0 | Avg(50): 476.8\n",
      "Episode 6700 | Return: 500.0 | Avg(50): 431.2\n",
      "Episode 6800 | Return: 500.0 | Avg(50): 447.7\n",
      "Episode 6900 | Return: 500.0 | Avg(50): 491.6\n",
      "Episode 7000 | Return: 500.0 | Avg(50): 456.0\n",
      "Episode 7100 | Return: 472.0 | Avg(50): 490.3\n",
      "Episode 7200 | Return: 500.0 | Avg(50): 491.3\n",
      "Episode 7300 | Return: 500.0 | Avg(50): 475.3\n",
      "Episode 7400 | Return: 500.0 | Avg(50): 484.4\n",
      "Episode 7500 | Return: 500.0 | Avg(50): 484.3\n",
      "Episode 7600 | Return: 500.0 | Avg(50): 486.2\n",
      "Episode 7700 | Return: 500.0 | Avg(50): 477.4\n",
      "Episode 7800 | Return: 500.0 | Avg(50): 496.6\n",
      "Episode 7900 | Return: 500.0 | Avg(50): 496.0\n",
      "Episode 8000 | Return: 500.0 | Avg(50): 495.4\n",
      "Episode 8100 | Return: 500.0 | Avg(50): 493.0\n",
      "Episode 8200 | Return: 500.0 | Avg(50): 486.3\n",
      "Episode 8300 | Return: 500.0 | Avg(50): 469.3\n",
      "Episode 8400 | Return: 500.0 | Avg(50): 477.8\n",
      "Episode 8500 | Return: 500.0 | Avg(50): 471.9\n",
      "Episode 8600 | Return: 500.0 | Avg(50): 482.0\n",
      "Episode 8700 | Return: 500.0 | Avg(50): 491.8\n",
      "Episode 8800 | Return: 500.0 | Avg(50): 485.8\n",
      "Episode 8900 | Return: 500.0 | Avg(50): 470.7\n",
      "Episode 9000 | Return: 500.0 | Avg(50): 481.2\n",
      "Episode 9100 | Return: 221.0 | Avg(50): 457.1\n",
      "Episode 9200 | Return: 500.0 | Avg(50): 472.7\n",
      "Episode 9300 | Return: 500.0 | Avg(50): 477.1\n",
      "Episode 9400 | Return: 500.0 | Avg(50): 482.5\n",
      "Episode 9500 | Return: 500.0 | Avg(50): 469.8\n",
      "Episode 9600 | Return: 500.0 | Avg(50): 480.5\n",
      "Episode 9700 | Return: 500.0 | Avg(50): 487.9\n",
      "Episode 9800 | Return: 500.0 | Avg(50): 489.6\n",
      "Episode 9900 | Return: 500.0 | Avg(50): 487.8\n",
      "Episode 10000 | Return: 208.0 | Avg(50): 460.5\n",
      "Episode 10100 | Return: 500.0 | Avg(50): 488.0\n",
      "Episode 10200 | Return: 500.0 | Avg(50): 494.1\n",
      "Episode 10300 | Return: 500.0 | Avg(50): 499.2\n",
      "Episode 10400 | Return: 500.0 | Avg(50): 490.7\n",
      "Episode 10500 | Return: 500.0 | Avg(50): 491.5\n",
      "Episode 10600 | Return: 500.0 | Avg(50): 484.5\n",
      "Episode 10700 | Return: 492.0 | Avg(50): 446.3\n",
      "Episode 10800 | Return: 402.0 | Avg(50): 487.4\n",
      "Episode 10900 | Return: 500.0 | Avg(50): 483.5\n",
      "Episode 11000 | Return: 500.0 | Avg(50): 475.5\n",
      "Episode 11100 | Return: 500.0 | Avg(50): 480.0\n",
      "Episode 11200 | Return: 248.0 | Avg(50): 489.5\n",
      "Episode 11300 | Return: 500.0 | Avg(50): 483.6\n",
      "Episode 11400 | Return: 500.0 | Avg(50): 480.8\n",
      "Episode 11500 | Return: 500.0 | Avg(50): 491.4\n",
      "Episode 11600 | Return: 500.0 | Avg(50): 499.4\n",
      "Episode 11700 | Return: 500.0 | Avg(50): 493.7\n",
      "Episode 11800 | Return: 500.0 | Avg(50): 477.9\n",
      "Episode 11900 | Return: 500.0 | Avg(50): 497.5\n",
      "Episode 12000 | Return: 500.0 | Avg(50): 487.5\n",
      "Episode 12100 | Return: 500.0 | Avg(50): 493.1\n",
      "Episode 12200 | Return: 500.0 | Avg(50): 500.0\n",
      "Episode 12300 | Return: 500.0 | Avg(50): 484.6\n",
      "Episode 12400 | Return: 500.0 | Avg(50): 479.4\n",
      "Episode 12500 | Return: 500.0 | Avg(50): 478.7\n",
      "Episode 12600 | Return: 500.0 | Avg(50): 491.2\n",
      "Episode 12700 | Return: 500.0 | Avg(50): 478.8\n",
      "Episode 12800 | Return: 500.0 | Avg(50): 498.0\n",
      "Episode 12900 | Return: 500.0 | Avg(50): 485.7\n",
      "Episode 13000 | Return: 500.0 | Avg(50): 498.3\n",
      "Episode 13100 | Return: 500.0 | Avg(50): 477.3\n",
      "Episode 13200 | Return: 500.0 | Avg(50): 463.9\n",
      "Episode 13300 | Return: 500.0 | Avg(50): 497.6\n",
      "Episode 13400 | Return: 500.0 | Avg(50): 500.0\n",
      "Episode 13500 | Return: 500.0 | Avg(50): 489.6\n",
      "Episode 13600 | Return: 500.0 | Avg(50): 481.2\n",
      "Episode 13700 | Return: 500.0 | Avg(50): 489.6\n",
      "Episode 13800 | Return: 338.0 | Avg(50): 472.1\n"
     ]
    }
   ],
   "source": [
    "returns_history = []\n",
    "\n",
    "for ep in range(1, num_episodes + 1):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    log_probs = []   # store log œÄ_Œ∏(a_t | s_t)\n",
    "    rewards = []     # store r_t\n",
    "\n",
    "    # Generate an episode\n",
    "    while not done:\n",
    "        s_t = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)   # shape (1, obs_dim)\n",
    "        dist = policy(s_t)                                          # œÄ_Œ∏(. | s_t)\n",
    "        a_t = dist.sample()                                         # sample action\n",
    "        log_prob_t = dist.log_prob(a_t)                             # log œÄ_Œ∏(a_t | s_t)\n",
    "\n",
    "        obs_next, r, term, trunc, _ = env.step(a_t.item())\n",
    "        done = term or trunc\n",
    "\n",
    "        log_probs.append(log_prob_t)\n",
    "        rewards.append(r)\n",
    "\n",
    "        obs = obs_next\n",
    "\n",
    "    #  Value update\n",
    "    T = len(rewards)\n",
    "    G = 0.0\n",
    "    returns = []\n",
    "\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    log_probs = torch.stack(log_probs)      # shape (T,)\n",
    "\n",
    "    # Your time to work on it\n",
    "    loss = -(log_probs * returns).sum()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    ep_return = sum(rewards)\n",
    "    returns_history.append(ep_return)\n",
    "\n",
    "    if ep % 100 == 0:\n",
    "        avg = np.mean(returns_history[-50:])\n",
    "        print(f\"Episode {ep:4d} | Return: {ep_return:4.1f} | \"\n",
    "              f\"Avg(50): {avg:5.1f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7addcab-5c31-4644-af1a-bc01dc957a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86683be-8318-4650-864d-553512bbc8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5640ae4f-fa4a-4938-97ac-a12b946a7122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
