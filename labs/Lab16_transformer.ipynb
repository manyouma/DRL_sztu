{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae1214d-4c22-4796-8fce-fc71e6553ceb",
   "metadata": {},
   "source": [
    "## Lab 16 ‚Äî Transformer ‚ÄúSanity Tests‚Äù \n",
    "\n",
    "In this lab, we will use a real open-source Transformer (via **HuggingFace Transformers**) as a black box and run a set of **basic tests** to understand what is happening inside a modern LLM pipeline. Instead of training from scratch, we focus on **observability**: tokenization behavior, embedding geometry, and next-token prediction.\n",
    "\n",
    "You will:\n",
    "- Load a pretrained tokenizer + causal language model (e.g., **Qwen2.5**).\n",
    "- Inspect **tokenization outputs** (tokens, token IDs, and how spaces are handled).\n",
    "- Extract **input embeddings** and run a small **embedding similarity** experiment across words from different semantic categories.\n",
    "- Implement a simple generation loop to compare **greedy decoding** vs **sampling with temperature**.\n",
    "- Probe the model by printing the **top-k next token probabilities** for a given prompt, and interpret what those probabilities mean.\n",
    "\n",
    "By the end, you should be able to explain:  \n",
    "(1) why tokenization details matter, (2) what embeddings represent geometrically, and (3) how a Transformer turns a prompt into a probability distribution over the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca227f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14daf861-62a4-4921-988f-aac8fabfa0b2",
   "metadata": {},
   "source": [
    "## Obtaining a DeepSeek API Key\n",
    "\n",
    "To call the DeepSeek API, you first need to obtain a personal **API key**.\n",
    "\n",
    "1. Visit the DeepSeek official website:  \n",
    "   https://platform.deepseek.com/\n",
    "\n",
    "2. Sign up for an account (or log in if you already have one).\n",
    "\n",
    "3. Go to the **API / Developer** section of the dashboard.\n",
    "\n",
    "4. Create a new API key and copy it.\n",
    "\n",
    "5. Paste the key into your notebook or script:\n",
    "   ```python\n",
    "   api_key = \"YOUR_API_KEY_HERE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03785eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\"\n",
    "url = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df67901",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\" \n",
    "#model_name = \"Qwen/Qwen2.5-0.5B\"          \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "#model = AutoModel.from_pretrained(model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f555ede-6096-47b9-a1d2-08a59e74ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698e0eaa-bc6b-4cf3-a566-4e8f55ca65ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c85d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Tokenizer:\", type(tokenizer).__name__)\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Special token:\")\n",
    "print(\" - CLS:\", tokenizer.cls_token)\n",
    "print(\" - SEP:\", tokenizer.sep_token) \n",
    "print(\" - PAD:\", tokenizer.pad_token)\n",
    "print(\" - UNK:\", tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = \"All happy families are alike; each unhappy family is unhappy in its own way.\"\n",
    "tokens = tokenizer.tokenize(test_texts)\n",
    "token_ids = tokenizer.encode(test_texts)\n",
    "print(f\"\\nOriginal: {test_texts}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids[:30]}...\")\n",
    "print(f\"toke num: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1fca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_space_handling():\n",
    "    examples = [\n",
    "        \"hello world\",  \n",
    "        \"hello  world\", \n",
    "        \"hello\",         \n",
    "        \" hello\",     \n",
    "    ]\n",
    "    \n",
    "    for text in examples:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        print(f\"'{text}' ‚Üí {tokens}\")\n",
    "        for token in tokens:\n",
    "            if 'ƒ†' in token:\n",
    "                print(f\" Note that  '{token}' has a space before it\")\n",
    "                \n",
    "explain_space_handling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4481c74-8459-4929-b041-fce2c76820da",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.get_input_embeddings()\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b56c26b-321e-45c0-8830-89d0a58c763a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "def get_word_embedding(word, model, tokenizer):\n",
    "    embedding_layer = model.get_input_embeddings()\n",
    "    token_ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "\n",
    "    token_ids = torch.tensor(token_ids, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = embedding_layer(token_ids)   # [num_tokens, dim]\n",
    "        return embeddings.mean(dim=0)            \n",
    "\n",
    "def cosine_similarity_torch(vec1, vec2):\n",
    "    return F.cosine_similarity(\n",
    "        vec1.unsqueeze(0), \n",
    "        vec2.unsqueeze(0), \n",
    "        dim=1\n",
    "    ).item()\n",
    "\n",
    "words = [\"cat\", \"dog\", \"lion\", \"wind\", \"rain\", \"snow\", \"run\", \"walk\", \"jump\"]\n",
    "\n",
    "print(\"üîç Similarity matrix\")\n",
    "print(\"=\" * 120)\n",
    "print(\" \" * 12 + \"\".join([f\"{word:>10}\" for word in words]))\n",
    "\n",
    "\n",
    "word_embeddings = {\n",
    "    word: get_word_embedding(word, model, tokenizer)\n",
    "    for word in words\n",
    "}\n",
    "\n",
    "for word1 in words:\n",
    "    print(f\"{word1:>12}: \", end=\"\")\n",
    "    for word2 in words:\n",
    "        sim = cosine_similarity_torch(\n",
    "            word_embeddings[word1],\n",
    "            word_embeddings[word2]\n",
    "        )\n",
    "\n",
    "        if sim > 0.2:\n",
    "            print(f\"\\033[92m{sim:>10.3f}\\033[0m\", end=\"\")  \n",
    "        elif sim < 0.1:\n",
    "            print(f\"\\033[91m{sim:>10.3f}\\033[0m\", end=\"\")  \n",
    "        else:\n",
    "            print(f\"{sim:>10.3f}\", end=\"\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53015f3-603c-4f6a-afff-ec40b5cd2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_generate(prompt, model, tokenizer, max_new_tokens=50, entropy=0.5):\n",
    "\n",
    "    device = model.device\n",
    "    \n",
    "    print(f\"Input: '{prompt}'\")\n",
    "    print(f\"Entropy: {entropy} \")\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs.input_ids\n",
    "    \n",
    "    generated_tokens = []\n",
    "    \n",
    "    for i in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            \n",
    "\n",
    "            if hasattr(outputs, 'logits'):\n",
    "                logits = outputs.logits\n",
    "            else:\n",
    "                logits = outputs.last_hidden_state\n",
    "            \n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "\n",
    "            if entropy == 0.0:\n",
    "                next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "                strategy = \"Greedy\"\n",
    "                \n",
    "            else:\n",
    "\n",
    "                temperature = 0.1 + entropy * 1.9 \n",
    "                next_token_logits = next_token_logits / temperature\n",
    "                probs = torch.softmax(next_token_logits, dim=-1)\n",
    "                next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "                strategy = f\"temperature(t={temperature:.1f})\"\n",
    "            \n",
    "            new_token = tokenizer.decode(next_token_id[0], skip_special_tokens=True)\n",
    "            generated_tokens.append(new_token)\n",
    "            \n",
    "            print(f\"Token {i+1}: '{new_token}' ({strategy})\")\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "            \n",
    "            # ÂÅúÊ≠¢Êù°‰ª∂\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "            if new_token in ['\\n', '.', '!', '?', '„ÄÇ', 'ÔºÅ', 'Ôºü']:\n",
    "                break\n",
    "    \n",
    "    generated_text = prompt + ''.join(generated_tokens)\n",
    "    print(f\"Test: {generated_text}\")\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe833f5-6fc9-4243-b36b-e95b1a11a4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question, model, tokenizer, max_answer_tokens=50, entropy=0.5):\n",
    "    \n",
    "\n",
    "    prompt = f\"QuestionÔºö{question}\\n\"\n",
    "    print(f\"ü§î question: {question}\")\n",
    "\n",
    "    full_response = robust_generate(prompt, model, tokenizer, max_new_tokens=max_answer_tokens, entropy=0.5)\n",
    "    answer = full_response.replace(prompt, \"\").strip()\n",
    "    \n",
    "    print(f\"\\nüéØ Final answer: {answer}\")\n",
    "    print(f\"üìä Length: {len(answer)} characters\")\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a839c5-f173-44ef-85b5-e4b589b47ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Â≠îÂ≠êÊòØË∞Å\"\n",
    "answer = ask_question(question, model, tokenizer, max_answer_tokens=50, entropy=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d3f475-6749-4e5e-bcab-b16885ebec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_next_token_probabilities(prompt, model, tokenizer, top_k=20):\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        if hasattr(outputs, 'logits'):\n",
    "            logits = outputs.logits\n",
    "        else:\n",
    "            logits = outputs.last_hidden_state\n",
    "        \n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        \n",
    "        # Calculate probability\n",
    "        probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        \n",
    "        # Obtain the top_k most likely token\n",
    "        top_probs, top_indices = torch.topk(probs, top_k)\n",
    "        \n",
    "        print(f\"üîç Promppt: '{prompt}'\")\n",
    "        print(f\"üìä Top {top_k} predictions for the next token:\\n\")\n",
    "        \n",
    "        for i, (prob, idx) in enumerate(zip(top_probs[0], top_indices[0])):\n",
    "            token_text = tokenizer.decode([idx])\n",
    "\n",
    "            display_text = repr(token_text)[1:-1]  # ÂéªÊéâÂºïÂè∑\n",
    "            \n",
    "            print(f\"{i+1:2d}. '{display_text:10s}' (ID: {idx:5d}) - Ê¶ÇÁéá: {prob.item():.4f}\")\n",
    "\n",
    "\n",
    "current_prompt = \"ÈóÆÈ¢òÔºöÂ≠îÂ≠êÊòØË∞Å\\nÂ≠îÂ≠êÊòØÊàëÂõΩÂè§‰ª£ÁöÑÂ§ßÊÄùÊÉ≥ÂÆ∂Ôºå\"\n",
    "show_next_token_probabilities(current_prompt, model, tokenizer, top_k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c5c085-53f4-43e3-abc5-dbf6b9cd1268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70672c-6f34-41a3-bc64-c4a5cbfb372c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa39524-9f94-40d7-b0ec-0cdc9132873d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
