{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a971857-7f18-444a-af38-af4d0bab0430",
   "metadata": {},
   "source": [
    "# Lab 14: Soft Actor-Critic (SAC) on HalfCheetah-v4\n",
    "\n",
    "In this lab, we will move from **TD3 (deterministic policy)** to **Soft Actor-Critic (SAC)**, which is one of the most important **modern off-policy stochastic actor-critic algorithms**.\n",
    "\n",
    "Unlike TD3, SAC does **not** learn a single deterministic action. Instead, it learns a **probability distribution over actions**, and explicitly encourages **exploration via entropy maximization**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c615dd-8837-4a8b-bb35-14a694aad828",
   "metadata": {},
   "source": [
    "## 1. Key Idea of SAC\n",
    "\n",
    "SAC optimizes the following objective:\n",
    "\n",
    "$$\n",
    "\\max_\\pi \\; \\mathbb{E}\\left[ \\sum_t r(s_t, a_t) + \\alpha \\mathcal{H}(\\pi(\\cdot | s_t)) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ r(s, a)$: task reward\n",
    "- $ \\mathcal{H}(\\pi) $: entropy of the policy\n",
    "- $ \\alpha $: temperature coefficient (controls exploration strength)\n",
    "\n",
    "✅ High entropy ⇒ more exploration  \n",
    "✅ Low entropy ⇒ more exploitation  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaab26a-0d63-4c12-a8e2-29ca13f795c9",
   "metadata": {},
   "source": [
    "## . What Changes Compared to TD3?\n",
    "\n",
    "| Component | TD3 | SAC |\n",
    "|----------|------|------|\n",
    "| Policy | Deterministic | Stochastic (Gaussian) |\n",
    "| Critics | 2 Q networks | 2 Q networks |\n",
    "| Entropy term | ❌ No | ✅ Yes |\n",
    "| Exploration | Added noise manually | Built-in via entropy |\n",
    "| Target | Max Q-value | Soft Q-value |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ed2e19-1d92-4cf0-abb5-6bf637c14903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bca5de1-1caa-47bd-8638-6b270d673ae0",
   "metadata": {},
   "source": [
    "## 1. Gaussian Policy in Soft Actor-Critic (SAC)\n",
    "\n",
    "Unlike TD3, which uses a **deterministic policy**  \n",
    "$$\n",
    "a = \\pi_\\theta(s)\n",
    "$$\n",
    "SAC uses a **stochastic policy**, where actions are sampled from a **Gaussian distribution**:\n",
    "\n",
    "$$\n",
    "a \\sim \\mathcal{N}(\\mu_\\theta(s), \\sigma_\\theta(s))\n",
    "$$\n",
    "\n",
    "This means the policy does not output a single fixed action, but a **probability distribution over actions**.\n",
    "\n",
    "For each state $s$, the policy network outputs two vectors:\n",
    "\n",
    "- **Mean**: $\\mu_\\theta(s)$\n",
    "- **Log standard deviation**: $\\log \\sigma_\\theta(s) $\n",
    "\n",
    "So the policy represents the distribution:\n",
    "\n",
    "$$\n",
    "\\pi(a|s) = \\mathcal{N}(\\mu_\\theta(s), \\sigma_\\theta(s))\n",
    "$$\n",
    "\n",
    "The log standard deviation is clipped to avoid:\n",
    "- extremely large variance\n",
    "- numerical instability\n",
    "- exploding gradients\n",
    "\n",
    "To enable backpropagation through a random sample, SAC uses the **reparameterization trick**:\n",
    "\n",
    "$$\n",
    "u = \\mu_\\theta(s) + \\sigma_\\theta(s) \\cdot \\epsilon,\n",
    "\\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "This transforms a random sampling process into a **deterministic function with random noise**, allowing gradients to flow through the policy parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c03c0e-e0e2-45d6-b5f2-167341557631",
   "metadata": {},
   "source": [
    "Most continuous-control environments require bounded actions:\n",
    "\n",
    "$$\n",
    "a \\in [-1, 1]\n",
    "$$\n",
    "\n",
    "So the sampled value $ u $ is passed through:\n",
    "\n",
    "$$\n",
    "a = \\tanh(u)\n",
    "$$\n",
    "\n",
    "and then rescaled:\n",
    "\n",
    "$$\n",
    "a_{\\text{env}} = a \\cdot a_{\\max}\n",
    "$$\n",
    "\n",
    "This guarantees that all actions sent to the environment are valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f85bfa5-7499-4f08-97d7-92e15ead7646",
   "metadata": {},
   "source": [
    "Because the action is transformed using `tanh`, the log-probability must be corrected using the **change-of-variables rule**:\n",
    "\n",
    "$$\n",
    "\\log \\pi(a|s)\n",
    "=\n",
    "\\log \\mathcal{N}(u; \\mu, \\sigma)\n",
    "-\n",
    "\\log(1 - \\tanh^2(u))\n",
    "$$\n",
    "\n",
    "This corrected log-probability is **essential** for:\n",
    "\n",
    "- The critic target update\n",
    "- The actor (policy) update\n",
    "- The temperature parameter $ \\alpha $ update\n",
    "\n",
    "Without this correction, the entropy estimate would be wrong and training would become unstable.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "During evaluation, SAC does **not use random sampling**.  \n",
    "Instead, it uses the **mean action**:\n",
    "\n",
    "$$\n",
    "a_{\\text{eval}} = \\tanh(\\mu_\\theta(s)) \\cdot a_{\\max}\n",
    "$$\n",
    "\n",
    "This ensures:\n",
    "- stable test performance\n",
    "- reproducible results\n",
    "- no randomness during evaluation\n",
    "\n",
    "---\n",
    "\n",
    "####  What the Gaussian Policy Produces\n",
    "\n",
    "For each input state \\( s \\), the Gaussian policy provides:\n",
    "\n",
    "| Quantity | Mathematical Meaning | Purpose |\n",
    "|----------|------------------------|---------|\n",
    "| $ a $ | sampled action | environment interaction |\n",
    "| $ \\log \\pi(a|s) $ | log-probability | critic target, actor loss, α update |\n",
    "| $ \\tanh(\\mu) $ | mean (deterministic) action | evaluation |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b764a9-8828-4b30-aa6f-de8a5f161149",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int, max_action: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "\n",
    "        self.mean_layer = nn.Linear(256, action_dim)\n",
    "        self.log_std_layer = nn.Linear(256, action_dim)\n",
    "\n",
    "        # For clamping log_std\n",
    "        self.LOG_STD_MIN = -20\n",
    "        self.LOG_STD_MAX = 2\n",
    "\n",
    "    def forward(self, state: torch.Tensor):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        mean = self.mean_layer(x)\n",
    "        log_std = self.log_std_layer(x)\n",
    "        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, state: torch.Tensor):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        normal = Normal(mean, std)\n",
    "        u = normal.rsample()\n",
    "\n",
    "        tanh_u = torch.tanh(u)\n",
    "        action = tanh_u * self.max_action\n",
    "\n",
    "        log_prob = normal.log_prob(u)\n",
    "        \n",
    "        epsilon = 1e-6\n",
    "        log_prob -= torch.log(1 - tanh_u.pow(2) + epsilon)\n",
    "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        mean_action = torch.tanh(mean) * self.max_action\n",
    "\n",
    "        return action, log_prob, mean_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6553b106-21cb-4fb4-a65a-40f640fc6f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.q1_fc1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.q1_fc2 = nn.Linear(256, 256)\n",
    "        self.q1_out = nn.Linear(256, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.q2_fc1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.q2_fc2 = nn.Linear(256, 256)\n",
    "        self.q2_out = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor):\n",
    "        xu = torch.cat([state, action], dim=-1)  # (batch, state_dim + action_dim)\n",
    "\n",
    "        # Q1\n",
    "        x1 = F.relu(self.q1_fc1(xu))\n",
    "        x1 = F.relu(self.q1_fc2(x1))\n",
    "        q1 = self.q1_out(x1)\n",
    "\n",
    "        # Q2\n",
    "        x2 = F.relu(self.q2_fc1(xu))\n",
    "        x2 = F.relu(self.q2_fc2(x2))\n",
    "        q2 = self.q2_out(x2)\n",
    "\n",
    "        return q1, q2\n",
    "\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, size):\n",
    "        self.size = size\n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "\n",
    "        self.states = np.zeros((size, state_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros((size, action_dim), dtype=np.float32)\n",
    "        self.rewards = np.zeros((size,), dtype=np.float32)\n",
    "        self.next_states = np.zeros((size, state_dim), dtype=np.float32)\n",
    "        self.dones = np.zeros((size,), dtype=np.float32)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.states[self.ptr] = state\n",
    "        self.actions[self.ptr] = action\n",
    "        self.rewards[self.ptr] = reward\n",
    "        self.next_states[self.ptr] = next_state\n",
    "        self.dones[self.ptr] = done\n",
    "\n",
    "        self.ptr += 1\n",
    "        if self.ptr >= self.size:\n",
    "            self.ptr = 0\n",
    "            self.full = True\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size if self.full else self.ptr\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        max_size = self.size if self.full else self.ptr\n",
    "        idx = np.random.randint(0, max_size, size=batch_size)\n",
    "\n",
    "        states = torch.as_tensor(self.states[idx], dtype=torch.float32, device=device)\n",
    "        actions = torch.as_tensor(self.actions[idx], dtype=torch.float32, device=device)\n",
    "        rewards = torch.as_tensor(self.rewards[idx], dtype=torch.float32, device=device).unsqueeze(-1)\n",
    "        next_states = torch.as_tensor(self.next_states[idx], dtype=torch.float32, device=device)\n",
    "        dones = torch.as_tensor(self.dones[idx], dtype=torch.float32, device=device).unsqueeze(-1)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "\n",
    "def soft_update(source: nn.Module, target: nn.Module, tau: float):\n",
    "    with torch.no_grad():\n",
    "        for param, target_param in zip(source.parameters(), target.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32130f34-10ca-41f2-a655-e83c87b8a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"HalfCheetah-v4\"\n",
    "seed = 0\n",
    "\n",
    "# ==========================\n",
    "#  Hyperparameters\n",
    "# ==========================\n",
    "replay_size = int(1e6)\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "lr = 3e-4\n",
    "batch_size = 256\n",
    "\n",
    "# Number of steps purely using random actions at the beginning\n",
    "start_steps = 10_000\n",
    "# Total environment interaction steps\n",
    "max_steps = 300_000\n",
    "# Start learning only after some initial experience\n",
    "update_after = 1_000\n",
    "# Number of gradient updates per environment step\n",
    "update_every = 1\n",
    "# Interval to run evaluation\n",
    "eval_interval = 5_000\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9626cdf3-5a2d-4cc1-a515-0f2ba1b584f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dim: 17\n",
      "Action dim: 6\n",
      "Max action: 1.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.reset(seed=seed)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "print(\"State dim:\", state_dim)\n",
    "print(\"Action dim:\", action_dim)\n",
    "print(\"Max action:\", max_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c60005c-ef27-4a5f-badf-1bb762c3a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = GaussianPolicy(state_dim, action_dim, max_action).to(device)\n",
    "critic = Critic(state_dim, action_dim).to(device)\n",
    "critic_target = Critic(state_dim, action_dim).to(device)\n",
    "\n",
    "critic_target.load_state_dict(critic.state_dict())\n",
    "\n",
    "policy_optimizer = torch.optim.Adam(policy.parameters(), lr=lr)\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=lr)\n",
    "\n",
    "log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "alpha_optimizer = torch.optim.Adam([log_alpha], lr=lr)\n",
    "\n",
    "target_entropy = -float(action_dim)\n",
    "\n",
    "replay_buffer = ReplayBuffer(state_dim, action_dim, replay_size)\n",
    "\n",
    "state, _ = env.reset(seed=seed)\n",
    "episode_return = 0.0\n",
    "episode_length = 0\n",
    "episode_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c6ed7a2-3c63-436a-8f57-6bba85230ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000 | Episode 0 | Return: -415.3 | Length: 1000\n",
      "Step 2000 | Episode 1 | Return: -203.5 | Length: 1000\n",
      "Step 3000 | Episode 2 | Return: -408.0 | Length: 1000\n",
      "Step 4000 | Episode 3 | Return: -330.8 | Length: 1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 61\u001b[0m\n\u001b[0;32m     57\u001b[0m critic_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     59\u001b[0m new_actions, log_pi, _ \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39msample(states)\n\u001b[1;32m---> 61\u001b[0m q1_new, q2_new \u001b[38;5;241m=\u001b[39m \u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m q_new \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(q1_new, q2_new)\n\u001b[0;32m     64\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m (alpha\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m*\u001b[39m log_pi \u001b[38;5;241m-\u001b[39m q_new)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32m~\\.conda\\envs\\atari\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\atari\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mCritic.forward\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Q1\u001b[39;00m\n\u001b[0;32m     19\u001b[0m x1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq1_fc1(xu))\n\u001b[1;32m---> 20\u001b[0m x1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq1_fc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m q1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq1_out(x1)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Q2\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\atari\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\atari\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\atari\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(1, max_steps + 1):\n",
    "\n",
    "    if t < start_steps:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        state_tensor = torch.as_tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            sampled_action, _, _ = policy.sample(state_tensor)\n",
    "        action = sampled_action.cpu().numpy()[0]\n",
    "\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    episode_return += reward\n",
    "    episode_length += 1\n",
    "\n",
    "    replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        print(\n",
    "            f\"Step {t} | \"\n",
    "            f\"Episode {episode_idx} | \"\n",
    "            f\"Return: {episode_return:.1f} | \"\n",
    "            f\"Length: {episode_length}\"\n",
    "        )\n",
    "        state, _ = env.reset()\n",
    "        episode_return = 0.0\n",
    "        episode_length = 0\n",
    "        episode_idx += 1\n",
    "\n",
    "\n",
    "    if t >= update_after and len(replay_buffer) >= batch_size:\n",
    "        for _ in range(update_every):\n",
    "            \n",
    "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size, device)\n",
    "\n",
    "            alpha = log_alpha.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_actions, next_log_pi, _ = policy.sample(next_states)\n",
    "\n",
    "                q1_next_target, q2_next_target = critic_target(next_states, next_actions)\n",
    "                q_next_target = torch.min(q1_next_target, q2_next_target)\n",
    "\n",
    "                target_q = rewards + (1.0 - dones) * gamma * (\n",
    "                    q_next_target - alpha.detach() * next_log_pi\n",
    "                )\n",
    "\n",
    "            q1_pred, q2_pred = critic(states, actions)\n",
    "\n",
    "            critic_loss = F.mse_loss(q1_pred, target_q) + F.mse_loss(q2_pred, target_q)\n",
    "\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            new_actions, log_pi, _ = policy.sample(states)\n",
    "\n",
    "            q1_new, q2_new = critic(states, new_actions)\n",
    "            q_new = torch.min(q1_new, q2_new)\n",
    "\n",
    "            actor_loss = (alpha.detach() * log_pi - q_new).mean()\n",
    "\n",
    "            policy_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "\n",
    "            alpha_loss = -(log_alpha * (log_pi + target_entropy).detach()).mean()\n",
    "\n",
    "            alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            alpha_optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for param, target_param in zip(critic.parameters(), critic_target.parameters()):\n",
    "                    target_param.data.copy_(\n",
    "                        tau * param.data + (1.0 - tau) * target_param.data\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca8cad3-9426-40d4-a27f-f97ec772ebd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e89ce-698f-4927-a9c0-ad80f0aae48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5088d86d-95b7-431d-bcd2-b95b07ad514f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e988b37b-8e84-46c8-bcdd-cad46168a133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
