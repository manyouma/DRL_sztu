{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839c9bab",
   "metadata": {},
   "source": [
    "# üßä Lab 7: Frozen Lake Revisited and Farewell\n",
    "\n",
    "In this lab, we **return once more to the Frozen Lake** ‚Äî now larger and more mysterious than before.  \n",
    "This lab is designed to **tie everything together** through a focused review and practical reflection.\n",
    "\n",
    "\n",
    "## üéØ Learning Goals\n",
    "\n",
    "- Revisit the **core ideas** of **Value Iteration (VI)** and **Policy Iteration (PI)**.  \n",
    "- Understand how **Monte Carlo** and **Temporal-Difference (TD)** methods extend the same foundation to unknown environments.  \n",
    "- Gain intuition for how **model-based** and **model-free** reinforcement learning relate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b7dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "import random\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "from util_frozen import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3214c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFF\n",
      "FFHFF\n",
      "FHHFF\n",
      "FHHFF\n",
      "FFFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a smaller 3x3 map\n",
    "DESC_5x5 = [\n",
    "    \"SFFFF\",\n",
    "    \"FFHFF\",\n",
    "    \"FHHFF\",\n",
    "    \"FHHFF\",\n",
    "    \"FFFFG\",\n",
    "]\n",
    "env = gym.make(\"FrozenLake-v1\",desc=DESC_5x5,is_slippery=True, render_mode=\"ansi\")\n",
    "obs, info = env.reset(seed=42)\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5402244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the state transition matrix\n",
    "P, R, absorbing, shape2d, flatmap = build_frozenlake_transitions(DESC_5x5, is_slippery=True)\n",
    "T_per_action = [P[:, a, :] for a in range(4)] \n",
    "P_all = np.array([T_per_action[0], T_per_action[1], T_per_action[2], T_per_action[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92cf4e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 25\n",
    "Reward = np.zeros((n_states,), dtype = int) \n",
    "Reward[-1] = 1\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fbcca0",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Part 1 ‚Äì Revisiting the Lake\n",
    "\n",
    "First we re-implement the **value iteration** approach, which combines evaluation and \n",
    "improvement into a single update rule:\n",
    "\n",
    "$$\n",
    "V_{k+1}(s) \\;=\\; \\max_a \\Big[ R(s) + \\gamma \\sum_{s'} P(s'|s,a) \\, V_k(s') \\Big].\n",
    "$$\n",
    "\n",
    "Once the value function converges, we extract the optimal policy by choosing, \n",
    "in each state, the action that achieves the maximum:\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\arg\\max_a \\Big[ R(s) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s') \\Big].\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97536865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the original value \n",
    "V_0 = np.zeros_like(Reward, dtype=float)\n",
    "V_new = np.zeros_like(V_0, dtype=float)\n",
    "NUM_ITER = 150\n",
    "NUM_ACT = P_all.shape[0]\n",
    "for i_step in np.arange(NUM_ITER):\n",
    "    for i_state in np.arange(n_states):\n",
    "        action_values = np.zeros((4,), dtype=float)\n",
    "\n",
    "        #### Your time to work on it\n",
    "\n",
    "        ######\n",
    "        \n",
    "    V_0 = V_new\n",
    "\n",
    "    \n",
    "    if np.mod(i_step, 10) ==0:\n",
    "        print(f\"iter:{i_step}, values: {V_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e10f73-1756-4931-b378-5479f40c917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_value_grid(V_new, DESC_5x5, title=\"5x5 FrozenLake ‚Äì Value Iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef55ef",
   "metadata": {},
   "source": [
    "Next, we try to re-implement the **Truncated (Modified) Policy Iteration** algorithm, where we alternate between **partial policy evaluation** and **policy improvement**:\n",
    "\n",
    "$$\n",
    "V^{(j+1)}(s) \\;=\\; R(s) \\;+\\; \\gamma \\sum_{s'} P(s' \\mid s, \\pi_k(s))\\, V^{(j)}(s'),\n",
    "\\quad j = 0,1,\\dots,m-1,\n",
    "$$\n",
    "\n",
    "followed by:\n",
    "\n",
    "$$\n",
    "\\pi_{k+1}(s) \\;=\\; \\arg\\max_{a} \\Big[ \\, R(s) \\;+\\; \\gamma \\sum_{s'} P(s' \\mid s, a)\\, V^{(m)}(s') \\, \\Big].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ed431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an original value \n",
    "V_0 = np.zeros_like(Reward, dtype=float)\n",
    "V_new = np.zeros_like(V_0, dtype=float)\n",
    "# Initialize an original policy \n",
    "PI_0 = np.zeros_like(Reward)\n",
    "PI_new = np.zeros_like(Reward)\n",
    "NUM_ITER = 40\n",
    "NUM_ACT = P_all.shape[0]\n",
    "NUM_PE = 10\n",
    "\n",
    "for i_step in np.arange(NUM_ITER):\n",
    "    for i_VI in np.arange(NUM_PE): \n",
    "    #### Your time to work on it ######\n",
    "        \n",
    "        for i_state in np.arange(n_states):\n",
    "            \n",
    "            chosen_action =  # ......\n",
    "            V_new[i_state] = # ......\n",
    "           \n",
    "            \n",
    "    for i_state in np.arange(n_states):\n",
    "        action_values = np.zeros((4,), dtype=float)\n",
    "        for i_action in np.arange(NUM_ACT):\n",
    "            action_values[i_action] = # .....\n",
    "        PI_new[i_state] = # ......\n",
    "        \n",
    "     #####\n",
    "        \n",
    "    V_0 = V_new\n",
    "    PI_0 = PI_new\n",
    "    print(f\"iter:{i_step}, values: {V_new}, policy: {PI_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e1104-fd19-45ad-96ca-c0e58b6aa4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_value_and_policy_grid(V_new, PI_new, DESC_5x5, title=\"Values + Policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000fbe72-873a-4d54-8419-09d97a6faf36",
   "metadata": {},
   "source": [
    "# üçÄ Part 2 ‚Äî Monte Carlo (MC) Control with $Œµ$-Greedy Policy\n",
    "\n",
    "\n",
    "In this section we implement **Monte Carlo $Œµ$-Greedy Control**, a practical variant of **Monte Carlo Exploring Starts** that does **not** require special start states.\n",
    "\n",
    "---\n",
    "\n",
    "## üìò Algorithm: MC Œµ-Greedy Control\n",
    "\n",
    "**Goal:** Search for an optimal policy $œÄ^*$ and action-value function $q^*$ through sampling and $Œµ$-greedy improvement.\n",
    "\n",
    "### Initialization\n",
    "- Initialize policy $\\pi_0(a | s)$ and action-value function $q(s, a)$ for all $(s, a)$.  \n",
    "- Initialize `R(s,a) = 0`, `N(s,a) = 0`.  \n",
    "- Choose an exploration parameter $Œµ \\in (0, 1]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "19ff4f51-02e1-4fd3-b704-40cfe6d20161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(Q, s, epsilon):\n",
    "    \"\"\"Sample Œµ-greedy action using action-values Q.\"\"\"\n",
    "    n_actions = Q.shape[1]\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_actions)         # explore\n",
    "    return np.argmax(Q[s])                          # exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f68a0-6342-470b-b9bb-14f737379041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_states = env.observation_space.n     # total number of states\n",
    "n_actions = env.action_space.n         # total number of actions\n",
    "# Action‚Äìvalue function q(s,a)\n",
    "Q = np.zeros((n_states, n_actions), dtype=float)\n",
    "Q[-1,:] =10\n",
    "# Cumulative returns R(s,a)\n",
    "Returns = np.zeros_like(Q, dtype=float)\n",
    "# Visit counts N(s,a)\n",
    "Num = np.zeros_like(Q, dtype=float)\n",
    "\n",
    "# Exploration parameter\n",
    "epsilon = 1\n",
    "min_epsilon = 0.1\n",
    "epsilon_decay = 0.95\n",
    "\n",
    "pi = np.zeros(n_states, dtype=int)\n",
    "\n",
    "print(\"Initialization complete:\")\n",
    "print(f\"States: {n_states}, Actions: {n_actions}, Œµ = {epsilon}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778fcfe9-ad24-4336-9811-e80140d1897c",
   "metadata": {},
   "source": [
    "### For each episode\n",
    "1. **Episode Generation**  \n",
    "   - Choose a starting state‚Äìaction pair $(s_0, a_0)$ or simply start from the default start state.  \n",
    "   - Follow the **current Œµ-greedy policy** $\\pi$ to generate an episode of length $T$:  \n",
    "     $ (s_0, a_0, r_1, s_1, a_1, \\dots, s_{T-1}, a_{T-1}, r_T) $.\n",
    "2. **Initialization for this episode:** $G = 0$\n",
    "\n",
    "3. **Backward Return Computation**  \n",
    "   For each step of the episode, $t = T ‚àí 1, T ‚àí 2, \\ldots, 0$ we do the following:\n",
    "   $$\n",
    "   g \\leftarrow \\gamma g + r_{t+1}\n",
    "   $$\n",
    "   $$\n",
    "   \\text{R}(s_t, a_t) \\leftarrow \\text{R}(s_t, a_t) + g\n",
    "   $$\n",
    "   $$\n",
    "   \\text{N}(s_t, a_t) \\leftarrow \\text{N}(s_t, a_t) + 1\n",
    "   $$\n",
    "   $$\n",
    "   q(s_t, a_t) \\leftarrow \\frac{\\text{R}(s_t, a_t)}{\\text{N}(s_t, a_t)}\n",
    "   $$\n",
    "4. **Policy Improvement**\n",
    "   - Let $ a^* = \\arg\\max_a q(s_t, a) $.  \n",
    "   - Update œÄ to be $\\epsilon$-greedy with respect to q:\n",
    "\n",
    "     $$\n",
    "     \\pi(a|s_t) =\n",
    "     \\begin{cases}\n",
    "     1 - \\varepsilon + \\dfrac{\\varepsilon}{|\\mathcal{A}(s_t)|}, & a = a^* \\\\\n",
    "     \\dfrac{\\varepsilon}{|\\mathcal{A}(s_t)|}, & a \\neq a^*\n",
    "     \\end{cases}\n",
    "     $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9611b2-85e3-4764-a310-b90735e3f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 20000  # Here, one episode is too short, we do multiple episodes\n",
    "\n",
    "for i_ter in np.arange(100):\n",
    "    Q        = np.zeros((n_states, n_actions), dtype=float)\n",
    "    Returns  = np.zeros_like(Q)\n",
    "    Num      = np.zeros_like(Q)\n",
    "\n",
    "    # Decay the epsilon coefficient to encourage exploration in the beginning\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        s, _ = env.reset()\n",
    "        states, actions, rewards = [s], [], []\n",
    "        \n",
    "        done, steps = False, 0\n",
    "        while steps < max_steps:\n",
    "            a = epsilon_greedy_action(Q, s, epsilon)\n",
    "            # a slight modification on the environment to make the result match\n",
    "            if done:\n",
    "                s_next = s \n",
    "                r = Reward[s]\n",
    "                actions.append(a); rewards.append(r); states.append(s_next) \n",
    "            else: \n",
    "                s_next, r, term, trunc, _ = env.step(a)\n",
    "                done = bool(term or trunc)\n",
    "                actions.append(a); rewards.append(r); states.append(s_next)    \n",
    "                s = s_next\n",
    "            \n",
    "            steps += 1\n",
    "    \n",
    "        G = 0.0\n",
    "        T = len(actions)\n",
    "        for t in range(T-1, -1, -1):\n",
    "            s_t, a_t = states[t], actions[t]\n",
    "            G = # .........\n",
    "            Returns[s_t, a_t] = # .....\n",
    "            Num[s_t, a_t] += 1.0\n",
    "            Q[s_t, a_t] = # ........\n",
    "\n",
    "    # Policy improvement\n",
    "    pi = # ......\n",
    "    \n",
    "    print(f\"iter: {i_ter}, epsilon = {epsilon} \")\n",
    "    render_value_and_policy_grid(Q.max(axis=1), pi, DESC_5x5, title=\"Values + Policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ecdc4-e767-4dea-9d1b-cf75b550acdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5ea483-6556-4a4c-9999-b8955f0721cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f66f6-2398-43af-b7e1-208a69ff05de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a074f5fd-5b0a-4b58-97c9-fb8d6deb6b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470120f-ea29-4854-a662-fb4f48c27e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed2fe0bd-14fb-430e-bd21-c45e20dcc2db",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "edd59bbe-3022-4677-a018-6ad0d04ece34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf83638-bf30-411d-9f7a-9cde2fa7e7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
