{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46095493-da07-426d-9791-96d4535848b3",
   "metadata": {},
   "source": [
    "# üéÆ Lab 9: Deep Q-Network (DQN) on Atari\n",
    "\n",
    "In this lab, you will extend your reinforcement learning skills from classical control environments (such as **CartPole**) to more complex **Atari games** like *Pong-v5*.  \n",
    "You will implement a **Deep Q-Network (DQN)** that learns directly from raw pixel observations.\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand how DQN combines **Q-learning** with **deep neural networks** to handle high-dimensional visual inputs.  \n",
    "- Implement essential components:  \n",
    "  - Replay Buffer  \n",
    "  - Target Network  \n",
    "  - Œµ-Greedy Exploration Strategy  \n",
    "- Train an agent to achieve meaningful performance on an Atari environment.  \n",
    "- Visualize training progress and recorded gameplay frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44119655-5e54-41fd-8fc6-21e7078e92c5",
   "metadata": {},
   "source": [
    "###  Part 1: Environment Setup\n",
    "\n",
    "Before starting this lab, you need to create a new Conda environment (Python 3.10) and install the required packages for Atari reinforcement learning.\n",
    "\n",
    "- Step 1. Create and activate the environment\n",
    "```bash\n",
    "conda create -n atari python=3.10 -y\n",
    "conda activate atari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac4cc6c-6b46-4f3a-b412-b56fa6630c93",
   "metadata": {},
   "source": [
    "- Step 2. Use pip to install Gymnasium with Atari support, PyTorch, and the utilities used later in the lab.\n",
    "```bash\n",
    "pip install gymnasium[atari,accept-rom-license]==0.29.1\n",
    "pip install autorom[accept-rom-license]\n",
    "pip install stable-baselines3[extra]\n",
    "pip install opencv-python imageio matplotlib\n",
    "AutoROM --accept-license "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c880577-cb2d-4e9a-9965-bb0abb6f1a5a",
   "metadata": {},
   "source": [
    "- Step 3. Install Torch and TorchRL\n",
    "```bash\n",
    "pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126\n",
    "pip install torchrl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbd16ae-2a1c-4797-9950-8ae560377aba",
   "metadata": {},
   "source": [
    "- Step 4. Verify you installation by using the ‚ÄúPong‚Äù environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d340faf6-fff9-487c-9e1d-84937189e081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manyo\\.conda\\envs\\atari\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"ALE/Pong-v5\", render_mode=\"rgb_array\")\n",
    "frames = []\n",
    "\n",
    "obs, info = env.reset(seed=0)\n",
    "done = False\n",
    "while not done:\n",
    "    obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    frames.append(env.render())  \n",
    "    done = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb6b047b-e57e-49a4-a3b6-e08e30972267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae04a6d3b3f4613b749d60f726f68e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Frame', max=990), Output()), _dom_classes=('widget-inter‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df878efdf82644baa788b4716ac91a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': '<Figure size 640x480 with 1 Axes>', 'i‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "frames = np.load(\"pong_frames_uint8.npy\")\n",
    "N = len(frames)\n",
    "\n",
    "out = widgets.Output()\n",
    "slider = widgets.IntSlider(min=0, max=N-1, step=1, value=0, description=\"Frame\")\n",
    "\n",
    "@widgets.interact(i=slider)\n",
    "def _show(i):\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(frames[i])\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "display(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c406ac0f-8a9e-4820-877a-65094b4c75ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- Overview of the Atari Environment\n",
    "\n",
    "The **Atari environments** are among the most widely used benchmarks in reinforcement learning research.  \n",
    "They provide visually rich and challenging tasks that allow agents to learn control policies directly from **raw pixel inputs**.  \n",
    "These environments are part of the **Arcade Learning Environment (ALE)**, accessible via **Gymnasium**.\n",
    "\n",
    "-  Components of the Environment\n",
    "\n",
    "| Component | Description |\n",
    "|------------|--------------|\n",
    "| **State (Observation)** | A raw RGB image of size **(210 √ó 160 √ó 3)** representing the game screen. For DQN, these frames are usually converted to grayscale, resized (e.g., 84 √ó 84), and stacked (e.g., 4 frames) to provide temporal context. |\n",
    "| **Action Space** | A discrete set of valid joystick actions that differ between games. For example, in **Pong**, there are 6 possible actions: <br> `0: NOOP`  (no operation) <br> `1: FIRE`  (start game) <br> `2: MOVE RIGHT` <br> `3: MOVE LEFT` <br> `4: MOVE UP` <br> `5: MOVE DOWN` |\n",
    "| **Reward** | A scalar signal returned after each action. <br> ‚Ä¢ In **Pong**, +1 is given when the agent scores a point, and -1 when the opponent scores. <br> ‚Ä¢ In **Breakout**, the agent receives +1 for breaking a brick. <br> The cumulative reward reflects the agent‚Äôs game score. |\n",
    "| **Done Flag** | Indicates whether the game has ended (win, lose, or max steps reached). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c85b7af-0226-4706-8985-c49aaecaf928",
   "metadata": {},
   "source": [
    "#### Common Preprocessing Steps\n",
    "To stabilize learning, observations are typically preprocessed as follows:\n",
    "1. Convert RGB frames to grayscale.  \n",
    "2. Resize to 84 √ó 84.  \n",
    "3. Stack the most recent 4 frames.  \n",
    "4. Normalize pixel values to `[0, 1]`.  \n",
    "\n",
    "This reduces computational cost and helps the agent perceive motion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553db8b8-c1d8-4dcd-8631-84d56cfc9aeb",
   "metadata": {},
   "source": [
    "###  Part 2: Introduction to TorchRL\n",
    "\n",
    "TorchRL is a PyTorch-based library for **Reinforcement Learning (RL)** research and education. It provides a modular framework that integrates environments, data collection, replay buffers, transforms, and policy learning ‚Äî all built on top of **PyTorch** and **TensorDict**.\n",
    "\n",
    "---\n",
    "Traditional RL implementations often require extensive boilerplate code for:\n",
    "- environment wrappers and preprocessing (e.g., grayscale, resize, frame stacking)\n",
    "- replay buffer design and sampling\n",
    "- batched rollouts and asynchronous data collection\n",
    "- stable interfacing with PyTorch tensors and GPU devices  \n",
    "\n",
    "TorchRL simplifies these tasks with consistent data structures and modular components, making RL experiments both **reproducible and scalable**. Below are the core concepts of TorchRL\n",
    "\n",
    "---\n",
    "| Concept | Description |\n",
    "|----------|-------------|\n",
    "| **`TensorDict`** | A dictionary-like container that holds tensors together with consistent batch shapes (used for observations, actions, rewards, etc.). |\n",
    "| **`EnvBase` / `GymEnv`** | TorchRL‚Äôs base class for environments, compatible with Gymnasium environments (e.g., `ALE/Pong-v5`). |\n",
    "| **`TransformedEnv`** | A wrapper that applies a chain of **transforms** (e.g., grayscale, resize, normalization) to the environment automatically. |\n",
    "| **`ReplayBuffer`** | A memory module for storing and sampling transitions. TorchRL supports both simple and prioritized buffers. |\n",
    "| **`Collector`** | Handles rollouts and data collection efficiently, supporting multiple parallel environments. |\n",
    "| **`LossModules`** | Ready-to-use loss implementations for algorithms like DQN, A2C, PPO, SAC, etc. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c73efcb5-82cc-4d6f-9f9a-82dc6d86c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchrl.envs import GymEnv, TransformedEnv, Compose\n",
    "from torchrl.envs.transforms import ToTensorImage, GrayScale, Resize, CatFrames, DoubleToFloat, RewardClipping\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "from torchrl.data.replay_buffers.samplers import RandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a1dcf-8639-46bf-8544-14b88439a749",
   "metadata": {},
   "source": [
    "First, we introduce **TensorDict** and the **TorchRL replay buffer** by sampling trajectories from the Atari environment.\n",
    "\n",
    "In TorchRL, data collected from the environment (such as observations, actions, rewards, and next states) are stored inside a **TensorDict** ‚Äî a dictionary-like container that holds PyTorch tensors with consistent batch shapes.  \n",
    "Each environment step returns a TensorDict that organizes data in a structured and device-aware format, making it easy to manipulate, transform, and store for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4d6fe5-f26a-4479-98ca-fa1717f39382",
   "metadata": {},
   "source": [
    "Raw Atari frames are high-dimensional RGB images (210√ó160√ó3) that are not directly suitable for deep Q-learning.  \n",
    "TorchRL allows automatic preprocessing using **environment transforms**, which wrap the base environment in a `TransformedEnv`.  \n",
    "Each transform modifies the observation data inside the TensorDict before it‚Äôs returned.\n",
    "\n",
    "Typical transforms for DQN on Atari include:\n",
    "- `ToTensorImage()` ‚Äî converts images from HWC uint8 ‚Üí CHW float [0, 1]  \n",
    "- `GrayScale()` ‚Äî converts RGB to grayscale (reducing input channels from 3 ‚Üí 1)  \n",
    "- `Resize(84, 84)` ‚Äî resizes frames to the standard 84√ó84 input  \n",
    "- `CatFrames(N=4)` ‚Äî stacks the last 4 frames to capture motion information  \n",
    "- `RewardClipping(-1, 1)` ‚Äî stabilizes training by limiting reward magnitude  \n",
    "- `DoubleToFloat()` ‚Äî ensures float32 precision for network input  \n",
    "\n",
    "Below is an example setup for a preprocessed Atari environment in TorchRL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87959900-0650-4e0a-b5b8-dd1432b35b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Gymnasium environment\n",
    "base_env = GymEnv(\"ALE/Pong-v5\", from_pixels=True, pixels_only=True, render_mode=\"rgb_array\")\n",
    "n_actions = base_env.action_space.n\n",
    "obs_shape = (4, 84, 84)\n",
    "\n",
    "# Apply preprocessing transforms\n",
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    Compose(\n",
    "        ToTensorImage(),         # Convert to tensor format\n",
    "        GrayScale(),             # Convert RGB ‚Üí grayscale\n",
    "        Resize(84, 84),          # Resize to 84√ó84\n",
    "        CatFrames(N=4, dim=-3),  # Stack 4 frames ‚Üí (4, 84, 84)\n",
    "        DoubleToFloat(),         # Ensure float32 precision\n",
    "        RewardClipping(-1, 1),   # Clip rewards to [-1, 1]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a48f462-9853-4454-be09-d78d5e71f8b7",
   "metadata": {},
   "source": [
    "#### TensorDict: the Core Data Container\n",
    "\n",
    "A TensorDict acts like a dictionary, but it ensures that all tensors it contains share the same batch dimensions.  \n",
    "For example, an environment step may produce a TensorDict of the form:\n",
    "\n",
    "```python\n",
    "TensorDict({\n",
    "    'pixels': Tensor(...),          # current observation\n",
    "    'action': Tensor(...),\n",
    "    'next': {\n",
    "        'pixels': Tensor(...),      # next observation\n",
    "        'reward': Tensor(...),\n",
    "        'done': Tensor(...)\n",
    "    }\n",
    "}) \n",
    "\n",
    "```\n",
    "This structure allows TorchRL to manage complex rollouts and batch operations with minimal boilerplate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc3b061-a156-43c2-ae73-b8e253542b7a",
   "metadata": {},
   "source": [
    "#### The Native TorchRL Replay Buffer\n",
    "\n",
    "TorchRL provides a powerful and flexible replay buffer system built around **TensorDicts**.  \n",
    "The `TensorDictReplayBuffer` class, together with `LazyMemmapStorage`, allows you to efficiently store and sample transitions collected from the environment.\n",
    "\n",
    "Key advantages include:\n",
    "- Seamless integration with TensorDict-based environments  \n",
    "- Automatic device handling (CPU/GPU)  \n",
    "- Support for both in-memory and disk-backed storage  \n",
    "- Built-in random or prioritized sampling strategies  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "098c6e45-6614-4b69-8817-7bb09f84d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.data.replay_buffers.samplers import RandomSampler\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "from tensordict import TensorDict\n",
    "\n",
    "rb = TensorDictReplayBuffer(\n",
    "    storage=LazyMemmapStorage(max_size=50_000),  # disk-backed storage (efficient and scalable)\n",
    "    sampler=RandomSampler(),                     # uniform random sampling\n",
    "    batch_size=32,                               # default sample batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87aa458d-5b8c-4680-88c6-4cbfd6604a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay buffer size: 5000\n"
     ]
    }
   ],
   "source": [
    "# Collect a trajectory with random transitions\n",
    "td = env.reset()\n",
    "for _ in range(5000):\n",
    "    \n",
    "    # sample an action\n",
    "    a = env.action_spec.rand()\n",
    "    obs = td.get(\"pixels\")\n",
    "    td = env.step(td.set(\"action\", a))\n",
    "    next_obs = td.get((\"next\", \"pixels\"))\n",
    "    r = td.get((\"next\", \"reward\"))\n",
    "    d = td.get((\"next\", \"done\"))\n",
    "\n",
    "    transition = TensorDict(\n",
    "        {\n",
    "            \"obs\": obs,\n",
    "            \"action\": a,\n",
    "            \"reward\": r,\n",
    "            \"next_obs\": next_obs,\n",
    "            \"done\": d,\n",
    "        },\n",
    "        batch_size=[],\n",
    "    )\n",
    "    rb.add(transition)\n",
    "\n",
    "    if d.item():\n",
    "        td = env.reset()\n",
    "\n",
    "print(\"Replay buffer size:\", len(rb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59522b6-f7a0-444c-9c74-6b72b52872e6",
   "metadata": {},
   "source": [
    "üîç Note: Difference Between `td` and `transition`\n",
    "\n",
    "| Variable | Role | Structure | Usage |\n",
    "|-----------|------|------------|--------|\n",
    "| **`td`** | The live **TensorDict** returned by the environment through `env.reset()` or `env.step()`. It contains both the current and next-step information (nested under `\"next\"`). | `{ \"action\": ..., \"next\": { \"pixels\": ..., \"reward\": ..., \"done\": ... } }` | Used for **interacting with the environment** ‚Äî passed into `env.step()` and updated after each action. |\n",
    "| **`transition`** | A **flattened TensorDict** created from the fields of `td`, containing exactly one tuple \\((s_t, a_t, r_t, s_{t+1}, done_t)\\). | `{ \"obs\": ..., \"action\": ..., \"reward\": ..., \"next_obs\": ..., \"done\": ... }` | Used for **storing in the replay buffer** and later sampling for training (e.g., in DQN updates). |\n",
    "\n",
    "**In short:**\n",
    "- `td` is the environment‚Äôs **structured live output** for the current step.  \n",
    "- `transition` is the **simplified snapshot** of one experience transition that gets pushed into the replay buffer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba3bbf42-7b1c-4cf0-b036-4434f6f72ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = rb.sample(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1726852-3cc1-4215-8884-28a214796d1d",
   "metadata": {},
   "source": [
    "###  Part 3: DQN on Atari\n",
    "\n",
    "In this section, you will complete the implementation of the **Deep Q-Network (DQN)** algorithm for an Atari environment (e.g., *Pong*).  \n",
    "The provided code initializes the environment, replay buffer, and Q-networks. Your task is to **connect all the pieces** to form the full DQN learning process.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Background\n",
    "\n",
    "DQN learns an approximate action-value function $ Q_\\theta(s, a) $ by minimizing the **Bellman error**:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\mathbb{E}\\Big[(Q_\\theta(s_t, a_t) - y_t)^2\\Big],\n",
    "$$\n",
    "where\n",
    "$$\n",
    "y_t = r_t + \\gamma (1 - d_t) \\max_{a'} Q_{\\theta^-}(s_{t+1}, a')\n",
    "$$\n",
    "and $ Q_{\\theta^-} $ is the **target network** with delayed parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Provided Components\n",
    "\n",
    "You are already given:\n",
    "- ‚úÖ A TorchRL Atari environment (`env`)\n",
    "- ‚úÖ Replay buffer `rb`\n",
    "- ‚úÖ Online and target Q-networks (`q`, `q_target`)\n",
    "- ‚úÖ An optimizer and discount factor (`optimizer`, `gamma`)\n",
    "- ‚úÖ Pre-written code for:\n",
    "  - Sampling from `rb`\n",
    "  - Computing `target` and `loss`\n",
    "  - Performing one gradient update\n",
    "\n",
    "You will now write the **training loop** to combine these elements.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e783fa-2b31-442e-a813-e6e0d0b2075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your time to work on it (See below for some hints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0497455e-74ed-4567-ab26-5a1a67b5a983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79a4d8-92ab-417f-999a-80a2bb459dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b19def2-8968-4f88-9fee-13b1e5ae658c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328e5522-0b18-4950-a697-c2f9f4d1796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639aff26-d3ec-4e25-b98d-f8a74a0b804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hint 1: Neural Network Design\n",
    "'''\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512), nn.ReLU(),\n",
    "            nn.Linear(512, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,4,84,84) float32 in [0,1]\n",
    "        z = self.conv(x)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        return self.fc(z)\n",
    "\n",
    "q = QNet(n_actions).to(device)\n",
    "q_target = QNet(n_actions).to(device)\n",
    "q_target.load_state_dict(q.state_dict())\n",
    "q_target.eval()\n",
    "\n",
    "optimizer = optim.Adam(q.parameters(), lr=1e-4)\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27052f4a-eed8-4673-ac0a-aacab0faf8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hint 2: How to implement gradient descent for Q-learning\n",
    "'''\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch = rb.sample(batch_size)\n",
    "obs_b      = batch[\"obs\"].to(device)              # (B,4,84,84)\n",
    "act_b      = batch[\"action\"].long().to(device)    # (B,)\n",
    "rew_b      = batch[\"reward\"].to(device).squeeze(-1)  # make sure it's (B,)\n",
    "next_obs_b = batch[\"next_obs\"].to(device)         # (B,4,84,84)\n",
    "done_b     = batch[\"done\"].to(device).float().squeeze(-1)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # target = r + gamma * (1-done) * max_a' Q_target(s',a')\n",
    "    q_next = q_target(next_obs_b).max(1).values\n",
    "    target = rew_b + gamma * (1.0 - done_b) * q_next\n",
    "\n",
    "act_b_ind = act_b.argmax(dim=-1)\n",
    "q_values = q(obs_b).gather(1, act_b_ind.view(-1, 1)).squeeze(1)\n",
    "loss = F.smooth_l1_loss(q_values, target)\n",
    "\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "loss.backward()\n",
    "nn.utils.clip_grad_norm_(q.parameters(), max_norm=10.0)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef8f6d04-985a-4b25-b64d-d5c21609d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 3: Epsilon-greedy policy\n",
    "def select_action(obs, eps: float):\n",
    "    if torch.rand(1).item() < eps:\n",
    "        # Use TorchRL action_spec for a proper tensor action\n",
    "        return env.action_spec.rand()  # scalar tensor (long)\n",
    "    with torch.no_grad():\n",
    "        x = obs.unsqueeze(0).to(device)   # (1,4,84,84)\n",
    "        qvals = q(x)                               # (1,n_actions)\n",
    "        a = torch.argmax(qvals, dim=1).to(\"cpu\")   # back to CPU\n",
    "        return a.squeeze(0)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7d7bcb7-bb00-40e1-87cf-07e99e58ea2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5265be55-61ff-4c73-bf1a-99debc1d2f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
