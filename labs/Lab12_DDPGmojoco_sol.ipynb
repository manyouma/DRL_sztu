{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f120db-8f79-4afe-8bf0-3ba8c4a80de2",
   "metadata": {},
   "source": [
    "# Lab12: Deep Deterministic Policy Gradient (DDPG) for Continuous Control\n",
    "\n",
    "In this lab, you will implement and train a **Deep Deterministic Policy Gradient (DDPG)** agent to solve a **continuous control task** using the MuJoCo physics simulator.  \n",
    "The target environment is:\n",
    "\n",
    "> **Hopper-v4** — a one-legged robot that must learn to hop forward as fast and as stably as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62bd435-ee7e-4303-be97-1bf176f68b2d",
   "metadata": {},
   "source": [
    "## 1. Environment: Hopper-v4\n",
    "\n",
    "The [Hopper environment](https://gymnasium.farama.org/environments/mujoco/hopper/) is a physics-based locomotion task simulated using **MuJoCo**.\n",
    "\n",
    "- **Observation Space:** 11-dimensional continuous state  \n",
    "- **Action Space:** 3-dimensional continuous torque control  \n",
    "- **Objective:** Move forward as fast as possible without falling  \n",
    "- **Episode Ends When:** The robot falls or becomes unstable  \n",
    "\n",
    "This task represents a realistic robotic control problem with:\n",
    "- Nonlinear dynamics  \n",
    "- High-dimensional state space  \n",
    "- Continuous actions  \n",
    "- Long-term credit assignment  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea66739-b05b-4b87-a8a5-ec53550e4cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env id: HalfCheetah-v4\n",
      "Collected frames: 300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b859e3a8622483a80160d0533bff8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Frame', max=299), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_frame(i)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "env = gym.make(\"HalfCheetah-v4\", render_mode=\"rgb_array\")\n",
    "print(\"Env id:\", env.spec.id)  \n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "frames = []\n",
    "num_steps = 300\n",
    "\n",
    "for t in range(num_steps):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    frame = env.render()\n",
    "    frames.append(frame.copy())   \n",
    "\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.close()\n",
    "print(\"Collected frames:\", len(frames))\n",
    "\n",
    "def show_frame(i):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(frames[i])\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Hopper-v4 frame {i}\")\n",
    "    plt.show()\n",
    "\n",
    "interact(\n",
    "    show_frame,\n",
    "    i=IntSlider(0, min=0, max=len(frames)-1, step=1, description=\"Frame\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79921ebb-160d-474c-9c68-ae3381e3e4ab",
   "metadata": {},
   "source": [
    "## 2. Background\n",
    "\n",
    "Many real-world control problems involve **continuous actions**, such as:\n",
    "- Robot joint torques  \n",
    "- Vehicle steering and acceleration  \n",
    "- Control forces in physical systems  \n",
    "\n",
    "Classical Deep Q-Networks (DQN) cannot be directly applied to these problems because they require **discrete** action spaces.  \n",
    "DDPG extends Q-learning to **continuous control** by combining:\n",
    "\n",
    "- A **policy network (Actor)** that outputs continuous actions.\n",
    "- A **value network (Critic)** that estimates the Q-function.\n",
    "- **Target networks** for stable training.\n",
    "- A **replay buffer** for off-policy learning.\n",
    "\n",
    "DDPG is one of the foundational algorithms for continuous reinforcement learning and serves as the basis for more advanced methods such as **TD3** and **SAC**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dd8e51-763b-4c47-9def-04e731a0d7f9",
   "metadata": {},
   "source": [
    "## 3. Algorithm Implementation : DDPG\n",
    "\n",
    "DDPG consists of the following components:\n",
    "\n",
    "- **Actor Network**  \n",
    "  Outputs a deterministic continuous action given a state.\n",
    "\n",
    "- **Critic Network**  \n",
    "  Estimates the Q-value of state–action pairs.\n",
    "\n",
    "- **Target Networks**  \n",
    "  Slowly updated copies of the actor and critic for stable learning.\n",
    "\n",
    "- **Replay Buffer**  \n",
    "  Stores past transitions for off-policy training.\n",
    "\n",
    "- **Exploration Noise**  \n",
    "  Added to the actor’s output during training for sufficient exploration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bc66d20-17f7-42d3-a1c4-1ce5aab936c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f541dc25-9d07-486f-835c-88ed46d964e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, max_action):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, act_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.max_action * self.net(x)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        return self.net(torch.cat([s, a], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de9eb6e-b045-4377-99ac-a4921b4d33c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=1_000_000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, s, a, r, s2, d):\n",
    "        self.buffer.append((s, a, r, s2, d))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s, a, r, s2, d = map(np.array, zip(*batch))\n",
    "        return (\n",
    "            torch.FloatTensor(s),\n",
    "            torch.FloatTensor(a),\n",
    "            torch.FloatTensor(r).unsqueeze(1),\n",
    "            torch.FloatTensor(s2),\n",
    "            torch.FloatTensor(d).unsqueeze(1),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cabfadf0-e155-4629-ad29-a925f5cea973",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"HalfCheetah-v4\")\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "actor = Actor(obs_dim, act_dim, max_action).to(device)\n",
    "critic = Critic(obs_dim, act_dim).to(device)\n",
    "actor_target = Actor(obs_dim, act_dim, max_action).to(device)\n",
    "critic_target = Critic(obs_dim, act_dim).to(device)\n",
    "\n",
    "actor_target.load_state_dict(actor.state_dict())\n",
    "critic_target.load_state_dict(critic.state_dict())\n",
    "\n",
    "actor_opt = optim.Adam(actor.parameters(), lr=1e-4)\n",
    "critic_opt = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "buffer = ReplayBuffer()\n",
    "\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "batch_size = 256\n",
    "exploration_noise = 0.1\n",
    "\n",
    "total_steps = 300_000\n",
    "warmup_steps = 10_000\n",
    "\n",
    "state, _ = env.reset()\n",
    "episode_reward = 0\n",
    "episode_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5679b493-4cbd-4634-a2de-fe6e9c9e0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"ddpg_cheetah_actor_class.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6321ecc4-9659-47d1-af0d-09d563323bae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/300000\n",
      "Episode 0 | Reward: -185.6 | Lenght: 1000\n",
      "Episode 1 | Reward: -256.7 | Lenght: 1000\n",
      "Episode 2 | Reward: -324.6 | Lenght: 1000\n",
      "Episode 3 | Reward: -404.8 | Lenght: 1000\n",
      "Episode 4 | Reward: -275.0 | Lenght: 1000\n",
      "Episode 5 | Reward: -260.0 | Lenght: 1000\n"
     ]
    }
   ],
   "source": [
    "episode = 0\n",
    "\n",
    "for step in range(total_steps):\n",
    "\n",
    "    # --- select action ---\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    action = actor(state_tensor).cpu().data.numpy().flatten()\n",
    "\n",
    "    if step < warmup_steps:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action += np.random.normal(0, exploration_noise, size=act_dim)\n",
    "\n",
    "    action = np.clip(action, -max_action, max_action)\n",
    "\n",
    "    # --- step env ---\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    buffer.push(state, action, reward, next_state, float(done))\n",
    "\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    episode_length += 1\n",
    "    \n",
    "    # --- reset if done ---\n",
    "    if done:\n",
    "        print(f\"Episode {episode} | Reward: {episode_reward:.1f} | Lenght: {episode_length}\")\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        episode += 1\n",
    "\n",
    "    # --- update ---\n",
    "    if len(buffer) > batch_size:\n",
    "\n",
    "        s, a, r, s2, d = buffer.sample(batch_size)\n",
    "        s = s.to(device)\n",
    "        a = a.to(device)\n",
    "        r = r.to(device)\n",
    "        s2 = s2.to(device)\n",
    "        d = d.to(device)\n",
    "\n",
    "        # Critic update\n",
    "        with torch.no_grad():\n",
    "            a2 = actor_target(s2)\n",
    "            q_target = r + gamma * (1 - d) * critic_target(s2, a2)\n",
    "\n",
    "        q_val = critic(s, a)\n",
    "        critic_loss = nn.MSELoss()(q_val, q_target)\n",
    "\n",
    "        critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_opt.step()\n",
    "\n",
    "        # Actor update\n",
    "        actor_loss = -critic(s, actor(s)).mean()\n",
    "\n",
    "        actor_opt.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_opt.step()\n",
    "\n",
    "        # Target update\n",
    "        for p, p_t in zip(actor.parameters(), actor_target.parameters()):\n",
    "            p_t.data.copy_(tau * p.data + (1 - tau) * p_t.data)\n",
    "\n",
    "        for p, p_t in zip(critic.parameters(), critic_target.parameters()):\n",
    "            p_t.data.copy_(tau * p.data + (1 - tau) * p_t.data)\n",
    "\n",
    "    # --- occasionally show progress ---\n",
    "    if step % 10_000 == 0:\n",
    "        print(f\"Step {step}/{total_steps}\")\n",
    "        torch.save(actor.state_dict(), SAVE_PATH)\n",
    "        \n",
    "print(\"Training finished and model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418a4cf-32b0-49c7-b6fe-8f23ea20cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d2ae0-fb04-4d67-b3e0-4c571f282ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
