{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839c9bab",
   "metadata": {},
   "source": [
    "### ðŸ§Š Lab 4 â€” Value Iteration and Policy Iteration for Frozen Lake\n",
    "In this lab we will continue our exploration of Markov Decision Processes (MDPs) \n",
    "using the simplified **FrozenLake** environment. Building on Lab 3, we will \n",
    "practice two key ideas:\n",
    "\n",
    "1. **Finding the Optimal Policy (Value Iteration)**  We apply value iteration to compute the optimal state values and extract the corresponding optimal policy that maximizes long-term return.\n",
    "\n",
    "2. **Finding the Optimal Policy (Policy Iteration)** We apply policy iteration, which alternates between evaluating the current policy and improving it until convergence to the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f7b7dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "import random\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3214c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a smaller 3x3 map\n",
    "DESC_3x3 = [\n",
    "    \"SFF\",\n",
    "    \"FHF\",\n",
    "    \"FFG\",\n",
    "]\n",
    "env = gym.make(\"FrozenLake-v1\",desc=DESC_3x3,is_slippery=True, render_mode=\"ansi\")\n",
    "obs, info = env.reset(seed=42)\n",
    "#print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9dc459a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_LEFT = [\n",
    "    [2, 1, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "    [1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 0, 3, 1, 0, 1, 0],\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 1, 0, 0, 2, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
    "]\n",
    "\n",
    "P_DOWN = [\n",
    "    [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 1, 3, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0, 2, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 1, 2],\n",
    "]\n",
    "\n",
    "P_RIGHT = [\n",
    "    [1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 2, 0, 0, 1, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 3, 0, 0, 1, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
    "    [0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 1, 2],\n",
    "]\n",
    "\n",
    "P_UP = [\n",
    "    [2, 1, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 0, 3, 1, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0, 2, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 3],\n",
    "]\n",
    "P_all = np.array([P_LEFT, P_DOWN, P_RIGHT, P_UP]).transpose(0, 2, 1)/3\n",
    "POLICY = [\n",
    "    1,  # state 0 (top-left)\n",
    "    2,  # state 1\n",
    "    1,  # state 2\n",
    "    1,  # state 3\n",
    "    1,  # state 4 (center, possibly hole)\n",
    "    1,  # state 5\n",
    "    2,  # state 6\n",
    "    2,  # state 7\n",
    "    2,  # state 8 (goal state)\n",
    "]\n",
    "\n",
    "n_states = len(POLICY)\n",
    "P_pi = np.zeros((n_states, n_states))\n",
    "for s in range(n_states):\n",
    "    a = POLICY[s]             # action chosen at state s\n",
    "    P_pi[s, :] = P_all[a, s]  # copy the probabilities for that action\n",
    "\n",
    "Reward = [\n",
    "0,  # state 0 (top-left)\n",
    "0,  # state 1\n",
    "0,  # state 2\n",
    "0,  # state 3\n",
    "0,  # state 4 (center, possibly hole)\n",
    "0,  # state 5\n",
    "0,  # state 6\n",
    "0,  # state 7\n",
    "1,  # state 8 (goal state)\n",
    "]\n",
    "Reward = np.array(Reward)\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fbcca0",
   "metadata": {},
   "source": [
    "## Part 1: Finding the Optimal Policy (Value Iteration)\n",
    "\n",
    "So far, we have focused on **policy evaluation** â€” estimating the value of a \n",
    "given policy. In this part, we move to **control**, where the goal is to find \n",
    "the **optimal policy** that maximizes long-term returns.\n",
    "\n",
    "The key idea is to apply **value iteration**, which combines evaluation and \n",
    "improvement into a single update rule:\n",
    "\n",
    "$$\n",
    "V_{k+1}(s) \\;=\\; \\max_a \\Big[ R(s) + \\gamma \\sum_{s'} P(s'|s,a) \\, V_k(s') \\Big].\n",
    "$$\n",
    "\n",
    "Once the value function converges, we extract the optimal policy by choosing, \n",
    "in each state, the action that achieves the maximum:\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\arg\\max_a \\Big[ R(s) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s') \\Big].\n",
    "$$\n",
    "\n",
    "### Steps\n",
    "1. Initialize the value function $V_0(s)$ arbitrarily (e.g., all zeros).  \n",
    "2. Repeatedly update each stateâ€™s value using the **max over actions** rule.  \n",
    "3. Stop when values converge (the updates change very little).  \n",
    "4. Derive the optimal policy $\\pi^*$ by picking the greedy action at each state.  \n",
    "\n",
    "### What to do\n",
    "- Implement value iteration with your transition model `P_all_prob` and rewards.  \n",
    "- Print the **optimal state values**.  \n",
    "- Print the **optimal policy** as action indices (0 = LEFT, 1 = DOWN, 2 = RIGHT, 3 = UP).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "97536865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your time to work on it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef55ef",
   "metadata": {},
   "source": [
    "## Part 2: Finding the Optimal Policy (Truncated Policy Iteration)\n",
    "So far, we have seen two extremes:  \n",
    "- **Value Iteration**, which maximizes at every update, and  \n",
    "- **Policy Iteration**, which fully evaluates a policy before improving it.  \n",
    "\n",
    "**Truncated (Modified) Policy Iteration** strikes a balance by performing only a **few** policy-evaluation sweeps before each policy-improvement step.\n",
    "\n",
    "Formally, we alternate between **partial policy evaluation** and **policy improvement**:\n",
    "\n",
    "$$\n",
    "V^{(j+1)}(s) \\;=\\; R(s) \\;+\\; \\gamma \\sum_{s'} P(s' \\mid s, \\pi_k(s))\\, V^{(j)}(s'),\n",
    "\\quad j = 0,1,\\dots,m-1,\n",
    "$$\n",
    "\n",
    "followed by:\n",
    "\n",
    "$$\n",
    "\\pi_{k+1}(s) \\;=\\; \\arg\\max_{a} \\Big[ \\, R(s) \\;+\\; \\gamma \\sum_{s'} P(s' \\mid s, a)\\, V^{(m)}(s') \\, \\Big].\n",
    "$$\n",
    "\n",
    "### Steps\n",
    "1. Initialize the value function $ V_0(s) $ (e.g., all zeros) and an initial policy $ \\pi_0 $.  \n",
    "2. **Partial Evaluation**: With $ \\pi_k$ fixed, perform only $ m $ Bellman **expectation** sweeps to update $ V $.  \n",
    "3. **Policy Improvement**: Update the policy by choosing the action that maximizes the expected return with the updated $V $.  \n",
    "4. Repeat Steps 2â€“3 until the policy no longer changes (or value/policy changes are below a small threshold).\n",
    "\n",
    "### What to do\n",
    "- Implement truncated policy iteration using your transition model `P_all_prob` and rewards `R`.  \n",
    "- Choose a small number of evaluation sweeps \\( m \\) (e.g., 1â€“3) **or** use a tolerance \\( \\theta \\) to stop early within each evaluation phase.  \n",
    "- Print the **final state values** and the **resulting policy** as action indices  \n",
    "  `(0 = LEFT, 1 = DOWN, 2 = RIGHT, 3 = UP)`.  \n",
    "- (Optional) Compare iterations/time vs. **full policy iteration** and **value iteration** on your map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c2ed431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your time to work on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e6a16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb3049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4450a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
