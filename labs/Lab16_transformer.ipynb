{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca227f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03785eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\"\n",
    "url = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df67901",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\" \n",
    "#model_name = \"Qwen/Qwen2.5-0.5B\"          \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "#model = AutoModel.from_pretrained(model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f555ede-6096-47b9-a1d2-08a59e74ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698e0eaa-bc6b-4cf3-a566-4e8f55ca65ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using Device: {device}\")\n",
    "\n",
    "# å¦‚æœæœ‰GPUï¼Œæ˜¾ç¤ºæ˜¾å¡ä¿¡æ¯\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# æŠŠæ¨¡å‹ç§»åˆ°GPUä¸Šï¼\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c85d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹DeepSeek tokenizerçš„å…·ä½“ä¿¡æ¯\n",
    "print(\"Tokenizer:\", type(tokenizer).__name__)\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Special token:\")\n",
    "print(\" - CLS:\", tokenizer.cls_token)\n",
    "print(\" - SEP:\", tokenizer.sep_token) \n",
    "print(\" - PAD:\", tokenizer.pad_token)\n",
    "print(\" - UNK:\", tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = \"All happy families are alike; each unhappy family is unhappy in its own way.\"\n",
    "tokens = tokenizer.tokenize(test_texts)\n",
    "token_ids = tokenizer.encode(test_texts)\n",
    "print(f\"\\nOriginal: {test_texts}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids[:30]}...\")\n",
    "print(f\"toke num: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1fca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_space_handling():\n",
    "    examples = [\n",
    "        \"hello world\",  \n",
    "        \"hello  world\", \n",
    "        \"hello\",         \n",
    "        \" hello\",     \n",
    "    ]\n",
    "    \n",
    "    for text in examples:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        print(f\"'{text}' â†’ {tokens}\")\n",
    "        for token in tokens:\n",
    "            if 'Ä ' in token:\n",
    "                print(f\" Note that  '{token}' has a space before it\")\n",
    "                \n",
    "explain_space_handling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4481c74-8459-4929-b041-fce2c76820da",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.get_input_embeddings()\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b56c26b-321e-45c0-8830-89d0a58c763a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "def get_word_embedding(word, model, tokenizer):\n",
    "    embedding_layer = model.get_input_embeddings()\n",
    "    token_ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "\n",
    "    token_ids = torch.tensor(token_ids, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = embedding_layer(token_ids)   # [num_tokens, dim]\n",
    "        return embeddings.mean(dim=0)              # å¹³å‡æ›´ç¨³\n",
    "\n",
    "def cosine_similarity_torch(vec1, vec2):\n",
    "    return F.cosine_similarity(\n",
    "        vec1.unsqueeze(0), \n",
    "        vec2.unsqueeze(0), \n",
    "        dim=1\n",
    "    ).item()\n",
    "\n",
    "# å•è¯åˆ—è¡¨\n",
    "words = [\"cat\", \"dog\", \"lion\", \"wind\", \"rain\", \"snow\", \"run\", \"walk\", \"jump\"]\n",
    "\n",
    "print(\"ğŸ” Similarity matrix\")\n",
    "print(\"=\" * 120)\n",
    "print(\" \" * 12 + \"\".join([f\"{word:>10}\" for word in words]))\n",
    "\n",
    "# è®¡ç®— embedding\n",
    "word_embeddings = {\n",
    "    word: get_word_embedding(word, model, tokenizer)\n",
    "    for word in words\n",
    "}\n",
    "\n",
    "# ç›¸ä¼¼åº¦çŸ©é˜µ\n",
    "for word1 in words:\n",
    "    print(f\"{word1:>12}: \", end=\"\")\n",
    "    for word2 in words:\n",
    "        sim = cosine_similarity_torch(\n",
    "            word_embeddings[word1],\n",
    "            word_embeddings[word2]\n",
    "        )\n",
    "\n",
    "        if sim > 0.2:\n",
    "            print(f\"\\033[92m{sim:>10.3f}\\033[0m\", end=\"\")  \n",
    "        elif sim < 0.1:\n",
    "            print(f\"\\033[91m{sim:>10.3f}\\033[0m\", end=\"\")  \n",
    "        else:\n",
    "            print(f\"{sim:>10.3f}\", end=\"\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53015f3-603c-4f6a-afff-ec40b5cd2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_generate(prompt, model, tokenizer, max_new_tokens=50, entropy=0.5):\n",
    "\n",
    "    device = model.device\n",
    "    \n",
    "    print(f\"Input: '{prompt}'\")\n",
    "    print(f\"Entropy: {entropy} \")\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs.input_ids\n",
    "    \n",
    "    generated_tokens = []\n",
    "    \n",
    "    for i in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            \n",
    "\n",
    "            if hasattr(outputs, 'logits'):\n",
    "                logits = outputs.logits\n",
    "            else:\n",
    "                logits = outputs.last_hidden_state\n",
    "            \n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "\n",
    "            if entropy == 0.0:\n",
    "                next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "                strategy = \"Greedy\"\n",
    "                \n",
    "            else:\n",
    "\n",
    "                temperature = 0.1 + entropy * 1.9 \n",
    "                next_token_logits = next_token_logits / temperature\n",
    "                probs = torch.softmax(next_token_logits, dim=-1)\n",
    "                next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "                strategy = f\"temperature(t={temperature:.1f})\"\n",
    "            \n",
    "            new_token = tokenizer.decode(next_token_id[0], skip_special_tokens=True)\n",
    "            generated_tokens.append(new_token)\n",
    "            \n",
    "            print(f\"Token {i+1}: '{new_token}' ({strategy})\")\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "            \n",
    "            # åœæ­¢æ¡ä»¶\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "            if new_token in ['\\n', '.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ']:\n",
    "                break\n",
    "    \n",
    "    generated_text = prompt + ''.join(generated_tokens)\n",
    "    print(f\"Test: {generated_text}\")\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe833f5-6fc9-4243-b36b-e95b1a11a4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question, model, tokenizer, max_answer_tokens=50, entropy=0.5):\n",
    "    \n",
    "    # æ„å»ºæç¤ºè¯ï¼Œæ˜ç¡®è¦æ±‚ç®€çŸ­å›ç­”\n",
    "    prompt = f\"Questionï¼š{question}\\n\"\n",
    "    \n",
    "    print(f\"ğŸ¤” question: {question}\")\n",
    "\n",
    "    \n",
    "    # ä½¿ç”¨robust_generateç”Ÿæˆå›ç­”\n",
    "    full_response = robust_generate(prompt, model, tokenizer, max_new_tokens=max_answer_tokens, entropy=0.5)\n",
    "    \n",
    "    # æå–å›ç­”éƒ¨åˆ†ï¼ˆå»æ‰é—®é¢˜ï¼‰\n",
    "    answer = full_response.replace(prompt, \"\").strip()\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Final answer: {answer}\")\n",
    "    print(f\"ğŸ“Š Length: {len(answer)} å­—\")\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a839c5-f173-44ef-85b5-e4b589b47ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"å­”å­æ˜¯è°\"\n",
    "answer = ask_question(question, model, tokenizer, max_answer_tokens=50, entropy=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74d3f475-6749-4e5e-bcab-b16885ebec4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æç¤º: 'é—®é¢˜ï¼šå­”å­æ˜¯è°\n",
      "å­”å­æ˜¯æˆ‘å›½å¤ä»£çš„å¤§æ€æƒ³å®¶ï¼Œ'\n",
      "ğŸ“Š å‰100ä¸ªæœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªtoken:\n",
      "\n",
      " 1. 'å¤§         ' (ID: 26288) - æ¦‚ç‡: 0.4290\n",
      " 2. 'å„’å®¶        ' (ID: 114358) - æ¦‚ç‡: 0.1371\n",
      " 3. 'ä¹Ÿæ˜¯        ' (ID: 100000) - æ¦‚ç‡: 0.0832\n",
      " 4. 'æ•™è‚²        ' (ID: 99460) - æ¦‚ç‡: 0.0435\n",
      " 5. 'ä»–         ' (ID: 42411) - æ¦‚ç‡: 0.0363\n",
      " 6. 'æ˜¯         ' (ID: 20412) - æ¦‚ç‡: 0.0262\n",
      " 7. 'ä»–æ˜¯        ' (ID: 104878) - æ¦‚ç‡: 0.0196\n",
      " 8. 'æ”¿æ²»        ' (ID: 101091) - æ¦‚ç‡: 0.0169\n",
      " 9. 'æˆ‘å›½        ' (ID: 101055) - æ¦‚ç‡: 0.0147\n",
      "10. 'ä¹Ÿ         ' (ID: 74763) - æ¦‚ç‡: 0.0113\n",
      "11. 'åŒæ—¶ä¹Ÿæ˜¯      ' (ID: 111733) - æ¦‚ç‡: 0.0104\n",
      "12. 'æ˜¥ç§‹        ' (ID: 109385) - æ¦‚ç‡: 0.0098\n",
      "13. 'åˆæ˜¯        ' (ID: 104458) - æ¦‚ç‡: 0.0090\n",
      "14. 'æ˜¯æˆ‘å›½       ' (ID: 108659) - æ¦‚ç‡: 0.0088\n",
      "15. 'ä¼Ÿå¤§çš„       ' (ID: 107792) - æ¦‚ç‡: 0.0079\n",
      "16. 'ä»–çš„        ' (ID: 100648) - æ¦‚ç‡: 0.0077\n",
      "17. 'è¢«         ' (ID: 99250) - æ¦‚ç‡: 0.0056\n",
      "18. 'è‘—å        ' (ID: 102280) - æ¦‚ç‡: 0.0054\n",
      "19. 'å„’         ' (ID: 102907) - æ¦‚ç‡: 0.0038\n",
      "20. 'è¢«ç§°ä¸º       ' (ID: 106253) - æ¦‚ç‡: 0.0037\n",
      "21. 'æœ‰         ' (ID: 18830) - æ¦‚ç‡: 0.0034\n",
      "22. 'å“²å­¦        ' (ID: 105384) - æ¦‚ç‡: 0.0033\n",
      "23. 'ä¸­å›½        ' (ID: 58695) - æ¦‚ç‡: 0.0026\n",
      "24. 'ä¸­å›½å¤ä»£      ' (ID: 115490) - æ¦‚ç‡: 0.0026\n",
      "25. 'æˆ‘ä»¬        ' (ID: 97639) - æ¦‚ç‡: 0.0026\n",
      "26. 'å¤ä»£        ' (ID: 102640) - æ¦‚ç‡: 0.0022\n",
      "27. 'å­”å­        ' (ID: 108752) - æ¦‚ç‡: 0.0022\n",
      "28. 'æ€æƒ³        ' (ID: 100383) - æ¦‚ç‡: 0.0021\n",
      "29. 'åˆ         ' (ID: 99518) - æ¦‚ç‡: 0.0020\n",
      "30. 'ï¼ˆ         ' (ID:  9909) - æ¦‚ç‡: 0.0019\n",
      "31. 'å         ' (ID: 13072) - æ¦‚ç‡: 0.0018\n",
      "32. 'è‘—åçš„       ' (ID: 105891) - æ¦‚ç‡: 0.0016\n",
      "33. 'é“å¾·        ' (ID: 102348) - æ¦‚ç‡: 0.0016\n",
      "34. 'ä»–åœ¨        ' (ID: 104677) - æ¦‚ç‡: 0.0015\n",
      "35. 'å¤         ' (ID: 99470) - æ¦‚ç‡: 0.0015\n",
      "36. 'ä¹Ÿè¢«        ' (ID: 107036) - æ¦‚ç‡: 0.0013\n",
      "37. 'æ˜¯ä¸­å›½       ' (ID: 105196) - æ¦‚ç‡: 0.0013\n",
      "38. 'ç†         ' (ID: 21887) - æ¦‚ç‡: 0.0012\n",
      "39. 'å…¶         ' (ID: 41146) - æ¦‚ç‡: 0.0011\n",
      "40. 'å…ˆ         ' (ID: 60726) - æ¦‚ç‡: 0.0011\n",
      "41. 'å­¦         ' (ID: 47764) - æ¦‚ç‡: 0.0011\n",
      "42. 'åŒæ—¶ä¹Ÿ       ' (ID: 104979) - æ¦‚ç‡: 0.0010\n",
      "43. ' ï¿½        ' (ID: 40666) - æ¦‚ç‡: 0.0010\n",
      "44. 'ä»–å¯¹        ' (ID: 108484) - æ¦‚ç‡: 0.0010\n",
      "45. 'æ–‡åŒ–        ' (ID: 99348) - æ¦‚ç‡: 0.0009\n",
      "46. '____      ' (ID:  2130) - æ¦‚ç‡: 0.0009\n",
      "47. 'è¢«èª‰ä¸º       ' (ID: 109721) - æ¦‚ç‡: 0.0008\n",
      "48. 'ä¸         ' (ID: 57218) - æ¦‚ç‡: 0.0008\n",
      "49. 'ä¸»è¦        ' (ID: 99558) - æ¦‚ç‡: 0.0008\n",
      "50. 'åŒæ—¶        ' (ID: 91572) - æ¦‚ç‡: 0.0008\n",
      "51. 'å¼€åˆ›        ' (ID: 106775) - æ¦‚ç‡: 0.0008\n",
      "52. 'æˆ‘         ' (ID: 35946) - æ¦‚ç‡: 0.0007\n",
      "53. 'å†å²        ' (ID: 100022) - æ¦‚ç‡: 0.0007\n",
      "54. 'åŸ         ' (ID: 52129) - æ¦‚ç‡: 0.0007\n",
      "55. 'å…¶ä¸­        ' (ID: 90919) - æ¦‚ç‡: 0.0006\n",
      "56. 'ä»–è¯´        ' (ID: 104318) - æ¦‚ç‡: 0.0006\n",
      "57. 'ä»         ' (ID: 45181) - æ¦‚ç‡: 0.0006\n",
      "58. 'æ–‡å­¦        ' (ID: 104179) - æ¦‚ç‡: 0.0006\n",
      "59. 'ä¸–ç•Œ        ' (ID: 99489) - æ¦‚ç‡: 0.0006\n",
      "60. 'ç”Ÿæ´»åœ¨       ' (ID: 109141) - æ¦‚ç‡: 0.0006\n",
      "61. 'æ°å‡º        ' (ID: 108906) - æ¦‚ç‡: 0.0006\n",
      "62. 'æ˜¯ä¸€ä½       ' (ID: 109182) - æ¦‚ç‡: 0.0006\n",
      "63. 'ä¼¦ç†        ' (ID: 112811) - æ¦‚ç‡: 0.0006\n",
      "64. 'å¤§å­¦        ' (ID: 99562) - æ¦‚ç‡: 0.0006\n",
      "65. 'ä½œä¸º        ' (ID: 100622) - æ¦‚ç‡: 0.0006\n",
      "66. 'æ›´æ˜¯        ' (ID: 103979) - æ¦‚ç‡: 0.0005\n",
      "67. 'å¯¹ä¸­å›½       ' (ID: 107052) - æ¦‚ç‡: 0.0005\n",
      "68. 'å’Œ         ' (ID: 33108) - æ¦‚ç‡: 0.0005\n",
      "69. 'ä¸»å¼         ' (ID: 106509) - æ¦‚ç‡: 0.0005\n",
      "70. 'ä¸­å›½çš„       ' (ID: 105538) - æ¦‚ç‡: 0.0005\n",
      "71. 'åˆ›ç«‹        ' (ID: 106685) - æ¦‚ç‡: 0.0005\n",
      "72. 'æ•™         ' (ID: 99182) - æ¦‚ç‡: 0.0005\n",
      "73. '(         ' (ID:     7) - æ¦‚ç‡: 0.0004\n",
      "74. 'å¤§å®¶        ' (ID: 99466) - æ¦‚ç‡: 0.0004\n",
      "75. 'ä»¥åŠ        ' (ID: 101034) - æ¦‚ç‡: 0.0004\n",
      "76. 'ä¼Ÿå¤§        ' (ID: 101635) - æ¦‚ç‡: 0.0004\n",
      "77. 'é“         ' (ID: 44793) - æ¦‚ç‡: 0.0004\n",
      "78. 'è€Œä¸”        ' (ID: 101885) - æ¦‚ç‡: 0.0004\n",
      "79. 'äºº         ' (ID: 17340) - æ¦‚ç‡: 0.0004\n",
      "80. 'åš         ' (ID: 99190) - æ¦‚ç‡: 0.0004\n",
      "81. 'å¯¹         ' (ID: 32664) - æ¦‚ç‡: 0.0004\n",
      "82. 'ä¸‹åˆ—        ' (ID: 107976) - æ¦‚ç‡: 0.0004\n",
      "83. 'æå‡º        ' (ID: 101080) - æ¦‚ç‡: 0.0004\n",
      "84. 'ç¤¾ä¼š        ' (ID: 99328) - æ¦‚ç‡: 0.0004\n",
      "85. 'äº¦         ' (ID: 103972) - æ¦‚ç‡: 0.0004\n",
      "86. 'ä¸€ä½        ' (ID: 101961) - æ¦‚ç‡: 0.0004\n",
      "87. 'ä»–ä¹Ÿ        ' (ID: 106133) - æ¦‚ç‡: 0.0004\n",
      "88. '          ' (ID:   220) - æ¦‚ç‡: 0.0003\n",
      "89. 'åˆ›å§‹äºº       ' (ID: 105406) - æ¦‚ç‡: 0.0003\n",
      "90. 'å› ä¸º        ' (ID: 99519) - æ¦‚ç‡: 0.0003\n",
      "91. 'ä½†         ' (ID: 77288) - æ¦‚ç‡: 0.0003\n",
      "92. 'è¿™         ' (ID: 43288) - æ¦‚ç‡: 0.0003\n",
      "93. 'ä»¥ä¸‹        ' (ID: 87752) - æ¦‚ç‡: 0.0003\n",
      "94. 'A         ' (ID:    32) - æ¦‚ç‡: 0.0003\n",
      "95. 'ä»¥         ' (ID: 23031) - æ¦‚ç‡: 0.0003\n",
      "96. 'å†å²ä¸Š       ' (ID: 105881) - æ¦‚ç‡: 0.0003\n",
      "97. 'ä»äº‹        ' (ID: 101193) - æ¦‚ç‡: 0.0003\n",
      "98. 'å› ä¸ºä»–       ' (ID: 106773) - æ¦‚ç‡: 0.0003\n",
      "99. 'ç›¸ä¼         ' (ID: 113135) - æ¦‚ç‡: 0.0003\n",
      "100. ' ï¿½        ' (ID:  4891) - æ¦‚ç‡: 0.0003\n"
     ]
    }
   ],
   "source": [
    "def show_next_token_probabilities(prompt, model, tokenizer, top_k=20):\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # è·å–logits\n",
    "        if hasattr(outputs, 'logits'):\n",
    "            logits = outputs.logits\n",
    "        else:\n",
    "            logits = outputs.last_hidden_state\n",
    "        \n",
    "        # æœ€åä¸€ä¸ªtokençš„logits\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        \n",
    "        # è®¡ç®—æ¦‚ç‡\n",
    "        probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        \n",
    "        # è·å–å‰top_kä¸ªæœ€å¯èƒ½çš„token\n",
    "        top_probs, top_indices = torch.topk(probs, top_k)\n",
    "        \n",
    "        print(f\"ğŸ” æç¤º: '{prompt}'\")\n",
    "        print(f\"ğŸ“Š å‰{top_k}ä¸ªæœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªtoken:\\n\")\n",
    "        \n",
    "        for i, (prob, idx) in enumerate(zip(top_probs[0], top_indices[0])):\n",
    "            token_text = tokenizer.decode([idx])\n",
    "            # å¤„ç†ç‰¹æ®Šå­—ç¬¦çš„æ˜¾ç¤º\n",
    "            display_text = repr(token_text)[1:-1]  # å»æ‰å¼•å·\n",
    "            \n",
    "            print(f\"{i+1:2d}. '{display_text:10s}' (ID: {idx:5d}) - æ¦‚ç‡: {prob.item():.4f}\")\n",
    "\n",
    "# æµ‹è¯•å½“å‰çŠ¶æ€\n",
    "current_prompt = \"é—®é¢˜ï¼šå­”å­æ˜¯è°\\nå­”å­æ˜¯æˆ‘å›½å¤ä»£çš„å¤§æ€æƒ³å®¶ï¼Œ\"\n",
    "show_next_token_probabilities(current_prompt, model, tokenizer, top_k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c5c085-53f4-43e3-abc5-dbf6b9cd1268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70672c-6f34-41a3-bc64-c4a5cbfb372c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa39524-9f94-40d7-b0ec-0cdc9132873d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
