{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3db9f95-e915-4710-bffe-e6e47588452b",
   "metadata": {},
   "source": [
    "# üß© Lab 10: REINFORCE on CartPole \n",
    "\n",
    "In the previous labs, we solved the CartPole control task using a Monte Carlo approach to estimate the value function. We discretized the state space, collected full trajectories, computed returns, and used those returns to update a tabular estimate of \\(Q(s,a)\\).\n",
    "\n",
    "In this lab, we will revisit the CartPole environment, but instead of estimating a value function, we will directly learn a **parameterized policy** using a neural network. This approach is known as **policy gradient**. Rather than selecting actions based on a Q-table, the policy network outputs a probability distribution over actions, and we update its parameters so that actions leading to higher returns become more likely.\n",
    "\n",
    "Our goal is to implement **REINFORCE**, one of the simplest policy‚Äêgradient algorithms:\n",
    "- collect full episodes under the current policy,\n",
    "- compute Monte Carlo returns for each time step,\n",
    "- and adjust the policy parameters in the direction that increases the log‚Äêprobability of good actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a20d8-5b0c-4dd3-b4e2-9c92e5919e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4520a062-b0e5-4c94-9c53-91b46ebe9a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")   # no need to discretize now\n",
    "obs, info = env.reset(seed=0)\n",
    "obs_dim = env.observation_space.shape[0]  # 4 for CartPole\n",
    "n_actions = env.action_space.n            # 2 for CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666fcc4f-a7a9-4856-ba23-2570f40400af",
   "metadata": {},
   "source": [
    "### Task 1: Define a Policy Network `PolicyNet`\n",
    "\n",
    "In this part, you will implement a small neural network that represents the policy  \n",
    "$\\pi_\\theta(a \\mid s)$ for CartPole.\n",
    "\n",
    "The observation space of `CartPole-v1` is a 4-dimensional vector:\n",
    "- cart position\n",
    "- cart velocity\n",
    "- pole angle\n",
    "- pole angular velocity\n",
    "\n",
    "The action space has 2 discrete actions:\n",
    "- `0`: push cart to the left  \n",
    "- `1`: push cart to the right  \n",
    "\n",
    "We will use a **multi-layer perceptron (MLP)** that:\n",
    "- takes the observation \\(s \\in \\mathbb{R}^4\\) as input,\n",
    "- outputs **logits** over the 2 actions (these will go into a `Categorical` distribution),\n",
    "- uses **two hidden layers** with ReLU activations.\n",
    "\n",
    "Hints for architecture:\n",
    "- Use `nn.Sequential` to stack layers.\n",
    "- A common choice for CartPole is:\n",
    "  - First hidden layer: around 100‚Äì150 units (e.g., `128`).\n",
    "  - Second hidden layer: smaller, e.g., about half of the first layer (e.g., `64`).\n",
    "- The final linear layer should map from the second hidden layer to `n_actions`\n",
    "  (no activation on the output layer; the `Categorical` distribution will handle the softmax internally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6934afe5-ca82-42ca-8229-ff11f170ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Policy network œÄ_Œ∏(a | s) -----\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, obs_dim, n_actions):\n",
    "        super().__init__()\n",
    "        # Your time to work on it\n",
    "        self.net = ####\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)  # shape: (batch, n_actions)\n",
    "        return torch.distributions.Categorical(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04832f45-a0ba-4fab-aa26-8acb97a4d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = PolicyNet(obs_dim, n_actions)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)\n",
    "gamma = 0.99\n",
    "num_episodes = 200000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e603189-ca5f-4a7a-8c1c-36a86d896a8a",
   "metadata": {},
   "source": [
    "### Task 2: Implement the REINFORCE Loss Function\n",
    "\n",
    "After collecting one full episode and computing the Monte Carlo returns $G_t$ for\n",
    "each time step, the final step is to update the policy parameters.  \n",
    "In REINFORCE, we adjust the policy in the direction that increases the\n",
    "log-probability of actions that resulted in high returns.\n",
    "\n",
    "Recall the update rule:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\, \\nabla_\\theta \n",
    "\\log \\pi_\\theta(a_t \\mid s_t) \\, G_t.\n",
    "$$\n",
    "\n",
    "In practice, instead of applying this update manually, we construct a **loss\n",
    "function** such that performing gradient descent on the loss produces the same\n",
    "update as gradient ascent on $J(\\theta)$.\n",
    "\n",
    "Your task:\n",
    "\n",
    "1. You have a list of `log_probs`, one for each action taken in the episode.\n",
    "2. You have a list of `returns`, containing the Monte Carlo return $G_t$ for each step.\n",
    "3. Combine them into a single scalar loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008da15-bd0b-4000-8fe0-712609b76453",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_history = []\n",
    "\n",
    "for ep in range(1, num_episodes + 1):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    log_probs = []   # store log œÄ_Œ∏(a_t | s_t)\n",
    "    rewards = []     # store r_t\n",
    "\n",
    "    # Generate an episode\n",
    "    while not done:\n",
    "        s_t = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)   # shape (1, obs_dim)\n",
    "        dist = policy(s_t)                                          # œÄ_Œ∏(. | s_t)\n",
    "        a_t = dist.sample()                                         # sample action\n",
    "        log_prob_t = dist.log_prob(a_t)                             # log œÄ_Œ∏(a_t | s_t)\n",
    "\n",
    "        obs_next, r, term, trunc, _ = env.step(a_t.item())\n",
    "        done = term or trunc\n",
    "\n",
    "        log_probs.append(log_prob_t)\n",
    "        rewards.append(r)\n",
    "\n",
    "        obs = obs_next\n",
    "\n",
    "    #  Value update\n",
    "    T = len(rewards)\n",
    "    G = 0.0\n",
    "    returns = []\n",
    "\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    log_probs = torch.stack(log_probs)      # shape (T,)\n",
    "\n",
    "    # Your time to work on it\n",
    "    loss = ####\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    ep_return = sum(rewards)\n",
    "    returns_history.append(ep_return)\n",
    "\n",
    "    if ep % 100 == 0:\n",
    "        avg = np.mean(returns_history[-50:])\n",
    "        print(f\"Episode {ep:4d} | Return: {ep_return:4.1f} | \"\n",
    "              f\"Avg(50): {avg:5.1f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7addcab-5c31-4644-af1a-bc01dc957a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86683be-8318-4650-864d-553512bbc8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5640ae4f-fa4a-4938-97ac-a12b946a7122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
