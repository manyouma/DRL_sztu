{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4825d8-a0ca-47d8-99f8-5bc7928d805b",
   "metadata": {},
   "source": [
    "# 🧩 Lab 8: TD Methods on CartPole \n",
    "\n",
    "In previous labs, we focused on **❄️ FrozenLake**, a discrete and low-dimensional environment that helped us understand fundamental ideas such as **Markov decision processes (MDPs)**, **state transitions**, and **tabular value functions**.  \n",
    "\n",
    "Now, we move beyond grid worlds and explore a **real control problem** — the **🏗️ CartPole environment**, a continuous and dynamic system widely used in reinforcement learning research.\n",
    "\n",
    "### Algorithms to Implement\n",
    "\n",
    "1. **Monte Carlo (MC)** – Learn from complete episodes using returns to update value estimates.  \n",
    "2. **SARSA (On-Policy TD Control)** – Learn from the agent’s actual actions while following an ε-greedy policy.  \n",
    "3. **Q-Learning (Off-Policy TD Control)** – Learn an optimal policy independent of the behavior policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49189832-7283-4254-94ed-f5135894dfd3",
   "metadata": {},
   "source": [
    "##  Part 1: Understanding the CartPole Environment\n",
    "\n",
    "We now move from static grid worlds to a **dynamic control system** — the classic `CartPole-v1` environment from **Gymnasium**.\n",
    "\n",
    "Let’s begin by creating the environment and taking a few random steps to see how it behaves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b6000-c9a6-4e32-9aac-57a558aa183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "\n",
    "# Use rgb_array so env.render() returns frames\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "obs, info = env.reset(seed=0)\n",
    "\n",
    "# Simple rule: push in the direction that reduces pole angle.\n",
    "# If theta > 0 (pole tilts to the right), push right; else push left.\n",
    "def greedy_policy(observation):\n",
    "    _, _, theta, theta_dot = observation\n",
    "    return 1 if theta > 0 else 0\n",
    "\n",
    "frames = []\n",
    "rewards = []\n",
    "num_steps = 100\n",
    "\n",
    "# capture initial frame\n",
    "frames.append(env.render())\n",
    "\n",
    "for t in range(num_steps):\n",
    "    action = greedy_policy(obs)\n",
    "    obs, r, terminated, truncated, info = env.step(action)\n",
    "    rewards.append(r)\n",
    "    frames.append(env.render())\n",
    "    if terminated or truncated:\n",
    "        # If episode ends early, reset and keep going until 100 frames collected\n",
    "        obs, info = env.reset()\n",
    "        frames.append(env.render())\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Stack frames into a numpy array (T, H, W, C), dtype=uint8\n",
    "frames = np.asarray(frames, dtype=np.uint8)\n",
    "print(f\"Frames shape: {frames.shape}, dtype: {frames.dtype}\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_axis_off()\n",
    "img = ax.imshow(frames[0])\n",
    "\n",
    "def animate(i):\n",
    "    img.set_data(frames[i])\n",
    "    return [img]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(frames), interval=30, blit=True)\n",
    "plt.close(fig)  # prevent duplicate static display\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183101d3-a4f8-4662-9c5f-fb5f1b28acb5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "###  Environment Overview\n",
    "\n",
    "The **CartPole** environment simulates a classic control problem:  \n",
    "A pole is attached by an unactuated joint to a cart that moves along a frictionless track.  \n",
    "The agent must apply forces to the cart (left or right) to keep the pole balanced upright.\n",
    "\n",
    "**Episode Termination Conditions:**\n",
    "- The pole angle exceeds **±12°** (≈ ±0.418 radians)  \n",
    "- The cart position exceeds **±2.4 meters**  \n",
    "- The episode length reaches **500 time steps**\n",
    "\n",
    "At each step, the agent receives a **reward of +1** for keeping the pole balanced.\n",
    "\n",
    "\n",
    "###  Observation (State) Space\n",
    "\n",
    "The observation returned by the environment is a 4-dimensional continuous vector:\n",
    "\n",
    "| Index | Variable | Description | Typical Range |\n",
    "|:------:|-----------|--------------|---------------|\n",
    "| 0 | `x` | Cart position (m) | −2.4 ~ 2.4 |\n",
    "| 1 | `x_dot` | Cart velocity (m/s) | −∞ ~ ∞ |\n",
    "| 2 | `theta` | Pole angle (radians) | −0.418 ~ 0.418 |\n",
    "| 3 | `theta_dot` | Pole angular velocity (radians/s) | −∞ ~ ∞ |\n",
    "\n",
    "These continuous values must often be **discretized** into bins for use with tabular RL algorithms such as **Monte Carlo**, **SARSA**, and **Q-Learning**.\n",
    "\n",
    "\n",
    "###  Action Space\n",
    "\n",
    "| Action | Meaning |\n",
    "|:-------:|---------|\n",
    "| `0` | Push the cart to the **left** |\n",
    "| `1` | Push the cart to the **right** |\n",
    "\n",
    "Each step:\n",
    "1. The agent observes the current state \\( s_t \\).  \n",
    "2. Chooses an action \\( a_t \\in \\{0, 1\\} \\).  \n",
    "3. Receives a reward \\( r_t = +1 \\) if the pole remains upright.  \n",
    "\n",
    "The objective is to **maximize the total cumulative reward**, i.e., keep the pole balanced for as long as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396b2868-5412-43f8-b489-7700ed20114c",
   "metadata": {},
   "source": [
    "## Part 2: Discretizing the State Space\n",
    "\n",
    "Unlike the discrete grid world in **FrozenLake**, the **CartPole** environment has a **continuous state space** — each state is represented by four real-valued variables:\n",
    "$$\n",
    "s = [x, \\dot{x}, \\theta, \\dot{\\theta}]\n",
    "$$\n",
    "To use **tabular methods** such as Monte Carlo, SARSA, or Q-learning, we must convert this continuous state into a **discrete representation**.\n",
    "\n",
    "###  Why Discretize?\n",
    "- Tabular RL algorithms index the Q-table by discrete states.\n",
    "- The continuous variables (like pole angle and velocity) take infinitely many values — impossible to store directly.\n",
    "- We therefore divide each dimension into a finite number of **bins** and map each observation to the corresponding **bin index**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285dcf3-a24a-454c-ae87-8f974a6e7a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# ----- Discretization -----\n",
    "NUM_BINS = np.array([6, 6, 12, 12])\n",
    "STATE_BOUNDS = np.array([\n",
    "    [-2.4,   2.4],\n",
    "    [-3.0,   3.0],\n",
    "    [-0.418, 0.418],\n",
    "    [-2.0,   2.0]\n",
    "])\n",
    "\n",
    "def discretize_state(obs):\n",
    "    lo, hi = STATE_BOUNDS[:,0], STATE_BOUNDS[:,1]\n",
    "    ratios = (np.clip(obs, lo, hi) - lo) / (hi - lo)\n",
    "    bins = (ratios * NUM_BINS).astype(int)\n",
    "    return tuple(np.clip(bins, 0, NUM_BINS - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fe4bef-61ea-4d0e-a01b-8d1bdb5067df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now print the new discrete state space\n",
    "obs, _ = env.reset(seed=0)\n",
    "discrete_state = discretize_state(obs)\n",
    "print(\"Observation:\", obs)\n",
    "print(\"Discretized State:\", discrete_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b88ce9-c229-40b4-b65a-88365a8a9dd4",
   "metadata": {},
   "source": [
    "##  Part 3: Monte Carlo Control on CartPole\n",
    "\n",
    "Now that we have a **discretized state space**, we can apply **Monte Carlo (MC)** learning to estimate the optimal control policy.\n",
    "\n",
    "Monte Carlo methods learn directly from **complete episodes** of experience — they wait until an episode finishes, then update the Q-values based on the **total return** from each visited state–action pair.\n",
    "\n",
    "---\n",
    "\n",
    "###  Key Idea\n",
    "\n",
    "For each episode:\n",
    "1. Generate a trajectory of states, actions, and rewards  \n",
    "   $$\n",
    "   (s_0, a_0, r_1, s_1, a_1, r_2, \\ldots, s_T)\n",
    "   $$\n",
    "2. Compute the **return**\n",
    "   $$\n",
    "   G_t = r_{t+1} + \\gamma r_{t+2} + \\cdots + \\gamma^{T-t-1} r_T\n",
    "   $$\n",
    "3. Update the Q-value:\n",
    "   $$\n",
    "   Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\big(G_t - Q(s_t, a_t)\\big)\n",
    "   $$\n",
    "4. Derive the **policy** using ε-greedy exploration:\n",
    "   $$\n",
    "   \\pi(s) = \n",
    "   \\begin{cases}\n",
    "   \\arg\\max_a Q(s, a) & \\text{(exploit)} \\\\\n",
    "   \\text{random action} & \\text{(explore with probability ε)}\n",
    "   \\end{cases}\n",
    "   $$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249b2207-a70b-4e98-add4-caa5df47641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- ε-greedy policy -----\n",
    "def greedy_action(Q, s, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_actions)\n",
    "    return int(np.argmax(Q[s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76694f73-71ea-4097-98d0-c76b73067afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Initialize tables -----\n",
    "Q = np.zeros((*NUM_BINS, n_actions))\n",
    "N = np.zeros((*NUM_BINS, n_actions), dtype=int)\n",
    "\n",
    "\n",
    "num_episodes = 5000\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "eps_min, eps_decay = 0.05, 0.995\n",
    "returns_history = []\n",
    "\n",
    "\n",
    "for ep in range(1, num_episodes + 1):\n",
    "    obs, _ = env.reset()\n",
    "    episode = []\n",
    "\n",
    "    # Your time to work on it \n",
    "    done = False\n",
    "    while not done:\n",
    "        s = discretize_state(obs)\n",
    "        a = #    \n",
    "        obs_next, r, term, trunc, _ = env.step(a)\n",
    "        episode.append((s, a, r))\n",
    "        obs = obs_next\n",
    "        done = term or trunc\n",
    "\n",
    "    T = len(episode)\n",
    "    G = 0.0\n",
    "    returns = np.zeros(T)\n",
    "    for t in range(T - 1, -1, -1):\n",
    "        _, _, r = episode[t]\n",
    "        G = gamma * G + r\n",
    "        returns[t] = G\n",
    "\n",
    "    for t, (s, a, _) in enumerate(episode):\n",
    "        \n",
    "        N[s][a] = #\n",
    "        n =  #\n",
    "        Q[s][a] = #\n",
    "\n",
    "\n",
    "    epsilon = max(eps_min, epsilon * eps_decay)\n",
    "    ep_return = sum(r for _, _, r in episode)\n",
    "    returns_history.append(ep_return)\n",
    "\n",
    "    if ep % 500 == 0:\n",
    "        avg = np.mean(returns_history[-100:])\n",
    "        print(f\"Episode {ep:5d} | ε={epsilon:.3f} | AvgReturn(100)={avg:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f5866-02da-4032-87df-ef11986e39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_greedy(Q, episodes=20):\n",
    "    total = 0.0\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_ret = 0.0\n",
    "        while not done:\n",
    "            s = discretize_state(obs)\n",
    "            a = np.argmax(Q[s])\n",
    "            obs, r, term, trunc, _ = env.step(a)\n",
    "            ep_ret += r\n",
    "            done = term or trunc\n",
    "        total += ep_ret\n",
    "    return total / episodes\n",
    "\n",
    "avg_eval = evaluate_greedy(Q)\n",
    "print(f\"\\nGreedy policy average return: {avg_eval:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895b4778-ac42-4b85-84a8-ef2308e3386b",
   "metadata": {},
   "source": [
    "## Part 4: n-Step SARSA (On-Policy TD Control)\n",
    "\n",
    "**Goal.** Extend SARSA from 1-step targets to **n-step returns**, trading off bias/variance and enabling smoother learning than pure Monte Carlo.\n",
    "\n",
    "###  Key Idea\n",
    "For each time index $ \\tau $, form the **n-step target** by accumulating up to $n$ future rewards and (if the episode continues) **bootstrapping** from $Q$ at $S_{\\tau+n}, A_{\\tau+n}$:\n",
    "$$\n",
    "G_{\\tau:\\tau+n} \\;=\\; \\sum_{i=\\tau+1}^{\\min(\\tau+n,\\,T)} \\gamma^{\\,i-(\\tau+1)}\\,R_i \\;+\\; \n",
    "\\begin{cases}\n",
    "\\gamma^{\\,n}\\, Q(S_{\\tau+n}, A_{\\tau+n}) & \\text{if } \\tau+n < T,\\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "Update:\n",
    "$$\n",
    "Q(S_\\tau, A_\\tau) \\leftarrow Q(S_\\tau, A_\\tau)\\;+\\;\\alpha\\left[\\,G_{\\tau:\\tau+n} - Q(S_\\tau, A_\\tau)\\,\\right].\n",
    "$$\n",
    "\n",
    "###  Mechanics (buffers & indices)\n",
    "- Maintain rolling buffers: **states** $S_0,S_1,\\dots$, **actions** $A_0,A_1,\\dots$, **rewards** $R_1,R_2,\\dots$.\n",
    "- Let $T$ be the time step when the episode terminates (first terminal or truncation).\n",
    "- At each step $t$, once $ \\tau = t-n+1 \\ge 0 $, compute the target for $(S_\\tau,A_\\tau)$.\n",
    "- Stop when \\( \\tau = T-1 \\).\n",
    "\n",
    "\n",
    "### 📏 Pseudocode (high level)\n",
    "1. Reset env; pick $A_0$ by ε-greedy.\n",
    "2. For $t=0,1,\\dots$:\n",
    "   - Step with $A_t$ → observe $R_{t+1}, S_{t+1}$; pick $A_{t+1}$ ε-greedy (unless terminal).\n",
    "   - Set $ \\tau = t - n + 1 $.  \n",
    "     If $ \\tau \\ge 0 $:  \n",
    "     &nbsp;&nbsp;• Compute $G_{\\tau:\\tau+n}$ (sum of up to $n$ rewards + bootstrap if $\\tau+n<T$).  \n",
    "     &nbsp;&nbsp;• Update $Q(S_\\tau,A_\\tau)$ toward $G_{\\tau:\\tau+n}$.\n",
    "   - Stop when $ \\tau = T-1 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d35cfc-33e1-4b71-9e0c-0e409f40eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Tables & hyperparams -----\n",
    "Q = np.zeros((*NUM_BINS, n_actions))\n",
    "num_episodes = 4000\n",
    "max_steps = 1000\n",
    "gamma = 0.99\n",
    "alpha = 0.1\n",
    "n = 4                           # n-step horizon (tune me)\n",
    "epsilon = 1.0\n",
    "eps_min, eps_decay = 0.05, 0.995\n",
    "\n",
    "returns_hist = []\n",
    "\n",
    "for ep in range(1, num_episodes + 1):\n",
    "    obs, _ = env.reset()\n",
    "    s0 = discretize_state(obs)\n",
    "    a0 = greedy_action(Q, s0, epsilon)\n",
    "\n",
    "    # Buffers (index from 0); r[t] stores r_{t}, so shift by 1 for clarity with algorithm\n",
    "    S = [s0]            # states S_0, S_1, ...\n",
    "    A = [a0]            # actions A_0, A_1, ...\n",
    "    R = [0.0]           # R[0] unused; will append R_1, R_2, ...\n",
    "    T = np.inf\n",
    "\n",
    "    t = 0\n",
    "    ep_return = 0.0\n",
    "\n",
    "    while True:\n",
    "        if t < T:\n",
    "            # Step in env using A_t\n",
    "            obs_next, r, term, trunc, _ = env.step(A[t])\n",
    "            ep_return += r\n",
    "            done = bool(term or trunc)\n",
    "            R.append(r)                       # this is R_{t+1}\n",
    "            if done:\n",
    "                T = t + 1\n",
    "            else:\n",
    "                s_next = discretize_state(obs_next)\n",
    "                S.append(s_next)              # S_{t+1}\n",
    "                a_next = greedy_action(Q, s_next, epsilon)\n",
    "                A.append(a_next)              # A_{t+1}\n",
    "\n",
    "        tau = t - n + 1                       # state to update\n",
    "        if tau >= 0:\n",
    "            # Compute G (n-step return starting at tau)\n",
    "            # G = sum_{i=tau+1}^{min(tau+n, T)} gamma^{i-(tau+1)} R_i\n",
    "\n",
    "            # Your time to work on it \n",
    "            G = 0.0\n",
    "            upper = int(min(tau + n, T))\n",
    "            power = 0\n",
    "            for i in range(tau + 1, upper + 1):\n",
    "                G = ####\n",
    "                power = #### \n",
    "            if tau + n < T:                   # bootstrap if within episode\n",
    "                \n",
    "                G = ######\n",
    "\n",
    "            s_tau = S[tau]\n",
    "            a_tau = A[tau]\n",
    "            Q[s_tau][a_tau]= ##### \n",
    "\n",
    "        if tau == T - 1:\n",
    "            break\n",
    "        t += 1\n",
    "\n",
    "    # ε schedule & logging\n",
    "    epsilon = max(eps_min, epsilon * eps_decay)\n",
    "    returns_hist.append(ep_return)\n",
    "    if ep % 500 == 0:\n",
    "        avg = np.mean(returns_hist[-100:])\n",
    "        print(f\"Episode {ep:5d} | ε={epsilon:.3f} | AvgReturn(100)={avg:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469348c2-ef75-4732-bbf0-0410f8ef6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_eval = evaluate_greedy(Q, episodes=20)\n",
    "print(f\"\\nGreedy policy average return (n={n} SARSA): {avg_eval:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c234a3-4048-465f-b931-c454b6330964",
   "metadata": {},
   "source": [
    "## Part 5: One-Step Q-Learning (Off-Policy TD Control)\n",
    "\n",
    "\n",
    "###  Core Update Rule\n",
    "\n",
    "For each transition $ (s_t, a_t, r_{t+1}, s_{t+1}) $:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\;\\leftarrow\\; Q(s_t, a_t) \\;+\\;\n",
    "\\alpha \\,\\Big[\\, r_{t+1} \\;+\\; \\gamma \\max_{a'} Q(s_{t+1}, a') \\;-\\; Q(s_t, a_t) \\,\\Big].\n",
    "$$\n",
    "\n",
    "- The **target** term  $ r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') $  uses the **best possible next action** under the current estimate $Q$.\n",
    "- Thus, Q-Learning learns an **optimal policy** even while following an **exploratory** (ε-greedy) one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201741c1-b43f-4a00-8083-5b05fe321392",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((*NUM_BINS, n_actions), dtype=float)\n",
    "num_episodes = 4000\n",
    "gamma = 0.99\n",
    "alpha = 0.1\n",
    "epsilon = 1.0\n",
    "eps_min, eps_decay = 0.05, 0.995\n",
    "returns_hist = []\n",
    "\n",
    "for ep in range(1, num_episodes + 1):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    ep_return = 0.0\n",
    "\n",
    "    while not done:\n",
    "        s = discretize_state(obs)\n",
    "        a = greedy_action(Q, s, 1)  # Use random exploration as greedy action\n",
    "\n",
    "        obs_next, r, term, trunc, _ = env.step(a)\n",
    "        done = bool(term or trunc)\n",
    "        ep_return += r\n",
    "\n",
    "        s_next = discretize_state(obs_next)\n",
    "        \n",
    "        # ........ Your time to work on it ........\n",
    "        if done: \n",
    "            td_target = r\n",
    "        else:\n",
    "            td_target = #\n",
    "            \n",
    "        Q[s][a] = #\n",
    "\n",
    "        obs = obs_next\n",
    "\n",
    "    # ε schedule & logging\n",
    "    epsilon = max(eps_min, epsilon * eps_decay)\n",
    "    returns_hist.append(ep_return)\n",
    "    if ep % 500 == 0:\n",
    "        avg = np.mean(returns_hist[-100:])\n",
    "        print(f\"Episode {ep:5d} | ε={epsilon:.3f} | AvgReturn(100)={avg:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c3474c-13bb-4fd6-abf9-cc87cb78135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_eval = evaluate_greedy(Q, episodes=20)\n",
    "print(f\"\\nGreedy policy average return (1-step Q-learning): {avg_eval:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0128f540-ab80-4e67-8f8b-0fbf9cbabf9b",
   "metadata": {},
   "source": [
    "## 🧭 Part 6: Create our own environment\n",
    "\n",
    "We implement a tiny **3×3 GridWorld** to learn how to build a custom Gym environment.\n",
    "\n",
    "- **Start:** $(0,0)$, **Goal:** $(2,2)$  \n",
    "- **Actions:** $0=\\text{Up},\\,1=\\text{Down},\\,2=\\text{Left},\\,3=\\text{Right}$  \n",
    "- **Rewards:** $+1$ on goal, $-0.01$ per step, $-0.05$ for bumping into a wall  \n",
    "- **Termination:** reaching the goal or hitting a step limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fcf565-f3e3-45eb-b3f8-7a8b62459252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Custom 3x3 Grid Environment\n",
    "# ----------------------------\n",
    "class Grid3x3Env(gym.Env):\n",
    "    \"\"\"\n",
    "    A tiny 3x3 GridWorld.\n",
    "      - Start at (0,0), goal at (2,2)\n",
    "      - Actions: 0=Up, 1=Down, 2=Left, 3=Right\n",
    "      - Rewards: +1 on reaching goal, -0.01 per step, -0.05 for bumping into a wall (stay in place)\n",
    "      - Episode ends on goal or step limit\n",
    "    Observation: Discrete(9) index r*3 + c\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"ansi\"]}\n",
    "\n",
    "    def __init__(self, render_mode=None, max_steps=30):\n",
    "        super().__init__()\n",
    "        self.N = 3\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Discrete(self.N * self.N)\n",
    "\n",
    "        self.start = (0, 0)\n",
    "        self.goal  = (2, 2)\n",
    "        self.max_steps = max_steps\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self._pos = None\n",
    "        self._steps = 0\n",
    "\n",
    "        # (dr, dc) for Up, Down, Left, Right\n",
    "        self._moves = [(-1,0), (1,0), (0,-1), (0,1)]\n",
    "\n",
    "    def _rc_to_obs(self, r, c): return r * self.N + c\n",
    "    def _obs_to_rc(self, obs):  return divmod(obs, self.N)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self._pos = self.start\n",
    "        self._steps = 0\n",
    "        return self._rc_to_obs(*self._pos), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self._steps += 1\n",
    "        r, c = self._pos\n",
    "        dr, dc = self._moves[int(action)]\n",
    "        nr, nc = r + dr, c + dc\n",
    "\n",
    "        reward = -0.01\n",
    "        terminated = False\n",
    "        truncated = self._steps >= self.max_steps\n",
    "\n",
    "        # check bounds\n",
    "        if 0 <= nr < self.N and 0 <= nc < self.N:\n",
    "            self._pos = (nr, nc)\n",
    "        else:\n",
    "            # bump wall: stay & extra penalty\n",
    "            reward -= 0.05\n",
    "\n",
    "        if self._pos == self.goal:\n",
    "            reward = 1.0\n",
    "            terminated = True\n",
    "\n",
    "        return self._rc_to_obs(*self._pos), reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self):\n",
    "        board = [[\" . \"]*self.N for _ in range(self.N)]\n",
    "        gr, gc = self.goal\n",
    "        board[gr][gc] = \"[G]\"\n",
    "        r, c = self._pos\n",
    "        board[r][c] = \" A \"\n",
    "        return \"\\n\".join(\"\".join(row) for row in board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3159a4-948f-4cfa-8398-943f519620cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
