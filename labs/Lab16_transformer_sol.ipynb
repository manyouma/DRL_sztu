{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae1214d-4c22-4796-8fce-fc71e6553ceb",
   "metadata": {},
   "source": [
    "## Lab 16 â€” Transformer â€œSanity Testsâ€ \n",
    "\n",
    "In this lab, we will use a real open-source Transformer (via **HuggingFace Transformers**) as a black box and run a set of **basic tests** to understand what is happening inside a modern LLM pipeline. Instead of training from scratch, we focus on **observability**: tokenization behavior, embedding geometry, and next-token prediction.\n",
    "\n",
    "You will:\n",
    "- Load a pretrained tokenizer + causal language model (e.g., **Qwen2.5**).\n",
    "- Inspect **tokenization outputs** (tokens, token IDs, and how spaces are handled).\n",
    "- Extract **input embeddings** and run a small **embedding similarity** experiment across words from different semantic categories.\n",
    "- Implement a simple generation loop to compare **greedy decoding** vs **sampling with temperature**.\n",
    "- Probe the model by printing the **top-k next token probabilities** for a given prompt, and interpret what those probabilities mean.\n",
    "\n",
    "By the end, you should be able to explain:  \n",
    "(1) why tokenization details matter, (2) what embeddings represent geometrically, and (3) how a Transformer turns a prompt into a probability distribution over the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca227f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14daf861-62a4-4921-988f-aac8fabfa0b2",
   "metadata": {},
   "source": [
    "## Obtaining a DeepSeek API Key\n",
    "\n",
    "To call the DeepSeek API, you first need to obtain a personal **API key**.\n",
    "\n",
    "1. Visit the DeepSeek official website:  \n",
    "   https://platform.deepseek.com/\n",
    "\n",
    "2. Sign up for an account (or log in if you already have one).\n",
    "\n",
    "3. Go to the **API / Developer** section of the dashboard.\n",
    "\n",
    "4. Create a new API key and copy it.\n",
    "\n",
    "5. Paste the key into your notebook or script:\n",
    "   ```python\n",
    "   api_key = \"YOUR_API_KEY_HERE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03785eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\"\n",
    "url = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9df67901",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\" \n",
    "#model_name = \"Qwen/Qwen2.5-0.5B\"          \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "#model = AutoModel.from_pretrained(model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f555ede-6096-47b9-a1d2-08a59e74ccf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "698e0eaa-bc6b-4cf3-a566-4e8f55ca65ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "Memory: 8.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f24c85d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: Qwen2TokenizerFast\n",
      "Vocab size: 151643\n",
      "Special token:\n",
      " - CLS: None\n",
      " - SEP: None\n",
      " - PAD: <|endoftext|>\n",
      " - UNK: None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Tokenizer:\", type(tokenizer).__name__)\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Special token:\")\n",
    "print(\" - CLS:\", tokenizer.cls_token)\n",
    "print(\" - SEP:\", tokenizer.sep_token) \n",
    "print(\" - PAD:\", tokenizer.pad_token)\n",
    "print(\" - UNK:\", tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f24dff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original: All happy families are alike; each unhappy family is unhappy in its own way.\n",
      "Tokens: ['All', 'Ä happy', 'Ä families', 'Ä are', 'Ä alike', ';', 'Ä each', 'Ä unhappy', 'Ä family', 'Ä is', 'Ä unhappy', 'Ä in', 'Ä its', 'Ä own', 'Ä way', '.']\n",
      "Token IDs: [2403, 6247, 8521, 525, 25992, 26, 1817, 42151, 2997, 374, 42151, 304, 1181, 1828, 1616, 13]...\n",
      "toke num: 16\n"
     ]
    }
   ],
   "source": [
    "test_texts = \"All happy families are alike; each unhappy family is unhappy in its own way.\"\n",
    "tokens = tokenizer.tokenize(test_texts)\n",
    "token_ids = tokenizer.encode(test_texts)\n",
    "print(f\"\\nOriginal: {test_texts}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids[:30]}...\")\n",
    "print(f\"toke num: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da1fca08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hello world' â†’ ['hello', 'Ä world']\n",
      " Note that  'Ä world' has a space before it\n",
      "'hello  world' â†’ ['hello', 'Ä ', 'Ä world']\n",
      " Note that  'Ä ' has a space before it\n",
      " Note that  'Ä world' has a space before it\n",
      "'hello' â†’ ['hello']\n",
      "' hello' â†’ ['Ä hello']\n",
      " Note that  'Ä hello' has a space before it\n"
     ]
    }
   ],
   "source": [
    "def explain_space_handling():\n",
    "    examples = [\n",
    "        \"hello world\",  \n",
    "        \"hello  world\", \n",
    "        \"hello\",         \n",
    "        \" hello\",     \n",
    "    ]\n",
    "    \n",
    "    for text in examples:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        print(f\"'{text}' â†’ {tokens}\")\n",
    "        for token in tokens:\n",
    "            if 'Ä ' in token:\n",
    "                print(f\" Note that  '{token}' has a space before it\")\n",
    "                \n",
    "explain_space_handling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4481c74-8459-4929-b041-fce2c76820da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(151936, 1536)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = model.get_input_embeddings()\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b56c26b-321e-45c0-8830-89d0a58c763a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Similarity matrix\n",
      "========================================================================================================================\n",
      "                   cat       dog      lion      wind      rain      snow       run      walk      jump\n",
      "         cat: \u001b[92m     1.000\u001b[0m\u001b[92m     0.248\u001b[0m\u001b[91m     0.097\u001b[0m\u001b[91m     0.073\u001b[0m\u001b[91m     0.013\u001b[0m\u001b[91m     0.002\u001b[0m\u001b[91m     0.068\u001b[0m\u001b[91m     0.078\u001b[0m\u001b[91m     0.059\u001b[0m\n",
      "         dog: \u001b[92m     0.248\u001b[0m\u001b[92m     1.000\u001b[0m     0.191\u001b[91m     0.078\u001b[0m\u001b[91m     0.047\u001b[0m     0.113\u001b[91m     0.054\u001b[0m     0.117\u001b[91m     0.038\u001b[0m\n",
      "        lion: \u001b[91m     0.097\u001b[0m     0.191\u001b[92m     1.000\u001b[0m\u001b[91m     0.068\u001b[0m     0.141     0.169\u001b[91m     0.023\u001b[0m\u001b[91m     0.089\u001b[0m\u001b[91m     0.083\u001b[0m\n",
      "        wind: \u001b[91m     0.073\u001b[0m\u001b[91m     0.078\u001b[0m\u001b[91m     0.068\u001b[0m\u001b[92m     1.000\u001b[0m     0.153     0.109\u001b[91m     0.073\u001b[0m     0.165\u001b[91m     0.098\u001b[0m\n",
      "        rain: \u001b[91m     0.013\u001b[0m\u001b[91m     0.047\u001b[0m     0.141     0.153\u001b[92m     1.000\u001b[0m\u001b[92m     0.247\u001b[0m     0.129\u001b[91m     0.089\u001b[0m\u001b[91m     0.003\u001b[0m\n",
      "        snow: \u001b[91m     0.002\u001b[0m     0.113     0.169     0.109\u001b[92m     0.247\u001b[0m\u001b[92m     1.000\u001b[0m\u001b[91m     0.034\u001b[0m\u001b[91m     0.086\u001b[0m\u001b[91m     0.081\u001b[0m\n",
      "         run: \u001b[91m     0.068\u001b[0m\u001b[91m     0.054\u001b[0m\u001b[91m     0.023\u001b[0m\u001b[91m     0.073\u001b[0m     0.129\u001b[91m     0.034\u001b[0m\u001b[92m     1.000\u001b[0m     0.153\u001b[91m     0.090\u001b[0m\n",
      "        walk: \u001b[91m     0.078\u001b[0m     0.117\u001b[91m     0.089\u001b[0m     0.165\u001b[91m     0.089\u001b[0m\u001b[91m     0.086\u001b[0m     0.153\u001b[92m     1.001\u001b[0m     0.160\n",
      "        jump: \u001b[91m     0.059\u001b[0m\u001b[91m     0.038\u001b[0m\u001b[91m     0.083\u001b[0m\u001b[91m     0.098\u001b[0m\u001b[91m     0.003\u001b[0m\u001b[91m     0.081\u001b[0m\u001b[91m     0.090\u001b[0m     0.160\u001b[92m     1.000\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "def get_word_embedding(word, model, tokenizer):\n",
    "    embedding_layer = model.get_input_embeddings()\n",
    "    token_ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "\n",
    "    token_ids = torch.tensor(token_ids, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = embedding_layer(token_ids)   # [num_tokens, dim]\n",
    "        return embeddings.mean(dim=0)            \n",
    "\n",
    "def cosine_similarity_torch(vec1, vec2):\n",
    "    return F.cosine_similarity(\n",
    "        vec1.unsqueeze(0), \n",
    "        vec2.unsqueeze(0), \n",
    "        dim=1\n",
    "    ).item()\n",
    "\n",
    "words = [\"cat\", \"dog\", \"lion\", \"wind\", \"rain\", \"snow\", \"run\", \"walk\", \"jump\"]\n",
    "\n",
    "print(\"ğŸ” Similarity matrix\")\n",
    "print(\"=\" * 120)\n",
    "print(\" \" * 12 + \"\".join([f\"{word:>10}\" for word in words]))\n",
    "\n",
    "\n",
    "word_embeddings = {\n",
    "    word: get_word_embedding(word, model, tokenizer)\n",
    "    for word in words\n",
    "}\n",
    "\n",
    "for word1 in words:\n",
    "    print(f\"{word1:>12}: \", end=\"\")\n",
    "    for word2 in words:\n",
    "        sim = cosine_similarity_torch(\n",
    "            word_embeddings[word1],\n",
    "            word_embeddings[word2]\n",
    "        )\n",
    "\n",
    "        if sim > 0.2:\n",
    "            print(f\"\\033[92m{sim:>10.3f}\\033[0m\", end=\"\")  \n",
    "        elif sim < 0.1:\n",
    "            print(f\"\\033[91m{sim:>10.3f}\\033[0m\", end=\"\")  \n",
    "        else:\n",
    "            print(f\"{sim:>10.3f}\", end=\"\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f53015f3-603c-4f6a-afff-ec40b5cd2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_generate(prompt, model, tokenizer, max_new_tokens=50, entropy=0.5):\n",
    "\n",
    "    device = model.device\n",
    "    \n",
    "    print(f\"Input: '{prompt}'\")\n",
    "    print(f\"Entropy: {entropy} \")\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs.input_ids\n",
    "    \n",
    "    generated_tokens = []\n",
    "    \n",
    "    for i in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            \n",
    "\n",
    "            if hasattr(outputs, 'logits'):\n",
    "                logits = outputs.logits\n",
    "            else:\n",
    "                logits = outputs.last_hidden_state\n",
    "            \n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "\n",
    "            if entropy == 0.0:\n",
    "                next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "                strategy = \"Greedy\"\n",
    "                \n",
    "            else:\n",
    "\n",
    "                temperature = 0.1 + entropy * 1.9 \n",
    "                next_token_logits = next_token_logits / temperature\n",
    "                probs = torch.softmax(next_token_logits, dim=-1)\n",
    "                next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "                strategy = f\"temperature(t={temperature:.1f})\"\n",
    "            \n",
    "            new_token = tokenizer.decode(next_token_id[0], skip_special_tokens=True)\n",
    "            generated_tokens.append(new_token)\n",
    "            \n",
    "            print(f\"Token {i+1}: '{new_token}' ({strategy})\")\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "            \n",
    "            # Stopping Criteria\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "            if new_token in ['\\n', '.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ']:\n",
    "                break\n",
    "    \n",
    "    generated_text = prompt + ''.join(generated_tokens)\n",
    "    print(f\"Test: {generated_text}\")\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fe833f5-6fc9-4243-b36b-e95b1a11a4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question, model, tokenizer, max_answer_tokens=50, entropy=0.5):\n",
    "    \n",
    "\n",
    "    prompt = f\"Questionï¼š{question}\\n\"\n",
    "    print(f\"ğŸ¤” question: {question}\")\n",
    "\n",
    "    full_response = robust_generate(prompt, model, tokenizer, max_new_tokens=max_answer_tokens, entropy=0.5)\n",
    "    answer = full_response.replace(prompt, \"\").strip()\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Final answer: {answer}\")\n",
    "    print(f\"ğŸ“Š Length: {len(answer)} characters\")\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45a839c5-f173-44ef-85b5-e4b589b47ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤” question: å­”å­æ˜¯è°\n",
      "Input: 'Questionï¼šå­”å­æ˜¯è°\n",
      "'\n",
      "Entropy: 0.5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manyo\\.conda\\envs\\atari\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:96: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 1: 'åœ¨' (temperature(t=1.1))\n",
      "Token 2: 'ç™¾å®¶' (temperature(t=1.1))\n",
      "Token 3: 'äº‰' (temperature(t=1.1))\n",
      "Token 4: 'é¸£' (temperature(t=1.1))\n",
      "Token 5: 'çš„æ—¶ä»£' (temperature(t=1.1))\n",
      "Token 6: 'é‡Œ' (temperature(t=1.1))\n",
      "Token 7: 'å­”å­' (temperature(t=1.1))\n",
      "Token 8: 'è¿˜' (temperature(t=1.1))\n",
      "Token 9: 'ç®—æ˜¯' (temperature(t=1.1))\n",
      "Token 10: 'å¤§' (temperature(t=1.1))\n",
      "Token 11: 'æ™ºæ…§' (temperature(t=1.1))\n",
      "Token 12: 'çš„' (temperature(t=1.1))\n",
      "Token 13: 'ä»£è¡¨' (temperature(t=1.1))\n",
      "Token 14: 'ï¼Œ' (temperature(t=1.1))\n",
      "Token 15: 'å' (temperature(t=1.1))\n",
      "Token 16: 'ä¸–' (temperature(t=1.1))\n",
      "Token 17: 'æœ‰' (temperature(t=1.1))\n",
      "Token 18: 'å©´' (temperature(t=1.1))\n",
      "Token 19: 'å„’' (temperature(t=1.1))\n",
      "Token 20: 'å…ˆç”Ÿ' (temperature(t=1.1))\n",
      "Token 21: 'ä¸‹' (temperature(t=1.1))\n",
      "Token 22: 'ç§»' (temperature(t=1.1))\n",
      "Token 23: 'å²çš„' (temperature(t=1.1))\n",
      "Token 24: 'ä»–' (temperature(t=1.1))\n",
      "Token 25: 'å‡ ä¹' (temperature(t=1.1))\n",
      "Token 26: 'å·²' (temperature(t=1.1))\n",
      "Token 27: 'æ¸¸' (temperature(t=1.1))\n",
      "Token 28: 'ä¸–' (temperature(t=1.1))\n",
      "Token 29: 'æ©' (temperature(t=1.1))\n",
      "Token 30: 'é­' (temperature(t=1.1))\n",
      "Token 31: 'ï¼Œ' (temperature(t=1.1))\n",
      "Token 32: 'ä¹Ÿè®¸' (temperature(t=1.1))\n",
      "Token 33: 'å' (temperature(t=1.1))\n",
      "Token 34: 'ä¸–' (temperature(t=1.1))\n",
      "Token 35: 'åª' (temperature(t=1.1))\n",
      "Token 36: 'çŸ¥' (temperature(t=1.1))\n",
      "Token 37: 'å…¶' (temperature(t=1.1))\n",
      "Token 38: 'å­—' (temperature(t=1.1))\n",
      "Token 39: 'è€Œ' (temperature(t=1.1))\n",
      "Token 40: 'ä¸çŸ¥' (temperature(t=1.1))\n",
      "Token 41: 'å…¶' (temperature(t=1.1))\n",
      "Token 42: 'äºº' (temperature(t=1.1))\n",
      "Token 43: 'ã€‚' (temperature(t=1.1))\n",
      "Test: Questionï¼šå­”å­æ˜¯è°\n",
      "åœ¨ç™¾å®¶äº‰é¸£çš„æ—¶ä»£é‡Œå­”å­è¿˜ç®—æ˜¯å¤§æ™ºæ…§çš„ä»£è¡¨ï¼Œåä¸–æœ‰å©´å„’å…ˆç”Ÿä¸‹ç§»å²çš„ä»–å‡ ä¹å·²æ¸¸ä¸–æ©é­ï¼Œä¹Ÿè®¸åä¸–åªçŸ¥å…¶å­—è€Œä¸çŸ¥å…¶äººã€‚\n",
      "\n",
      "ğŸ¯ Final answer: åœ¨ç™¾å®¶äº‰é¸£çš„æ—¶ä»£é‡Œå­”å­è¿˜ç®—æ˜¯å¤§æ™ºæ…§çš„ä»£è¡¨ï¼Œåä¸–æœ‰å©´å„’å…ˆç”Ÿä¸‹ç§»å²çš„ä»–å‡ ä¹å·²æ¸¸ä¸–æ©é­ï¼Œä¹Ÿè®¸åä¸–åªçŸ¥å…¶å­—è€Œä¸çŸ¥å…¶äººã€‚\n",
      "ğŸ“Š Length: 55 characters\n"
     ]
    }
   ],
   "source": [
    "question = \"å­”å­æ˜¯è°\"\n",
    "answer = ask_question(question, model, tokenizer, max_answer_tokens=50, entropy=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74d3f475-6749-4e5e-bcab-b16885ebec4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Promppt: 'é—®é¢˜ï¼šå­”å­æ˜¯è°\n",
      "å­”å­æ˜¯æˆ‘å›½å¤ä»£çš„å¤§æ€æƒ³å®¶ï¼Œ'\n",
      "ğŸ“Š Top 100 predictions for the next token:\n",
      "\n",
      " 1. 'å¤§         ' (ID: 26288) - Probability: 0.4290\n",
      " 2. 'å„’å®¶        ' (ID: 114358) - Probability: 0.1371\n",
      " 3. 'ä¹Ÿæ˜¯        ' (ID: 100000) - Probability: 0.0832\n",
      " 4. 'æ•™è‚²        ' (ID: 99460) - Probability: 0.0435\n",
      " 5. 'ä»–         ' (ID: 42411) - Probability: 0.0363\n",
      " 6. 'æ˜¯         ' (ID: 20412) - Probability: 0.0262\n",
      " 7. 'ä»–æ˜¯        ' (ID: 104878) - Probability: 0.0196\n",
      " 8. 'æ”¿æ²»        ' (ID: 101091) - Probability: 0.0169\n",
      " 9. 'æˆ‘å›½        ' (ID: 101055) - Probability: 0.0147\n",
      "10. 'ä¹Ÿ         ' (ID: 74763) - Probability: 0.0113\n",
      "11. 'åŒæ—¶ä¹Ÿæ˜¯      ' (ID: 111733) - Probability: 0.0104\n",
      "12. 'æ˜¥ç§‹        ' (ID: 109385) - Probability: 0.0098\n",
      "13. 'åˆæ˜¯        ' (ID: 104458) - Probability: 0.0090\n",
      "14. 'æ˜¯æˆ‘å›½       ' (ID: 108659) - Probability: 0.0088\n",
      "15. 'ä¼Ÿå¤§çš„       ' (ID: 107792) - Probability: 0.0079\n",
      "16. 'ä»–çš„        ' (ID: 100648) - Probability: 0.0077\n",
      "17. 'è¢«         ' (ID: 99250) - Probability: 0.0056\n",
      "18. 'è‘—å        ' (ID: 102280) - Probability: 0.0054\n",
      "19. 'å„’         ' (ID: 102907) - Probability: 0.0038\n",
      "20. 'è¢«ç§°ä¸º       ' (ID: 106253) - Probability: 0.0037\n",
      "21. 'æœ‰         ' (ID: 18830) - Probability: 0.0034\n",
      "22. 'å“²å­¦        ' (ID: 105384) - Probability: 0.0033\n",
      "23. 'ä¸­å›½        ' (ID: 58695) - Probability: 0.0026\n",
      "24. 'ä¸­å›½å¤ä»£      ' (ID: 115490) - Probability: 0.0026\n",
      "25. 'æˆ‘ä»¬        ' (ID: 97639) - Probability: 0.0026\n",
      "26. 'å¤ä»£        ' (ID: 102640) - Probability: 0.0022\n",
      "27. 'å­”å­        ' (ID: 108752) - Probability: 0.0022\n",
      "28. 'æ€æƒ³        ' (ID: 100383) - Probability: 0.0021\n",
      "29. 'åˆ         ' (ID: 99518) - Probability: 0.0020\n",
      "30. 'ï¼ˆ         ' (ID:  9909) - Probability: 0.0019\n",
      "31. 'å         ' (ID: 13072) - Probability: 0.0018\n",
      "32. 'è‘—åçš„       ' (ID: 105891) - Probability: 0.0016\n",
      "33. 'é“å¾·        ' (ID: 102348) - Probability: 0.0016\n",
      "34. 'ä»–åœ¨        ' (ID: 104677) - Probability: 0.0015\n",
      "35. 'å¤         ' (ID: 99470) - Probability: 0.0015\n",
      "36. 'ä¹Ÿè¢«        ' (ID: 107036) - Probability: 0.0013\n",
      "37. 'æ˜¯ä¸­å›½       ' (ID: 105196) - Probability: 0.0013\n",
      "38. 'ç†         ' (ID: 21887) - Probability: 0.0012\n",
      "39. 'å…¶         ' (ID: 41146) - Probability: 0.0011\n",
      "40. 'å…ˆ         ' (ID: 60726) - Probability: 0.0011\n",
      "41. 'å­¦         ' (ID: 47764) - Probability: 0.0011\n",
      "42. 'åŒæ—¶ä¹Ÿ       ' (ID: 104979) - Probability: 0.0010\n",
      "43. ' ï¿½        ' (ID: 40666) - Probability: 0.0010\n",
      "44. 'ä»–å¯¹        ' (ID: 108484) - Probability: 0.0010\n",
      "45. 'æ–‡åŒ–        ' (ID: 99348) - Probability: 0.0009\n",
      "46. '____      ' (ID:  2130) - Probability: 0.0009\n",
      "47. 'è¢«èª‰ä¸º       ' (ID: 109721) - Probability: 0.0008\n",
      "48. 'ä¸         ' (ID: 57218) - Probability: 0.0008\n",
      "49. 'ä¸»è¦        ' (ID: 99558) - Probability: 0.0008\n",
      "50. 'åŒæ—¶        ' (ID: 91572) - Probability: 0.0008\n",
      "51. 'å¼€åˆ›        ' (ID: 106775) - Probability: 0.0008\n",
      "52. 'æˆ‘         ' (ID: 35946) - Probability: 0.0007\n",
      "53. 'å†å²        ' (ID: 100022) - Probability: 0.0007\n",
      "54. 'åŸ         ' (ID: 52129) - Probability: 0.0007\n",
      "55. 'å…¶ä¸­        ' (ID: 90919) - Probability: 0.0006\n",
      "56. 'ä»–è¯´        ' (ID: 104318) - Probability: 0.0006\n",
      "57. 'ä»         ' (ID: 45181) - Probability: 0.0006\n",
      "58. 'æ–‡å­¦        ' (ID: 104179) - Probability: 0.0006\n",
      "59. 'ä¸–ç•Œ        ' (ID: 99489) - Probability: 0.0006\n",
      "60. 'ç”Ÿæ´»åœ¨       ' (ID: 109141) - Probability: 0.0006\n",
      "61. 'æ°å‡º        ' (ID: 108906) - Probability: 0.0006\n",
      "62. 'æ˜¯ä¸€ä½       ' (ID: 109182) - Probability: 0.0006\n",
      "63. 'ä¼¦ç†        ' (ID: 112811) - Probability: 0.0006\n",
      "64. 'å¤§å­¦        ' (ID: 99562) - Probability: 0.0006\n",
      "65. 'ä½œä¸º        ' (ID: 100622) - Probability: 0.0006\n",
      "66. 'æ›´æ˜¯        ' (ID: 103979) - Probability: 0.0005\n",
      "67. 'å¯¹ä¸­å›½       ' (ID: 107052) - Probability: 0.0005\n",
      "68. 'å’Œ         ' (ID: 33108) - Probability: 0.0005\n",
      "69. 'ä¸»å¼         ' (ID: 106509) - Probability: 0.0005\n",
      "70. 'ä¸­å›½çš„       ' (ID: 105538) - Probability: 0.0005\n",
      "71. 'åˆ›ç«‹        ' (ID: 106685) - Probability: 0.0005\n",
      "72. 'æ•™         ' (ID: 99182) - Probability: 0.0005\n",
      "73. '(         ' (ID:     7) - Probability: 0.0004\n",
      "74. 'å¤§å®¶        ' (ID: 99466) - Probability: 0.0004\n",
      "75. 'ä»¥åŠ        ' (ID: 101034) - Probability: 0.0004\n",
      "76. 'ä¼Ÿå¤§        ' (ID: 101635) - Probability: 0.0004\n",
      "77. 'é“         ' (ID: 44793) - Probability: 0.0004\n",
      "78. 'è€Œä¸”        ' (ID: 101885) - Probability: 0.0004\n",
      "79. 'äºº         ' (ID: 17340) - Probability: 0.0004\n",
      "80. 'åš         ' (ID: 99190) - Probability: 0.0004\n",
      "81. 'å¯¹         ' (ID: 32664) - Probability: 0.0004\n",
      "82. 'ä¸‹åˆ—        ' (ID: 107976) - Probability: 0.0004\n",
      "83. 'æå‡º        ' (ID: 101080) - Probability: 0.0004\n",
      "84. 'ç¤¾ä¼š        ' (ID: 99328) - Probability: 0.0004\n",
      "85. 'äº¦         ' (ID: 103972) - Probability: 0.0004\n",
      "86. 'ä¸€ä½        ' (ID: 101961) - Probability: 0.0004\n",
      "87. 'ä»–ä¹Ÿ        ' (ID: 106133) - Probability: 0.0004\n",
      "88. '          ' (ID:   220) - Probability: 0.0003\n",
      "89. 'åˆ›å§‹äºº       ' (ID: 105406) - Probability: 0.0003\n",
      "90. 'å› ä¸º        ' (ID: 99519) - Probability: 0.0003\n",
      "91. 'ä½†         ' (ID: 77288) - Probability: 0.0003\n",
      "92. 'è¿™         ' (ID: 43288) - Probability: 0.0003\n",
      "93. 'ä»¥ä¸‹        ' (ID: 87752) - Probability: 0.0003\n",
      "94. 'A         ' (ID:    32) - Probability: 0.0003\n",
      "95. 'ä»¥         ' (ID: 23031) - Probability: 0.0003\n",
      "96. 'å†å²ä¸Š       ' (ID: 105881) - Probability: 0.0003\n",
      "97. 'ä»äº‹        ' (ID: 101193) - Probability: 0.0003\n",
      "98. 'å› ä¸ºä»–       ' (ID: 106773) - Probability: 0.0003\n",
      "99. 'ç›¸ä¼         ' (ID: 113135) - Probability: 0.0003\n",
      "100. ' ï¿½        ' (ID:  4891) - Probability: 0.0003\n"
     ]
    }
   ],
   "source": [
    "def show_next_token_probabilities(prompt, model, tokenizer, top_k=20):\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        if hasattr(outputs, 'logits'):\n",
    "            logits = outputs.logits\n",
    "        else:\n",
    "            logits = outputs.last_hidden_state\n",
    "        \n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        \n",
    "        # Calculate probability\n",
    "        probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        \n",
    "        # Obtain the top_k most likely token\n",
    "        top_probs, top_indices = torch.topk(probs, top_k)\n",
    "        \n",
    "        print(f\"ğŸ” Promppt: '{prompt}'\")\n",
    "        print(f\"ğŸ“Š Top {top_k} predictions for the next token:\\n\")\n",
    "        \n",
    "        for i, (prob, idx) in enumerate(zip(top_probs[0], top_indices[0])):\n",
    "            token_text = tokenizer.decode([idx])\n",
    "\n",
    "            display_text = repr(token_text)[1:-1]  # å»æ‰å¼•å·\n",
    "            \n",
    "            print(f\"{i+1:2d}. '{display_text:10s}' (ID: {idx:5d}) - Probability: {prob.item():.4f}\")\n",
    "\n",
    "\n",
    "current_prompt = \"é—®é¢˜ï¼šå­”å­æ˜¯è°\\nå­”å­æ˜¯æˆ‘å›½å¤ä»£çš„å¤§æ€æƒ³å®¶ï¼Œ\"\n",
    "show_next_token_probabilities(current_prompt, model, tokenizer, top_k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c5c085-53f4-43e3-abc5-dbf6b9cd1268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70672c-6f34-41a3-bc64-c4a5cbfb372c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa39524-9f94-40d7-b0ec-0cdc9132873d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
