{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d05b727c-dfb4-4a99-afa5-3baa4202d0eb",
   "metadata": {},
   "source": [
    "# Lab 15: Benchmarking with Stable-Baselines3 and TensorBoard\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you will transition from **hand-implemented reinforcement learning algorithms** to a **standardized, research-grade deep reinforcement learning workflow** using **Stable-Baselines3 (SB3)** and **TensorBoard**.\n",
    "\n",
    "In earlier labs, you implemented algorithms such as **TRPO** and **PPO** from scratch in order to understand their mathematical foundations, optimization objectives, and implementation details. While such low-level implementations are essential for building intuition, **modern reinforcement learning research and applications almost always rely on well-tested libraries and systematic experiment logging tools**.\n",
    "\n",
    "This lab is designed to bridge that gap by introducing a practical and reproducible RL experimentation pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1fe6a6-e5f4-40fc-9e1f-e08d484f9d07",
   "metadata": {},
   "source": [
    "## Using Stable-Baselines3\n",
    "\n",
    "Stable-Baselines3 (SB3) is a widely used Python library that provides **reliable and standardized implementations of modern reinforcement learning algorithms**. Instead of manually implementing training loops, rollout buffers, and optimization steps, SB3 allows you to focus on **algorithm selection, hyperparameter tuning, and experimental analysis**.\n",
    "\n",
    "Using SB3 typically follows a simple workflow:\n",
    "\n",
    "1. **Create an environment** (often a vectorized environment for efficient data collection)\n",
    "2. **Instantiate a model** by selecting an algorithm and policy type\n",
    "3. **Train the model** using a fixed number of timesteps\n",
    "4. **Save, load, and evaluate** trained policies\n",
    "5. **Monitor training progress** using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ce4e94d-fa29-44f6-83f1-92cf680f96d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./tb_logs\\PPO_Hopper_base_2\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21       |\n",
      "|    ep_rew_mean     | 16.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 1212     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 27.1        |\n",
      "|    ep_rew_mean          | 27.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 867         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014268925 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.21       |\n",
      "|    explained_variance   | 0.0333      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.32        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    std                  | 0.98        |\n",
      "|    value_loss           | 21.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 39.2        |\n",
      "|    ep_rew_mean          | 47.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 797         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015037563 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.17       |\n",
      "|    explained_variance   | 0.443       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.2        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.964       |\n",
      "|    value_loss           | 87.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 59.1        |\n",
      "|    ep_rew_mean          | 100         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 753         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014524755 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.12       |\n",
      "|    explained_variance   | 0.413       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 83.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    std                  | 0.952       |\n",
      "|    value_loss           | 163         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 71.3        |\n",
      "|    ep_rew_mean          | 133         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 744         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 110         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011026027 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.09       |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 81.9        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0197     |\n",
      "|    std                  | 0.939       |\n",
      "|    value_loss           | 196         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 87.4       |\n",
      "|    ep_rew_mean          | 177        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 736        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 133        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00906096 |\n",
      "|    clip_fraction        | 0.107      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.05      |\n",
      "|    explained_variance   | 0.409      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 88.8       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0152    |\n",
      "|    std                  | 0.927      |\n",
      "|    value_loss           | 204        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 97.1        |\n",
      "|    ep_rew_mean          | 205         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 734         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 156         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008841287 |\n",
      "|    clip_fraction        | 0.0991      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.01       |\n",
      "|    explained_variance   | 0.573       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 78.5        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    std                  | 0.917       |\n",
      "|    value_loss           | 175         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 103         |\n",
      "|    ep_rew_mean          | 226         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 738         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 177         |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009000539 |\n",
      "|    clip_fraction        | 0.0975      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.97       |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 77.9        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    std                  | 0.903       |\n",
      "|    value_loss           | 128         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 110         |\n",
      "|    ep_rew_mean          | 249         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 742         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 198         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010087374 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.93       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.4        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    std                  | 0.897       |\n",
      "|    value_loss           | 70.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 55\u001b[0m\n\u001b[0;32m     31\u001b[0m policy_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     32\u001b[0m     activation_fn\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mTanh,            \n\u001b[0;32m     33\u001b[0m     net_arch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(pi\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m], vf\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m])  \n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     36\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m     vec_env,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     tensorboard_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tb_logs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m )\n\u001b[1;32m---> 55\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOTAL_TIMESTEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPPO_Hopper_base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     59\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# 最终模型（再存一次）\u001b[39;00m\n\u001b[0;32m     62\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_hopper_final\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\atari\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\atari\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:337\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdump_logs(iteration)\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\atari\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:275\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 275\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[0;32m    277\u001b[0m th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[1;32m~\\.conda\\envs\\atari\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\atari\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\atari\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "import os\n",
    "import torch\n",
    "\n",
    "ENV_ID = \"Hopper-v4\"\n",
    "SEED = 42\n",
    "N_ENVS = 8\n",
    "TOTAL_TIMESTEPS = 2_000_000\n",
    "SAVE_FREQ = 100_000\n",
    "\n",
    "\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "vec_env = make_vec_env(\n",
    "    ENV_ID,\n",
    "    n_envs=N_ENVS,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=SAVE_FREQ,\n",
    "    save_path=CHECKPOINT_DIR,\n",
    "    name_prefix=\"ppo_hopper\"\n",
    ")\n",
    "\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    activation_fn=torch.nn.Tanh,            \n",
    "    net_arch=dict(pi=[256, 256], vf=[256, 256])  \n",
    ")\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=256,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.0,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    verbose=1,\n",
    "    tensorboard_log = \"./tb_logs\"\n",
    ")\n",
    "\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=TOTAL_TIMESTEPS,\n",
    "    callback=checkpoint_callback,\n",
    "    tb_log_name=\"PPO_Hopper_base\"\n",
    ")\n",
    "\n",
    "# 最终模型（再存一次）\n",
    "model.save(\"ppo_hopper_final\")\n",
    "\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ffd44a-4346-4809-be1c-138f3787876d",
   "metadata": {},
   "source": [
    "To monitor training progress, TensorBoard can be launched from the project directory by running  \n",
    "`tensorboard --logdir tb_logs`.  \n",
    "This command starts a local web server (typically at `http://localhost:6006`) that visualizes the logs generated during training. Each run appears as a separate entry and can be enabled or disabled for comparison. TensorBoard allows you to inspect learning curves such as episode reward, loss terms, and training speed, making it an essen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b6fa7-deec-43e8-ad32-79bd5421883b",
   "metadata": {},
   "source": [
    "## Algorithm Comparison Task\n",
    "\n",
    "In this part of the lab, you will use **Stable-Baselines3** to solve the same continuous control task using four different reinforcement learning algorithms: **PPO, TRPO, DDPG, and SAC**. All algorithms should be trained under a **consistent experimental setup**, including the same environment, comparable network architectures, and a fixed training budget.\n",
    "\n",
    "You are required to log all training runs using **TensorBoard** and compare their performance by **plotting the episode reward curves**. Through this comparison, you should analyze differences in learning speed, training stability, and final performance. The goal is not only to determine which algorithm performs best, but also to understand *why* different algorithmic designs lead to different learning behaviors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f3853-40fb-4ab0-805f-77bd575c5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your time to work on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608e379-d75b-4662-b356-bcc5ad8382da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5592c3-5290-4ea4-9952-23f1f6da7826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b165af-415f-4428-becf-c95c2f98bb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a954e0-7cbe-46bf-b7a5-4aa4691f0de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
