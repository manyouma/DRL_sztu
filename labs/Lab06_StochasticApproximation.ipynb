{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 6 â€” Stochastic Approximation and PyTorch\n",
        "\n",
        "In this lab we study **stochastic approximation**â€”a family of methods that replace exact expectations with **sample-based updates**.\n",
        "\n",
        "## ðŸŽ¯ Objectives\n",
        "- Understand **Robbinsâ€“Monro** stochastic approximation and its link to mean estimation.\n",
        "- Implement **SGD (single-sample updates)** in NumPy/PyTorch.\n",
        "- Use **PyTorch autograd** to avoid manual gradient code.\n",
        "- Extend SGD to **mini-batch (batched) SGD** and compare behaviors."
      ],
      "metadata": {
        "id": "kGzDIP11zpm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part I: Averaging Update Rule\n",
        "\n",
        "We start from the recursive update:\n",
        "\n",
        "$\n",
        "w_{k+1} = w_k - \\frac{1}{k}\\bigl(w_k - x_k\\bigr)\n",
        "$\n"
      ],
      "metadata": {
        "id": "WEP9SUR1Str-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# generate some random data samples\n",
        "np.random.seed(0)\n",
        "x = np.random.randn(10)\n",
        "print(\"Samples:\", np.round(x, 3))\n",
        "\n",
        "# initialize\n",
        "w = 0.0\n",
        "w_list = []\n",
        "\n",
        "# recursive averaging using the update rule\n",
        "for k in range(1, len(x)+1):\n",
        "\n",
        "    # Your time to work on it !\n",
        "\n",
        "    w = #\n",
        "\n",
        "\n",
        "    w_list.append(w)\n",
        "\n",
        "# direct averages for comparison\n",
        "mean_list = [np.mean(x[:k]) for k in range(1, len(x)+1)]\n",
        "\n",
        "# compare both sequences\n",
        "print(\"\\nStep | Recursive w_k | Direct mean\")\n",
        "print(\"-\"*35)\n",
        "for k, (wk, mk) in enumerate(zip(w_list, mean_list), 1):\n",
        "    print(f\"{k:>4} | {wk:>13.6f} | {mk:>13.6f}\")\n"
      ],
      "metadata": {
        "id": "lcUwPs_1WN62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part II: Robbinsâ€“Monro Algorithm â€” Convergence Example\n",
        "\n",
        "We now illustrate the **Robbinsâ€“Monro stochastic approximation algorithm**.\n",
        "\n",
        "The goal is to find the root $x^\\star$ of an unknown function $g(x)$ such that $\n",
        "g(x^\\star) = 0$.\n",
        "\n",
        "---\n",
        "\n",
        "### Example Setup\n",
        "\n",
        "- $ g(x) = \\tanh(x - 1) $\n",
        "- True root: $ x^\\star = 1 $\n",
        "- Step size: $ a_k = \\frac{1}{k} $\n",
        "- Noiseless case: $ \\eta_k = 0 $\n",
        "\n",
        "Then the Robbinsâ€“Monro update becomes:\n",
        "\n",
        "$\n",
        "w_{k+1} = w_k - a_k g(w_k)\n",
        "$\n",
        "\n",
        "or explicitly,\n",
        "\n",
        "$\n",
        "w_{k+1} = w_k - \\frac{1}{k} \\tanh(w_k - 1).\n",
        "$\n"
      ],
      "metadata": {
        "id": "7dyqV62oZr88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Robbinsâ€“Monro parameters\n",
        "def g(x):\n",
        "    return np.tanh(x - 1)\n",
        "\n",
        "def robbins_monro(w0=3.0, steps=100):\n",
        "    w = w0\n",
        "    ws = [w]\n",
        "    for k in range(1, steps + 1):\n",
        "\n",
        "        # Your time to work on it\n",
        "        a_k = #\n",
        "        w = #\n",
        "\n",
        "        ws.append(w)\n",
        "    return np.array(ws)\n",
        "\n",
        "ws = robbins_monro(w0=3.0, steps=100)"
      ],
      "metadata": {
        "id": "i_4NTUskV_xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot trajectory\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(ws, label=r\"$w_k$\")\n",
        "plt.axhline(1.0, color='red', linestyle='--', label=r\"true root $x^*=1$\")\n",
        "plt.xlabel(\"Iteration $k$\")\n",
        "plt.ylabel(\"$w_k$\")\n",
        "plt.title(\"Robbinsâ€“Monro Iteration for $g(x)=\\\\tanh(x-1)$\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AmCcdD3C25KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def robbins_monro_path(w0=3.0, steps=15):\n",
        "    w = float(w0)\n",
        "    ws = [w]\n",
        "    segs = []  # list of ((x0,y0),(x1,y1)) segments for cobweb\n",
        "    for k in range(1, steps+1):\n",
        "        gw = g(w)\n",
        "        # vertical up: (w_k, 0) -> (w_k, g(w_k))\n",
        "        segs.append(((w, 0.0), (w, gw)))\n",
        "        # horizontal/diagonal back to axis: (w_k, g(w_k)) -> (w_{k+1}, 0)\n",
        "        a_k = 1.0 / k\n",
        "        w_next = w - a_k * gw\n",
        "        segs.append(((w, gw), (w_next, 0.0)))\n",
        "        w = w_next\n",
        "        ws.append(w)\n",
        "    return np.array(ws), segs\n",
        "\n",
        "# Build trajectory\n",
        "ws, segs = robbins_monro_path(w0=3.0, steps=25)\n",
        "\n",
        "# Plot g(w)\n",
        "wgrid = np.linspace(0, 4, 400)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(wgrid, g(wgrid), linewidth=2)\n",
        "\n",
        "# Axes lines and true root\n",
        "plt.axhline(0.0, linestyle='-')\n",
        "plt.axvline(1.0, linestyle='--')\n",
        "\n",
        "# Cobweb segments\n",
        "for (x0,y0),(x1,y1) in segs:\n",
        "    # up to curve\n",
        "    if y1 != 0.0:\n",
        "        plt.plot([x0,x1],[y0,y1], linestyle='-')\n",
        "    # down to axis\n",
        "    else:\n",
        "        plt.plot([x0,x1],[y0,y1], linestyle='--')\n",
        "\n",
        "# Mark w_k points on the axis\n",
        "plt.plot(ws, np.zeros_like(ws), marker='o', linestyle='')\n",
        "\n",
        "# Labels / limits\n",
        "plt.xlabel(\"$w$\")\n",
        "plt.ylabel(\"$g(w)$\")\n",
        "plt.title(r\"Convergence of Robbinsâ€“Monro Algorithm\")\n",
        "plt.xlim(0,4)\n",
        "plt.ylim(-1.2,1.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "peNysdpVbNrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part III: Full Gradient Descent\n",
        "\n",
        "We consider\n",
        "$$\n",
        "J(\\mathbf w)=\\mathbb E\\!\\left[\\tfrac12\\lVert \\mathbf w-\\mathbf X\\rVert^2\\right],\n",
        "\\qquad\n",
        "f(\\mathbf w,\\mathbf X)=\\tfrac12\\lVert \\mathbf w-\\mathbf X\\rVert^2,\n",
        "\\qquad\n",
        "\\nabla_{\\mathbf w} f(\\mathbf w,\\mathbf X)=\\mathbf w-\\mathbf X.\n",
        "$$\n",
        "\n",
        "\n",
        "This formulation can be interpreted as a **mean estimation problem** under a **Gaussian distribution**:\n",
        "- Suppose the random variable $\\mathbf X \\sim \\mathrm{Normal}(\\boldsymbol\\mu, \\Sigma)$.\n",
        "- The goal is to find a parameter vector $\\mathbf w$ that minimizes the *expected squared distance* between $\\mathbf w$ and samples $\\mathbf X$.\n",
        "- The function $f(\\mathbf w, \\mathbf X)$ measures the instantaneous â€œcostâ€ of choosing $\\mathbf w$ when the observed sample is $\\mathbf X$.\n",
        "- The expected loss $J(\\mathbf w)$ is minimized when $\\mathbf w$ equals the **true mean** $\\boldsymbol\\mu$.\n",
        "\n",
        "\n",
        "\n",
        "Since the expectation is over $\\mathbf X$,\n",
        "$$\n",
        "\\nabla J(\\mathbf w)=\\mathbb E[\\mathbf w-\\mathbf X]=\\mathbf w-\\boldsymbol\\mu,\n",
        "\\quad\\text{where } \\boldsymbol\\mu=\\mathbb E[\\mathbf X].\n",
        "$$\n",
        "\n",
        "Thus \\(J\\) is minimized at\n",
        "$$\n",
        "\\mathbf w^\\star=\\boldsymbol\\mu.\n",
        "$$\n",
        "\n",
        "**Full (deterministic) GD** with step size $\\eta>0$:\n",
        "$$\n",
        "\\mathbf w_{k+1}\n",
        "= \\mathbf w_k - \\eta\\,(\\mathbf w_k-\\boldsymbol\\mu)\n",
        "= (1-\\eta)\\,\\mathbf w_k + \\eta\\,\\boldsymbol\\mu .\n",
        "$$"
      ],
      "metadata": {
        "id": "5K9ao60eeNEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mu = np.array([1.5, 0.7])   # true mean\n",
        "eta = 0.3                   # step size, must satisfy 0 < eta < 2\n",
        "w0 = np.array([3.0, -1.5])  # starting point\n",
        "steps = 7\n",
        "\n",
        "def J(w, mu):\n",
        "    return 0.5*np.sum((w - mu)**2)\n",
        "\n",
        "# run GD\n",
        "ws = [w0.copy()]\n",
        "w = w0.copy()\n",
        "for k in range(steps):\n",
        "\n",
        "    # Your time to work on it\n",
        "    grad =            # âˆ‡J(w) = w - Î¼\n",
        "    w = #\n",
        "    ws.append(w.copy())\n",
        "ws = np.array(ws)"
      ],
      "metadata": {
        "id": "DME2Dn09eOv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting functions\n",
        "xx = np.linspace(mu[0]-3.0, mu[0]+3.0, 201)\n",
        "yy = np.linspace(mu[1]-3.0, mu[1]+3.0, 201)\n",
        "XX, YY = np.meshgrid(xx, yy)\n",
        "ZZ = 0.5*((XX-mu[0])**2 + (YY-mu[1])**2)\n",
        "plt.figure(figsize=(5.5,5))\n",
        "CS = plt.contour(XX, YY, ZZ, levels=15)\n",
        "plt.clabel(CS, inline=1, fontsize=8)\n",
        "\n",
        "plt.plot(ws[:,0], ws[:,1], marker='o', linewidth=1, ms=4)\n",
        "\n",
        "for i in range(len(ws)):\n",
        "    plt.text(ws[i,0], ws[i,1], str(i), fontsize=8, ha='center', va='bottom')"
      ],
      "metadata": {
        "id": "tGGQuKhlmebl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part IV: Stochastic Gradient Descent (SGD)\n",
        "\n",
        "### ðŸ§© Step 1: Sampling Data Points\n",
        "\n",
        "In practice, we often **donâ€™t have access to the full function or distribution** that generates our data â€” we only observe **samples** drawn from it.  For example, in machine learning we never know the *true* relationship between inputs and outputs; we only get data points collected from the real world.\n",
        "\n",
        "To mimic this situation, we generate **40 random data samples** from a **2D Gaussian distribution** with a known (but hidden) true mean.  \n",
        "Our goal later will be to estimate this mean using **Stochastic Gradient Descent (SGD)**, as if we didnâ€™t know it beforehand."
      ],
      "metadata": {
        "id": "VbEAsbw9nIVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- 1) Sample 40 points -----\n",
        "np.random.seed(7)\n",
        "mu_true = np.array([1.5, 0.7])\n",
        "Sigma = np.array([[0.5, 0.2],\n",
        "                  [0.2, 0.3]])\n",
        "N = 40\n",
        "X = np.random.multivariate_normal(mu_true, Sigma, size=N)\n",
        "X_bar = X.mean(axis=0)"
      ],
      "metadata": {
        "id": "6HvzAzHmnXEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- 2) Plot dataset, contours -----\n",
        "plt.figure(figsize=(6,6))\n",
        "cs = plt.contour(xx, yy, ZZ, levels=12)\n",
        "plt.clabel(cs, inline=1, fontsize=8)\n",
        "\n",
        "# data points and means\n",
        "plt.scatter(X[:,0], X[:,1], alpha=0.3, label=\"samples $X_i$\")\n",
        "plt.scatter(X_bar[0], X_bar[1], marker=\"*\", s=160, label=\"sample mean $\\\\bar X$\")\n",
        "plt.scatter(mu_true[0], mu_true[1], marker=\"x\", s=90, label=\"true mean $\\\\mu$\")\n",
        "\n",
        "\n",
        "plt.title(\"Sampled Data Points (N=40)\")\n",
        "plt.xlabel(\"$w_1$\"); plt.ylabel(\"$w_2$\")\n",
        "plt.axis(\"equal\"); plt.grid(True); plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8pdu7XQGrMN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ§© Step 2: Stochastic Gradient Descent\n",
        "However, in practice we only observe one sample $\\mathbf X_i$ at a time.  \n",
        "To approximate the gradient, we use the **instantaneous gradient** from that single sample:\n",
        "\n",
        "$\n",
        "\\nabla_{\\mathbf w} f(\\mathbf w, \\mathbf X_i) = \\mathbf w - \\mathbf X_i.\n",
        "$\n",
        "\n",
        "Then, we update the parameter using the SGD rule:\n",
        "\n",
        "$\n",
        "\\mathbf w_{t+1} = \\mathbf w_t - \\eta \\, (\\mathbf w_t - \\mathbf X_i),\n",
        "$\n",
        "\n",
        "where $\\eta > 0$ is the **learning rate** that controls the step size."
      ],
      "metadata": {
        "id": "dM1T1uNdmzZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 0.3           # 0 try 0.3 or 0.9 to see different behavior\n",
        "epochs = 1\n",
        "\n",
        "w = w0.copy()\n",
        "ws = [w.copy()]\n",
        "loss_hist = []\n",
        "\n",
        "rng = np.random.default_rng(0)\n",
        "for ep in range(epochs):\n",
        "    idx = np.arange(N)\n",
        "    #rng.shuffle(idx) # pick a random data\n",
        "    for i in idx:\n",
        "\n",
        "        # Your time to work on it\n",
        "        grad_i =          # âˆ‡(1/2)||w - X_i||^2\n",
        "        w =               #\n",
        "        ws.append(w.copy())\n",
        "\n",
        "\n",
        "\n",
        "ws = np.array(ws)"
      ],
      "metadata": {
        "id": "VHycICHmwHPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "cs = plt.contour(xx, yy, ZZ, levels=12)\n",
        "plt.clabel(cs, inline=1, fontsize=8)\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], alpha=0.2, label=\"samples $X_i$\")\n",
        "plt.plot(ws[:,0], ws[:,1], marker='x', ms=5, linewidth=1.2, label=\"SGD (one point/step)\")\n",
        "plt.scatter(mu_true[0], mu_true[1], marker=\"x\", s=90, label=\"true mean $\\\\mu$\")\n",
        "\n",
        "plt.title(\"SGD trajectory on empirical quadratic (one sample per step)\")\n",
        "plt.xlabel(\"$w_1$\"); plt.ylabel(\"$w_2$\")\n",
        "plt.axis(\"equal\"); plt.grid(True); plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H_3T4WZig_bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âš™ï¸ Part V: Using PyTorch â€” no manual gradients needed for SDG\n",
        "\n",
        "With **PyTorch autograd**, we donâ€™t hand-derive or code gradients like $\\nabla_{\\mathbf w} f(\\mathbf w,\\mathbf X)=\\mathbf w-\\mathbf X$.  \n",
        "Instead, we **build the computation with tensors**, and PyTorch computes gradients automatically via **reverse-mode autodiff**.\n"
      ],
      "metadata": {
        "id": "PDN_l37htS2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Torch tensors\n",
        "device = \"cpu\"\n",
        "X_torch = torch.from_numpy(X).float().to(device)\n",
        "X_bar = X_torch.mean(dim=0)\n",
        "\n",
        "# w become the parameter we want to estimate (2D)\n",
        "w = torch.nn.Parameter(torch.tensor([3.0, -1.5], dtype=torch.float32, device=device))\n",
        "\n",
        "# One-sample-at-a-time SGD (you can also use torch.optim.SGD with batch_size=1; here we show manual control)\n",
        "eta = 0.3\n",
        "epochs = 1\n",
        "\n",
        "# Hyperparameters\n",
        "eta = 0.1\n",
        "epochs = 3  # how many times we loop through the dataset\n",
        "ws = []     # record trajectory\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # shuffle the data order at each epoch\n",
        "    indices = torch.arange(N)\n",
        "    #indices = torch.randperm(len(X_torch))\n",
        "\n",
        "    for i in indices:\n",
        "        Xi = X_torch[i]\n",
        "\n",
        "        # Your time to work on it\n",
        "\n",
        "        loss = # loss = 0.5 * ||w - Xi||^2\n",
        "        loss.backward()\n",
        "\n",
        "        # SGD update\n",
        "        with torch.no_grad():\n",
        "            w -= eta * w.grad    # loss.backward() + w.grad gives you the gradient\n",
        "\n",
        "        # clear gradient buffer\n",
        "        w.grad.zero_()\n",
        "        # record current w for visualization\n",
        "        ws.append(w.detach().clone())\n",
        "ws = torch.stack(ws).numpy()"
      ],
      "metadata": {
        "id": "R_tEzv46v5fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "cs = plt.contour(xx, yy, ZZ, levels=12)\n",
        "plt.clabel(cs, inline=1, fontsize=8)\n",
        "plt.scatter(X[:,0], X[:,1], alpha=0.3, label=\"samples $X_i$\")\n",
        "plt.scatter(mu_true[0], mu_true[1], marker=\"x\", s=90, label=\"true mean $\\\\mu$\")\n",
        "plt.plot(ws[:,0], ws[:,1], marker='x', ms=4, linewidth=1., label=\"PyTorch SGD (BS=1)\")\n",
        "plt.title(\"PyTorch SGD trajectory (one sample per step)\")\n",
        "plt.xlabel(\"$w_1$\"); plt.ylabel(\"$w_2$\")\n",
        "plt.axis(\"equal\"); plt.grid(True); plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2g9TIHvBi6GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ“¦ Part VI: Batched SDG and Pytorch Optimizer\n",
        "\n",
        "To reduce the variance of single-sample updates and better utilize vectorized compute, we switch from **one-sample SGD** to **mini-batch SGD**:\n",
        "\n",
        "- Use a **DataLoader** to iterate over **batches of 4 samples**.\n",
        "- Define the mini-batch loss as  \n",
        "  $\n",
        "  \\frac{1}{B}\\sum_{i=1}^{B} \\tfrac12 \\lVert \\mathbf w - \\mathbf X_i \\rVert^2,\n",
        "  $\n",
        "  i.e., average of per-sample squared distances in the batch.\n",
        "- Let PyTorch handle gradients and updates via **`torch.optim.SGD`**.\n",
        "- Record $ \\mathbf w $ after each optimizer step for visualization later.\n",
        "\n",
        "This keeps the same objective (estimating the Gaussian mean) while making updates **more stable** than single-sample SGD.\n"
      ],
      "metadata": {
        "id": "t5RfNNV0x-Y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Batched SGD with batch_size=4 using torch.optim.SGD -----\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "# Assume X (Nx2 numpy) and mu_true, N are already defined\n",
        "X_torch = torch.from_numpy(X).float().to(device)\n",
        "X_bar   = X_torch.mean(dim=0)\n",
        "\n",
        "# Parameter to estimate (2D mean)\n",
        "w = torch.nn.Parameter(torch.tensor([3.0, -1.5], dtype=torch.float32, device=device))\n",
        "\n",
        "# DataLoader for mini-batch sampling\n",
        "batch_size = 4\n",
        "dataset = TensorDataset(X_torch)                 # each item is (Xi,)\n",
        "loader  = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "# Optimizer hyperparams\n",
        "eta    = 0.1\n",
        "epochs = 4\n",
        "opt    = torch.optim.SGD([w], lr=eta) # we now use an optimizer to update w\n",
        "\n",
        "# record trajectory of w\n",
        "ws = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for (Xb,) in loader:  # Xb: [B, 2]\n",
        "        # Mini-batch loss: average of 0.5 * ||w - Xi||^2 over the batch\n",
        "        # Broadcast w (2,) against Xb (B,2) -> per-sample squared norms then mean\n",
        "        per_sample = 0.5 * torch.sum((w - Xb) ** 2, dim=1)\n",
        "        loss = per_sample.mean()\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        ws.append(w.detach().clone())\n",
        "\n",
        "ws = torch.stack(ws).cpu().numpy()\n",
        "\n",
        "print(\"Final estimate (w):\", w.detach().cpu().numpy())\n",
        "print(\"Sample mean (X_bar):\", X_bar.detach().cpu().numpy())\n",
        "print(\"True mean (mu_true):\", mu_true)\n"
      ],
      "metadata": {
        "id": "sIowWNBfv6P3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "cs = plt.contour(xx, yy, ZZ, levels=12)\n",
        "plt.clabel(cs, inline=1, fontsize=8)\n",
        "plt.scatter(X[:,0], X[:,1], alpha=0.3, label=\"samples $X_i$\")\n",
        "plt.scatter(mu_true[0], mu_true[1], marker=\"x\", s=90, label=\"true mean $\\\\mu$\")\n",
        "plt.title(\"PyTorch Batched SGD trajectory\")\n",
        "plt.xlabel(\"$w_1$\"); plt.ylabel(\"$w_2$\")\n",
        "plt.axis(\"equal\"); plt.grid(True); plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P-YNdJigyXjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§© Part VII: MNIST with Mini-batch SGD (PyTorch)\n",
        "\n",
        "In this example, youâ€™ll train a simple classifier on **MNIST** using **mini-batch SGD**.\n",
        "Weâ€™ll use:\n",
        "- `DataLoader` (mini-batches, shuffling),\n",
        "- a small **CNN** (or switch to MLP if you prefer),\n",
        "- `torch.optim.SGD` (tweak `lr`, `momentum`, `batch_size`),\n",
        "- a standard **train / eval** loop with accuracy reporting.\n",
        "\n",
        "**Try:** compare `batch_size âˆˆ {1, 32, 128}`, change `lr`, and toggle `momentum` to see stability and speed differences.\n"
      ],
      "metadata": {
        "id": "8A1wS46W0WE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "import os"
      ],
      "metadata": {
        "id": "0GUJi-XQ0W8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)   # 28x28 -> 28x28\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)  # 14x14 -> 14x14\n",
        "        self.fc1   = nn.Linear(32*7*7, 64)\n",
        "        self.fc2   = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))         # [B,16,28,28]\n",
        "        x = F.max_pool2d(x, 2)            # [B,16,14,14]\n",
        "        x = F.relu(self.conv2(x))         # [B,32,14,14]\n",
        "        x = F.max_pool2d(x, 2)            # [B,32,7,7]\n",
        "        x = x.view(x.size(0), -1)         # flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = SmallCNN().to(device)"
      ],
      "metadata": {
        "id": "3_fL2oLj1U1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Train / Eval Loops -----\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss, total_correct, total = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "\n",
        "        # Your time to work on it\n",
        "        # forward\n",
        "        # .....\n",
        "\n",
        "        # backward + sgd step\n",
        "        # .....\n",
        "\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_correct += (preds == y).sum().item()\n",
        "        total += x.size(0)\n",
        "\n",
        "    return total_loss / total, total_correct / total"
      ],
      "metadata": {
        "id": "pU2UbIML1ubp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, total_correct, total = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_correct += (preds == y).sum().item()\n",
        "        total += x.size(0)\n",
        "    return total_loss / total, total_correct / total"
      ],
      "metadata": {
        "id": "UroZg-y64o4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Device -----\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ----- Data -----\n",
        "# Normalize to mean=0.1307, std=0.3081 (standard for MNIST)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_ds = datasets.MNIST(root=\"./data\", train=True,  download=True, transform=transform)\n",
        "test_ds  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=256,    shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "\n",
        "lr = 0.1\n",
        "momentum = 0.0   # try 0.9 later for smoother updates\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "9b4RPOL41VAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "for ep in range(1, epochs+1):\n",
        "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    te_loss, te_acc = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {ep:02d} | \"\n",
        "          f\"train loss {tr_loss:.4f} acc {tr_acc*100:5.2f}%  | \"\n",
        "          f\"test loss {te_loss:.4f} acc {te_acc*100:5.2f}%\")\n"
      ],
      "metadata": {
        "id": "nbz09YIi1OCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dyf4OX_c0a_6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}