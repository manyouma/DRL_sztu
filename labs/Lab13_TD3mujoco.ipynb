{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46971634-7f02-4245-89b3-e9e6d7a6c035",
   "metadata": {},
   "source": [
    "# Lab: Twin Delayed Deep Deterministic Policy Gradient (TD3) on HalfCheetah\n",
    "\n",
    "In this lab, we extend the previous **DDPG experiment** to its improved variant:  \n",
    "**Twin Delayed Deep Deterministic Policy Gradient (TD3)**, using the **HalfCheetah** continuous control task.\n",
    "\n",
    "HalfCheetah is a standard MuJoCo benchmark that requires learning stable, high-speed locomotion with continuous actions, making it well suited for evaluating actor–critic algorithms.\n",
    "\n",
    "TD3 was proposed to address several well-known failure modes of DDPG, including:\n",
    "- **Q-value overestimation**\n",
    "- **Training instability**\n",
    "- **High sensitivity to hyperparameters**\n",
    "\n",
    "TD3 introduces three key improvements over DDPG:\n",
    "\n",
    "1. **Twin Critics**  \n",
    "   Two independent Q-networks are trained, and the minimum of the two target Q-values is used to reduce overestimation bias.\n",
    "\n",
    "2. **Target Policy Smoothing**  \n",
    "   Gaussian noise is added to the target action when computing the TD target, making the critic less sensitive to sharp changes in the policy.\n",
    "\n",
    "3. **Delayed Policy Updates**  \n",
    "   The actor and target networks are updated less frequently than the critics, improving overall training stability.\n",
    "\n",
    "In this lab, you will:\n",
    "- Implement TD3 on the **HalfCheetah environment**.\n",
    "- Reuse the **same Actor and Critic network structures** from the previous lab.\n",
    "- Compare **DDPG vs TD3** in terms of learning speed, stability, and final performance.\n",
    "\n",
    "By keeping the network architecture unchanged and only modifying the learning algorithm, this lab demonstrates how **algorithmic design alone can significantly improve reinforcement learning performance**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d36fbbd-7940-4be3-93be-894eb8163c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd849eeb-8623-4673-8533-ba697b683622",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(state_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.l1(state))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        x = torch.tanh(self.l3(x))       \n",
    "        return x * self.max_action       \n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Q(s,a) -> R\"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        q = self.l3(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b909a95a-56bf-4635-ba23-6d9ce464f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "\n",
    "        self.state = np.zeros((max_size, state_dim), dtype=np.float32)\n",
    "        self.action = np.zeros((max_size, action_dim), dtype=np.float32)\n",
    "        self.next_state = np.zeros((max_size, state_dim), dtype=np.float32)\n",
    "        self.reward = np.zeros((max_size, 1), dtype=np.float32)\n",
    "        self.done = np.zeros((max_size, 1), dtype=np.float32)\n",
    "\n",
    "    def add(self, s, a, r, s2, d):\n",
    "        self.state[self.ptr] = s\n",
    "        self.action[self.ptr] = a\n",
    "        self.reward[self.ptr] = r\n",
    "        self.next_state[self.ptr] = s2\n",
    "        self.done[self.ptr] = d\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        idx = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "        state = torch.as_tensor(self.state[idx], dtype=torch.float32, device=device)\n",
    "        action = torch.as_tensor(self.action[idx], dtype=torch.float32, device=device)\n",
    "        reward = torch.as_tensor(self.reward[idx], dtype=torch.float32, device=device)\n",
    "        next_state = torch.as_tensor(self.next_state[idx], dtype=torch.float32, device=device)\n",
    "        done = torch.as_tensor(self.done[idx], dtype=torch.float32, device=device)\n",
    "\n",
    "        return state, action, reward, next_state, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c0487-db6f-4325-a50b-6af5eec41492",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"HalfCheetah-v4\"   \n",
    "env = gym.make(env_name)\n",
    "eval_env = gym.make(env_name)\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.reset(seed=seed)\n",
    "eval_env.reset(seed=seed + 1)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "max_episodes = 500\n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "start_timesteps = 25000  \n",
    "expl_noise = 0.1          \n",
    "\n",
    "batch_size = 256\n",
    "discount = 0.99\n",
    "tau = 0.005\n",
    "\n",
    "policy_noise = 0.2     \n",
    "noise_clip = 0.5         \n",
    "policy_delay = 2         \n",
    "\n",
    "\n",
    "actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "actor_target.load_state_dict(actor.state_dict())\n",
    "\n",
    "critic1 = Critic(state_dim, action_dim).to(device)\n",
    "critic2 = Critic(state_dim, action_dim).to(device)\n",
    "critic1_target = Critic(state_dim, action_dim).to(device)\n",
    "critic2_target = Critic(state_dim, action_dim).to(device)\n",
    "critic1_target.load_state_dict(critic1.state_dict())\n",
    "critic2_target.load_state_dict(critic2.state_dict())\n",
    "\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "critic1_optimizer = optim.Adam(critic1.parameters(), lr=1e-3)\n",
    "critic2_optimizer = optim.Adam(critic2.parameters(), lr=1e-3)\n",
    "\n",
    "replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "\n",
    "total_steps = 0\n",
    "gradient_step = 0  \n",
    "\n",
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55102f33-fea5-4857-b4ba-c8ef20619542",
   "metadata": {},
   "source": [
    "## Your time to work on it\n",
    "#### 1. Twin Critics (Overestimation Reduction)\n",
    "\n",
    "Instead of using **one Q-network**, TD3 uses **two independent critics**:\n",
    "\n",
    "- `critic1(s, a)`\n",
    "- `critic2(s, a)`\n",
    "\n",
    "When computing the target Q-value:\n",
    "\n",
    "$$\n",
    "Q_{\\text{target}} = \\min(Q_1', Q_2')\n",
    "$$\n",
    "\n",
    "- You still compute **both critic losses** using **the same TD target**.\n",
    "- You sum the two MSE losses and backpropagate.\n",
    "\n",
    "\n",
    "#### 2. Target Policy Smoothing (Noise on Target Action)\n",
    "When computing the target:\n",
    "\n",
    "$$\n",
    "a' = \\pi_{\\text{target}}(s') + \\epsilon,\\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma)\n",
    "$$\n",
    "\n",
    "- The noise is only added to the **target action**, not the online action.\n",
    "- The noise must be **clipped to a fixed range**.\n",
    "- The final target action must also be **clipped to the valid action bounds**.\n",
    "\n",
    "This prevents the critic from overfitting to narrow action peaks.\n",
    "\n",
    "#### 3. Delayed Policy Updates (Actor Slower Than Critics)\n",
    "\n",
    "In TD3:\n",
    "- The **critics are updated every step**\n",
    "- The **actor is updated once every `d` steps** (e.g., every 2 steps)\n",
    "\n",
    "- Keep a counter such as `gradient_step`.\n",
    "- Only update:\n",
    "  - the actor\n",
    "  - the actor target\n",
    "  - the critic targets  \n",
    "  when `gradient_step % policy_delay == 0`.\n",
    "\n",
    "\n",
    "### 4. Actor Update Rule\n",
    "\n",
    "The policy objective is:\n",
    "\n",
    "$$\n",
    "\\max_\\pi \\; \\mathbb{E}[Q_1(s, \\pi(s))]\n",
    "$$\n",
    "\n",
    "✅ Hint:\n",
    "- Use **only Q₁** for the actor update.\n",
    "- The loss should have the form:\n",
    "\n",
    "```python\n",
    "actor_loss = -critic1(state, actor(state)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "961a7586-70be-4082-b992-252debd17a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  100 | Steps:   22 | Reward:    13.71\n",
      "Episode  200 | Steps:   17 | Reward:    17.29\n",
      "Episode  300 | Steps:   36 | Reward:    30.40\n",
      "Episode  400 | Steps:   15 | Reward:    12.78\n",
      "Episode  500 | Steps:   22 | Reward:    14.02\n"
     ]
    }
   ],
   "source": [
    "for episode in range(1, max_episodes + 1):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0.0\n",
    "\n",
    "    for t in range(max_steps_per_episode):\n",
    "        total_steps += 1\n",
    "\n",
    "        if total_steps < start_timesteps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                s_tensor = torch.as_tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                a_tensor = actor(s_tensor)\n",
    "                action = a_tensor.cpu().numpy().flatten()\n",
    "                action = action + expl_noise * np.random.randn(*action.shape)\n",
    "                action = np.clip(action, -max_action, max_action)\n",
    "\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "\n",
    "        if replay_buffer.size >= batch_size:\n",
    "            gradient_step += 1\n",
    "\n",
    "            state_b, action_b, reward_b, next_state_b, done_b = replay_buffer.sample(\n",
    "                batch_size, device\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                noise = #\n",
    "\n",
    "                next_action_b = #\n",
    "\n",
    "                target_Q1 = #\n",
    "                target_Q2 = #\n",
    "                target_Q = #\n",
    "                target =  #\n",
    "\n",
    "\n",
    "            critic1_loss = #\n",
    "            critic2_loss = #\n",
    "            critic_loss = #\n",
    "\n",
    "            critic1_optimizer.zero_grad()\n",
    "            critic2_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic1_optimizer.step()\n",
    "            critic2_optimizer.step()\n",
    "\n",
    "\n",
    "            if gradient_step % policy_delay == 0:\n",
    "                actor_loss = #\n",
    "                actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                actor_optimizer.step()\n",
    "\n",
    "                # soft update\n",
    "                with torch.no_grad():\n",
    "                    for param, target_param in zip(actor.parameters(), actor_target.parameters()):\n",
    "                        target_param.data.copy_(\n",
    "                            tau * param.data + (1.0 - tau) * target_param.data\n",
    "                        )\n",
    "                    for param, target_param in zip(critic1.parameters(), critic1_target.parameters()):\n",
    "                        target_param.data.copy_(\n",
    "                            tau * param.data + (1.0 - tau) * target_param.data\n",
    "                        )\n",
    "                    for param, target_param in zip(critic2.parameters(), critic2_target.parameters()):\n",
    "                        target_param.data.copy_(\n",
    "                            tau * param.data + (1.0 - tau) * target_param.data\n",
    "                        )\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if episode%100 == 0:\n",
    "        print(f\"Episode {episode:4d} | Steps: {t+1:4d} | Reward: {episode_reward:8.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b219a-8d44-42bb-85a2-05fe21228066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
