{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46971634-7f02-4245-89b3-e9e6d7a6c035",
   "metadata": {},
   "source": [
    "# Lab 13: Twin Delayed Deep Deterministic Policy Gradient (TD3) on HalfCheetah\n",
    "\n",
    "In this lab, we extend the previous **DDPG experiment** to its improved variant:  \n",
    "**Twin Delayed Deep Deterministic Policy Gradient (TD3)**, using the **HalfCheetah** continuous control task.\n",
    "\n",
    "HalfCheetah is a standard MuJoCo benchmark that requires learning stable, high-speed locomotion with continuous actions, making it well suited for evaluating actor–critic algorithms.\n",
    "\n",
    "TD3 was proposed to address several well-known failure modes of DDPG, including:\n",
    "- **Q-value overestimation**\n",
    "- **Training instability**\n",
    "- **High sensitivity to hyperparameters**\n",
    "\n",
    "TD3 introduces three key improvements over DDPG:\n",
    "\n",
    "1. **Twin Critics**  \n",
    "   Two independent Q-networks are trained, and the minimum of the two target Q-values is used to reduce overestimation bias.\n",
    "\n",
    "2. **Target Policy Smoothing**  \n",
    "   Gaussian noise is added to the target action when computing the TD target, making the critic less sensitive to sharp changes in the policy.\n",
    "\n",
    "3. **Delayed Policy Updates**  \n",
    "   The actor and target networks are updated less frequently than the critics, improving overall training stability.\n",
    "\n",
    "In this lab, you will:\n",
    "- Implement TD3 on the **HalfCheetah environment**.\n",
    "- Reuse the **same Actor and Critic network structures** from the previous lab.\n",
    "- Compare **DDPG vs TD3** in terms of learning speed, stability, and final performance.\n",
    "\n",
    "By keeping the network architecture unchanged and only modifying the learning algorithm, this lab demonstrates how **algorithmic design alone can significantly improve reinforcement learning performance**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d36fbbd-7940-4be3-93be-894eb8163c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d8ab93-6b9c-45f3-bf59-0a0b285ec1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"HalfCheetah-v4\"   \n",
    "SAVE_PATH = \"TD3_Cheetah_actor.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd849eeb-8623-4673-8533-ba697b683622",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(state_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.l1(state))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        x = torch.tanh(self.l3(x))       \n",
    "        return x * self.max_action       \n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        q = self.l3(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b909a95a-56bf-4635-ba23-6d9ce464f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "\n",
    "        self.state = np.zeros((max_size, state_dim), dtype=np.float32)\n",
    "        self.action = np.zeros((max_size, action_dim), dtype=np.float32)\n",
    "        self.next_state = np.zeros((max_size, state_dim), dtype=np.float32)\n",
    "        self.reward = np.zeros((max_size, 1), dtype=np.float32)\n",
    "        self.done = np.zeros((max_size, 1), dtype=np.float32)\n",
    "\n",
    "    def add(self, s, a, r, s2, d):\n",
    "        self.state[self.ptr] = s\n",
    "        self.action[self.ptr] = a\n",
    "        self.reward[self.ptr] = r\n",
    "        self.next_state[self.ptr] = s2\n",
    "        self.done[self.ptr] = d\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        idx = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "        state = torch.as_tensor(self.state[idx], dtype=torch.float32, device=device)\n",
    "        action = torch.as_tensor(self.action[idx], dtype=torch.float32, device=device)\n",
    "        reward = torch.as_tensor(self.reward[idx], dtype=torch.float32, device=device)\n",
    "        next_state = torch.as_tensor(self.next_state[idx], dtype=torch.float32, device=device)\n",
    "        done = torch.as_tensor(self.done[idx], dtype=torch.float32, device=device)\n",
    "\n",
    "        return state, action, reward, next_state, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cec48a8-de82-4ce9-9044-80b25805980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "eval_env = gym.make(env_name)\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.reset(seed=seed)\n",
    "eval_env.reset(seed=seed + 1)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "max_episodes = 500\n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "start_timesteps = 25000  \n",
    "expl_noise = 0.1        \n",
    "\n",
    "batch_size = 256\n",
    "discount = 0.99\n",
    "tau = 0.005\n",
    "\n",
    "policy_noise = 0.2     \n",
    "noise_clip = 0.5          \n",
    "policy_delay = 2        \n",
    "\n",
    "actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "actor_target.load_state_dict(actor.state_dict())\n",
    "\n",
    "critic1 = Critic(state_dim, action_dim).to(device)\n",
    "critic2 = Critic(state_dim, action_dim).to(device)\n",
    "critic1_target = Critic(state_dim, action_dim).to(device)\n",
    "critic2_target = Critic(state_dim, action_dim).to(device)\n",
    "critic1_target.load_state_dict(critic1.state_dict())\n",
    "critic2_target.load_state_dict(critic2.state_dict())\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "critic1_optimizer = optim.Adam(critic1.parameters(), lr=1e-3)\n",
    "critic2_optimizer = optim.Adam(critic2.parameters(), lr=1e-3)\n",
    "replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "total_steps = 0\n",
    "gradient_step = 0  \n",
    "\n",
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "961a7586-70be-4082-b992-252debd17a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    1 | Steps: 1000 | Reward:  -301.23\n",
      "Episode    2 | Steps: 1000 | Reward:  -213.73\n",
      "Episode    3 | Steps: 1000 | Reward:  -141.26\n",
      "Episode    4 | Steps: 1000 | Reward:  -220.39\n",
      "Episode    5 | Steps: 1000 | Reward:  -235.70\n",
      "Episode    6 | Steps: 1000 | Reward:  -242.73\n",
      "Episode    7 | Steps: 1000 | Reward:  -195.29\n",
      "Episode    8 | Steps: 1000 | Reward:  -182.95\n",
      "Episode    9 | Steps: 1000 | Reward:  -322.25\n",
      "Episode   10 | Steps: 1000 | Reward:  -339.32\n",
      "Episode   11 | Steps: 1000 | Reward:  -199.36\n",
      "Episode   12 | Steps: 1000 | Reward:  -397.89\n",
      "Episode   13 | Steps: 1000 | Reward:  -273.43\n",
      "Episode   14 | Steps: 1000 | Reward:  -205.63\n",
      "Episode   15 | Steps: 1000 | Reward:  -263.46\n",
      "Episode   16 | Steps: 1000 | Reward:  -162.17\n",
      "Episode   17 | Steps: 1000 | Reward:  -344.87\n",
      "Episode   18 | Steps: 1000 | Reward:  -222.37\n",
      "Episode   19 | Steps: 1000 | Reward:  -380.59\n",
      "Episode   20 | Steps: 1000 | Reward:  -280.07\n",
      "Episode   21 | Steps: 1000 | Reward:  -352.40\n",
      "Episode   22 | Steps: 1000 | Reward:  -141.86\n",
      "Episode   23 | Steps: 1000 | Reward:  -319.60\n",
      "Episode   24 | Steps: 1000 | Reward:  -247.49\n",
      "Episode   25 | Steps: 1000 | Reward:  -311.14\n",
      "Episode   26 | Steps: 1000 | Reward:  -499.96\n",
      "Episode   27 | Steps: 1000 | Reward:  -308.19\n",
      "Episode   28 | Steps: 1000 | Reward:  -528.96\n",
      "Episode   29 | Steps: 1000 | Reward:  -650.97\n",
      "Episode   30 | Steps: 1000 | Reward:  -491.88\n",
      "Episode   31 | Steps: 1000 | Reward:  -690.33\n",
      "Episode   32 | Steps: 1000 | Reward:  -362.32\n",
      "Episode   33 | Steps: 1000 | Reward:  -527.30\n",
      "Episode   34 | Steps: 1000 | Reward:  -520.95\n",
      "Episode   35 | Steps: 1000 | Reward:  -330.28\n",
      "Episode   36 | Steps: 1000 | Reward:  -504.64\n",
      "Episode   37 | Steps: 1000 | Reward:   428.16\n",
      "Episode   38 | Steps: 1000 | Reward:  -164.88\n",
      "Episode   39 | Steps: 1000 | Reward:   266.30\n",
      "Episode   40 | Steps: 1000 | Reward:   349.91\n",
      "Episode   41 | Steps: 1000 | Reward:  -302.09\n",
      "Episode   42 | Steps: 1000 | Reward:   185.86\n",
      "Episode   43 | Steps: 1000 | Reward:   100.65\n",
      "Episode   44 | Steps: 1000 | Reward:  -320.37\n",
      "Episode   45 | Steps: 1000 | Reward:   334.77\n",
      "Episode   46 | Steps: 1000 | Reward:   234.70\n",
      "Episode   47 | Steps: 1000 | Reward:   937.45\n",
      "Episode   48 | Steps: 1000 | Reward:  1018.19\n",
      "Episode   49 | Steps: 1000 | Reward:  1254.96\n",
      "Episode   50 | Steps: 1000 | Reward:  1420.54\n",
      "Episode   51 | Steps: 1000 | Reward:  1647.75\n",
      "Episode   52 | Steps: 1000 | Reward:  1951.06\n",
      "Episode   53 | Steps: 1000 | Reward:  2559.31\n",
      "Episode   54 | Steps: 1000 | Reward:  3044.27\n",
      "Episode   55 | Steps: 1000 | Reward:  2895.16\n",
      "Episode   56 | Steps: 1000 | Reward:  2904.86\n",
      "Episode   57 | Steps: 1000 | Reward:  2786.42\n",
      "Episode   58 | Steps: 1000 | Reward:  2810.23\n",
      "Episode   59 | Steps: 1000 | Reward:  2866.40\n",
      "Episode   60 | Steps: 1000 | Reward:  3007.78\n",
      "Episode   61 | Steps: 1000 | Reward:  2991.19\n",
      "Episode   62 | Steps: 1000 | Reward:  3055.47\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m critic1_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     57\u001b[0m critic2_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 58\u001b[0m \u001b[43mcritic_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m critic1_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     60\u001b[0m critic2_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\.conda\\envs\\atari\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\atari\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\atari\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(1, max_episodes + 1):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0.0\n",
    "\n",
    "    for t in range(max_steps_per_episode):\n",
    "        total_steps += 1\n",
    "\n",
    "        if total_steps < start_timesteps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                s_tensor = torch.as_tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                a_tensor = actor(s_tensor)\n",
    "                action = a_tensor.cpu().numpy().flatten()\n",
    "                action = action + expl_noise * np.random.randn(*action.shape)\n",
    "                action = np.clip(action, -max_action, max_action)\n",
    "\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "\n",
    "        if replay_buffer.size >= batch_size:\n",
    "            gradient_step += 1\n",
    "\n",
    "            state_b, action_b, reward_b, next_state_b, done_b = replay_buffer.sample(\n",
    "                batch_size, device\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                noise = (torch.randn_like(action_b) * policy_noise).clamp(\n",
    "                    -noise_clip, noise_clip\n",
    "                )\n",
    "\n",
    "                next_action_b = actor_target(next_state_b)\n",
    "                next_action_b = (next_action_b + noise).clamp(-max_action, max_action)\n",
    "\n",
    "                target_Q1 = critic1_target(next_state_b, next_action_b)\n",
    "                target_Q2 = critic2_target(next_state_b, next_action_b)\n",
    "                target_Q = torch.min(target_Q1, target_Q2)\n",
    "                target = reward_b + (1.0 - done_b) * discount * target_Q\n",
    "\n",
    "\n",
    "            current_Q1 = critic1(state_b, action_b)\n",
    "            current_Q2 = critic2(state_b, action_b)\n",
    "\n",
    "            critic1_loss = mse_loss(current_Q1, target)\n",
    "            critic2_loss = mse_loss(current_Q2, target)\n",
    "            critic_loss = critic1_loss + critic2_loss\n",
    "\n",
    "            critic1_optimizer.zero_grad()\n",
    "            critic2_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic1_optimizer.step()\n",
    "            critic2_optimizer.step()\n",
    "\n",
    "\n",
    "            if gradient_step % policy_delay == 0:\n",
    "                # Actor loss = - E[ Q1(s, π(s)) ]\n",
    "                actor_loss = -critic1(state_b, actor(state_b)).mean()\n",
    "                actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                actor_optimizer.step()\n",
    "\n",
    "                # soft update\n",
    "                with torch.no_grad():\n",
    "                    for param, target_param in zip(actor.parameters(), actor_target.parameters()):\n",
    "                        target_param.data.copy_(\n",
    "                            tau * param.data + (1.0 - tau) * target_param.data\n",
    "                        )\n",
    "                    for param, target_param in zip(critic1.parameters(), critic1_target.parameters()):\n",
    "                        target_param.data.copy_(\n",
    "                            tau * param.data + (1.0 - tau) * target_param.data\n",
    "                        )\n",
    "                    for param, target_param in zip(critic2.parameters(), critic2_target.parameters()):\n",
    "                        target_param.data.copy_(\n",
    "                            tau * param.data + (1.0 - tau) * target_param.data\n",
    "                        )\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "    print(f\"Episode {episode:4d} | Steps: {t+1:4d} | Reward: {episode_reward:8.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b219a-8d44-42bb-85a2-05fe21228066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae7df9-d8f1-4ed5-86ab-21a5d9a766dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cb6653-aa98-4420-a4b0-719673f30a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f2dcfe-b640-462a-9587-7aa6a73c34e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc83b7d0-1342-423e-9ba2-14d0c5b6ac41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5216a429-e777-4cff-a12a-aaa71cba457a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fdd2f4-5931-4f18-83fb-50cc4dd3d21c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170f7999-b347-442f-97d2-5e2bc6879988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd493368-a059-405e-a157-2c7a32d5ded1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5ef0e-e08e-4031-bc25-ff1ec3cc0926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebb4178-80b6-43a9-94b2-69729933715c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3ef25-f13a-4cb6-a9ac-1b778bc056aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddfa364-d2fe-4aac-ae84-f87daf6f122f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
