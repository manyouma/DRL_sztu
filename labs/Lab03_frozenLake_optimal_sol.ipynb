{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839c9bab",
   "metadata": {},
   "source": [
    "# ðŸ§Š Lab 3 â€” Optimal Policy for Frozen Lake\n",
    "In this lab we will continue our exploration of Markov Decision Processes (MDPs) \n",
    "using the simplified **FrozenLake** environment. Building on Lab 2, we will \n",
    "practice three key ideas:\n",
    "\n",
    "1. **Iterative Policy Evaluation** We compute the value of a given policy by repeatedly applying the Bellman expectation update until convergence, and compare this with the exact closed-form solution from Lab 2.\n",
    "\n",
    "2. **Monte Carlo Simulation**  We estimate state values empirically by running many episodes in the FrozenLake    environment under the same policy, and compare these estimates to our analytical results.\n",
    "\n",
    "3. **Finding the Optimal Policy (Value Iteration)**  We apply value iteration to compute the optimal state values and extract the corresponding optimal policy that maximizes long-term return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b7dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "import random\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3214c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a smaller 3x3 map\n",
    "DESC_3x3 = [\n",
    "    \"SFF\",\n",
    "    \"FHF\",\n",
    "    \"FFG\",\n",
    "]\n",
    "env = gym.make(\"FrozenLake-v1\",desc=DESC_3x3,is_slippery=True, render_mode=\"ansi\")\n",
    "obs, info = env.reset(seed=42)\n",
    "#print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a22bb53-7da1-4bdf-b858-e8841343ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General transition builder for FrozenLake-style maps (works for any rectangular map).\n",
    "# Compatible with Gymnasium's action order: LEFT=0, DOWN=1, RIGHT=2, UP=3\n",
    "\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "ACTIONS = [LEFT, DOWN, RIGHT, UP]\n",
    "DIRS = {\n",
    "    LEFT:  (0, -1),\n",
    "    DOWN:  (1, 0),\n",
    "    RIGHT: (0, 1),\n",
    "    UP:    (-1, 0),\n",
    "}\n",
    "\n",
    "def _grid_to_idx(r: int, c: int, ncols: int) -> int:\n",
    "    return r * ncols + c\n",
    "\n",
    "def _idx_to_grid(s: int, ncols: int) -> Tuple[int, int]:\n",
    "    return divmod(s, ncols)\n",
    "\n",
    "def _clip_move(r: int, c: int, dr: int, dc: int, nrows: int, ncols: int) -> Tuple[int, int]:\n",
    "    rr, cc = r + dr, c + dc\n",
    "    rr = min(max(rr, 0), nrows - 1)\n",
    "    cc = min(max(cc, 0), ncols - 1)\n",
    "    return rr, cc\n",
    "\n",
    "def build_frozenlake_transitions(desc: List[str], is_slippery: bool = True):\n",
    "    \"\"\"\n",
    "    Build transition probabilities for a FrozenLake-like grid.\n",
    "\n",
    "    Args:\n",
    "        desc: list of strings (rows), made of {'S','F','H','G'}.\n",
    "        is_slippery: if True -> stochastic: {left, forward, right} each with prob 1/3.\n",
    "                     if False -> deterministic in the intended direction.\n",
    "\n",
    "    Returns:\n",
    "        P: np.ndarray (S, A, S), transition probabilities\n",
    "        R: np.ndarray (S, A, S), rewards (1 on entering 'G', else 0)\n",
    "        absorbing: np.ndarray (S,), True for 'H' or 'G' (absorbing/self-loop)\n",
    "        shape_2d: (nrows, ncols)\n",
    "        flatten_map: np.ndarray (S,), identity map (r,c)->s ordering\n",
    "    \"\"\"\n",
    "    nrows = len(desc)\n",
    "    ncols = len(desc[0])\n",
    "    S = nrows * ncols\n",
    "    A = 4\n",
    "\n",
    "    grid = np.array([list(row) for row in desc])\n",
    "    is_hole = (grid == 'H')\n",
    "    is_goal = (grid == 'G')\n",
    "    absorbing = (is_hole | is_goal).reshape(-1)\n",
    "\n",
    "    P = np.zeros((S, A, S), dtype=float)\n",
    "    R = np.zeros((S, A, S), dtype=float)\n",
    "\n",
    "    def step_from_state(s: int, a: int) -> int:\n",
    "        r, c = _idx_to_grid(s, ncols)\n",
    "        dr, dc = DIRS[a]\n",
    "        rr, cc = _clip_move(r, c, dr, dc, nrows, ncols)\n",
    "        return _grid_to_idx(rr, cc, ncols)\n",
    "\n",
    "    for s in range(S):\n",
    "        if absorbing[s]:\n",
    "            # Absorbing states self-loop for all actions\n",
    "            for a in ACTIONS:\n",
    "                P[s, a, s] = 1.0\n",
    "            continue\n",
    "\n",
    "        for a in ACTIONS:\n",
    "            if is_slippery:\n",
    "                left = (a - 1) % 4\n",
    "                right = (a + 1) % 4\n",
    "                for aa in [left, a, right]:\n",
    "                    s2 = step_from_state(s, aa)\n",
    "                    P[s, a, s2] += 1.0/3.0\n",
    "            else:\n",
    "                s2 = step_from_state(s, a)\n",
    "                P[s, a, s2] = 1.0\n",
    "\n",
    "            # Reward for ARRIVING at goal\n",
    "            for s2 in range(S):\n",
    "                if P[s, a, s2] > 0:\n",
    "                    rr, cc = _idx_to_grid(s2, ncols)\n",
    "                    if grid[rr, cc] == 'G':\n",
    "                        R[s, a, s2] = 1.0\n",
    "\n",
    "    flatten_map = np.arange(S, dtype=int)\n",
    "    return P, R, absorbing, (nrows, ncols), flatten_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f94c9105-e743-4d41-9c4d-6ce216bc92b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R, absorbing, shape2d, flatmap = build_frozenlake_transitions(DESC_3x3, is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5d5873a-82a7-4ed1-a017-a2222053590c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9, 9)\n"
     ]
    }
   ],
   "source": [
    "T_per_action = [P[:, a, :] for a in range(4)]  # list of 4 matrices, each (S,S)\n",
    "P_all = np.array([T_per_action[0], T_per_action[1], T_per_action[2], T_per_action[3]])\n",
    "print(P_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dc459a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY = [\n",
    "    1,  # state 0 (top-left)\n",
    "    2,  # state 1\n",
    "    1,  # state 2\n",
    "    1,  # state 3\n",
    "    1,  # state 4 (center, possibly hole)\n",
    "    1,  # state 5\n",
    "    2,  # state 6\n",
    "    2,  # state 7\n",
    "    2,  # state 8 (goal state)\n",
    "]\n",
    "\n",
    "n_states = len(POLICY)\n",
    "P_pi = np.zeros((n_states, n_states))\n",
    "for s in range(n_states):\n",
    "    a = POLICY[s]             # action chosen at state s\n",
    "    P_pi[s, :] = P_all[a, s]  # copy the probabilities for that action\n",
    "\n",
    "Reward = [\n",
    "0,  # state 0 (top-left)\n",
    "0,  # state 1\n",
    "0,  # state 2\n",
    "0,  # state 3\n",
    "0,  # state 4 (center, possibly hole)\n",
    "0,  # state 5\n",
    "0,  # state 6\n",
    "0,  # state 7\n",
    "1,  # state 8 (goal state)\n",
    "]\n",
    "Reward = np.array(Reward)\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5933d0b5",
   "metadata": {},
   "source": [
    "## Part 1: Iterative Policy Evaluation\n",
    "\n",
    "In Lab 2, we solved for the state-value function analytically using the \n",
    "closed-form expression:\n",
    "\n",
    "$$\n",
    "V = (I - \\gamma P_\\pi)^{-1} R\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d2a40a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State values V: [0.3  0.36 0.83 0.36 0.   1.58 0.83 1.58 3.68]\n"
     ]
    }
   ],
   "source": [
    "# Solve for the value of the policy I designed \n",
    "I = np.eye(9)\n",
    "A = I - gamma * P_pi\n",
    "V = np.linalg.solve(A, Reward)\n",
    "print(\"State values V:\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474115b8",
   "metadata": {},
   "source": [
    "While exact, this approach requires matrix inversion, which can be expensive \n",
    "for large state spaces. An alternative method is **iterative policy evaluation**, \n",
    "where we repeatedly apply the Bellman expectation update:\n",
    "\n",
    "$$\n",
    "V_{k+1}(s) \\;=\\; R(s) \\;+\\; \\gamma \\sum_{s'} P_\\pi(s,s') \\, V_k(s')\n",
    "$$\n",
    "\n",
    "### Steps\n",
    "1. Initialize the value function arbitrarily (often $V_0 = R$ or $V_0 = 0$).  \n",
    "2. Update all state values simultaneously using the Bellman update.  \n",
    "3. Repeat for a number of iterations, or until values converge.  \n",
    "\n",
    "In this exercise, we will:\n",
    "- Start with $V_0 = R$.  \n",
    "- Run the update for about 50 iterations.  \n",
    "- Observe how the values converge to the same result as the closed-form \n",
    "  solution from Lab 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae52c88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3  0.36 0.83 0.36 0.   1.58 0.83 1.58 3.68]\n"
     ]
    }
   ],
   "source": [
    "# Your time to work on it\n",
    "V_0 = Reward.copy()\n",
    "for i in np.arange(50):\n",
    "    V_0 = Reward + gamma*P_pi@V_0\n",
    "print(V_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6619c08b",
   "metadata": {},
   "source": [
    "## Part 2: Monte Carlo Simulation\n",
    "\n",
    "In Part 1, we evaluated a fixed policy analytically and iteratively.  \n",
    "Now we take a different approach: **simulation**.\n",
    "\n",
    "The idea is to estimate the value of each state by running many episodes \n",
    "of the FrozenLake environment under the same policy, and then averaging \n",
    "the observed discounted returns.\n",
    "\n",
    "Formally, the value of a state is defined as:\n",
    "\n",
    "$$\n",
    "V^\\pi(s) = \\mathbb{E}_\\pi \\Big[ \\sum_{t=0}^{\\infty} \n",
    "    \\gamma^t R_{t+1} \\;\\Big|\\; S_0 = s \\Big]\n",
    "$$\n",
    "\n",
    "Monte Carlo methods approximate this expectation by repeated sampling:\n",
    "\n",
    "1. Start from a given state $s$.  \n",
    "2. Run an episode by following the policy $\\pi$, recording the sequence \n",
    "   of rewards.  \n",
    "3. Compute the discounted return $G = r_1 + \\gamma r_2 + \\gamma^2 r_3 + \\dots$.  \n",
    "4. Repeat many times and take the **average return** as an estimate of $V^\\pi(s)$.  \n",
    "\n",
    "### What to do\n",
    "- Run many episodes (e.g., 5,000) under your chosen policy.  \n",
    "- Collect the empirical state values.  \n",
    "- Compare them with the results from **Part 1** (iterative evaluation) \n",
    "  and **Lab 2** (closed-form solution).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e66b8f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3024339601879312\n",
      "0.3577350783533311\n",
      "0.8212359010718963\n",
      "0.3657170622947161\n",
      "0.0\n",
      "1.5792289097629926\n",
      "0.8396689337714301\n",
      "1.604290363266661\n",
      "3.333333333333334\n"
     ]
    }
   ],
   "source": [
    "# Your time to work on it\n",
    "num_epi = 10000\n",
    "\n",
    "DESC_3x3 = [\n",
    "    \"SFF\",\n",
    "    \"FHF\",\n",
    "    \"FFG\",\n",
    "]\n",
    "env = gym.make(\"FrozenLake-v1\",desc=DESC_3x3,is_slippery=True, render_mode=\"ansi\")\n",
    "gamma = 0.9\n",
    "\n",
    "\n",
    "def run_epi(env, policy, start_state, render=False): \n",
    "    next_state, info = env.reset()\n",
    "    '''\n",
    "    Set the start state\n",
    "    '''\n",
    "    env.unwrapped.s = start_state \n",
    "    next_state = start_state\n",
    "    \n",
    "    current_gamma = 1/gamma\n",
    "    if render:\n",
    "        print(env.render())\n",
    "    while True:\n",
    "        current_gamma *= gamma \n",
    "        action = policy[next_state]\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        if render:\n",
    "            print(f\"state:{next_state}, action: {action}, reward: {reward}\")\n",
    "            print(next_state, reward, terminated, truncated,)\n",
    "            print(env.render())\n",
    "        if terminated or truncated:\n",
    "            if next_state == 8:\n",
    "                epi_reward = current_gamma / (1-gamma)\n",
    "            else:\n",
    "                epi_reward = 0\n",
    "            break\n",
    "    return epi_reward\n",
    "\n",
    "def run_state(state):\n",
    "    total_reward = 0\n",
    "    for i in np.arange(num_epi):\n",
    "        reward = run_epi(env, POLICY, state)\n",
    "        total_reward += reward\n",
    "    return total_reward/num_epi\n",
    "\n",
    "for i in np.arange(9):\n",
    "    print(run_state(i)/3)  # I don't understand where did this /3 come from, let me know if you figure it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd81ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec7f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
