{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839c9bab",
   "metadata": {},
   "source": [
    "# ðŸ§Š Lab 2 â€” Optimal Policy for Frozen Lake\n",
    "In this lab we will continue our exploration of Markov Decision Processes (MDPs) \n",
    "using the simplified **FrozenLake** environment. Building on Lab 2, we will \n",
    "practice three key ideas:\n",
    "\n",
    "1. **Iterative Policy Evaluation** We compute the value of a given policy by repeatedly applying the Bellman expectation update until convergence, and compare this with the exact closed-form solution from Lab 2.\n",
    "\n",
    "2. **Monte Carlo Simulation**  We estimate state values empirically by running many episodes in the FrozenLake    environment under the same policy, and compare these estimates to our analytical results.\n",
    "\n",
    "3. **Finding the Optimal Policy (Value Iteration)**  We apply value iteration to compute the optimal state values and extract the corresponding optimal policy that maximizes long-term return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7b7dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "import random\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3214c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a smaller 3x3 map\n",
    "DESC_3x3 = [\n",
    "    \"SFF\",\n",
    "    \"FHF\",\n",
    "    \"FFG\",\n",
    "]\n",
    "env = gym.make(\"FrozenLake-v1\",desc=DESC_3x3,is_slippery=True, render_mode=\"ansi\")\n",
    "obs, info = env.reset(seed=42)\n",
    "#print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dc459a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_LEFT = [\n",
    "    [2, 1, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "    [1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 0, 3, 1, 0, 1, 0],\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 1, 0, 0, 2, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
    "]\n",
    "\n",
    "P_DOWN = [\n",
    "    [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 1, 3, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0, 2, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 1, 2],\n",
    "]\n",
    "\n",
    "P_RIGHT = [\n",
    "    [1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 2, 0, 0, 1, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 3, 0, 0, 1, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
    "    [0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 1, 2],\n",
    "]\n",
    "\n",
    "P_UP = [\n",
    "    [2, 1, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 0, 3, 1, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0, 2, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 3],\n",
    "]\n",
    "P_all = np.array([P_LEFT, P_DOWN, P_RIGHT, P_UP]).transpose(0, 2, 1)/3\n",
    "POLICY = [\n",
    "    1,  # state 0 (top-left)\n",
    "    2,  # state 1\n",
    "    1,  # state 2\n",
    "    1,  # state 3\n",
    "    1,  # state 4 (center, possibly hole)\n",
    "    1,  # state 5\n",
    "    2,  # state 6\n",
    "    2,  # state 7\n",
    "    2,  # state 8 (goal state)\n",
    "]\n",
    "\n",
    "n_states = len(POLICY)\n",
    "P_pi = np.zeros((n_states, n_states))\n",
    "for s in range(n_states):\n",
    "    a = POLICY[s]             # action chosen at state s\n",
    "    P_pi[s, :] = P_all[a, s]  # copy the probabilities for that action\n",
    "\n",
    "Reward = [\n",
    "0,  # state 0 (top-left)\n",
    "0,  # state 1\n",
    "0,  # state 2\n",
    "0,  # state 3\n",
    "0,  # state 4 (center, possibly hole)\n",
    "0,  # state 5\n",
    "0,  # state 6\n",
    "0,  # state 7\n",
    "1,  # state 8 (goal state)\n",
    "]\n",
    "Reward = np.array(Reward)\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5933d0b5",
   "metadata": {},
   "source": [
    "## Part 1: Iterative Policy Evaluation\n",
    "\n",
    "In Lab 2, we solved for the state-value function analytically using the \n",
    "closed-form expression:\n",
    "\n",
    "$$\n",
    "V = (I - \\gamma P_\\pi)^{-1} R\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d2a40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve for the value of the policy I designed \n",
    "I = np.eye(9)\n",
    "A = I - gamma * P_pi\n",
    "V = np.linalg.solve(A, Reward)\n",
    "print(\"State values V:\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474115b8",
   "metadata": {},
   "source": [
    "While exact, this approach requires matrix inversion, which can be expensive \n",
    "for large state spaces. An alternative method is **iterative policy evaluation**, \n",
    "where we repeatedly apply the Bellman expectation update:\n",
    "\n",
    "$$\n",
    "V_{k+1}(s) \\;=\\; R(s) \\;+\\; \\gamma \\sum_{s'} P_\\pi(s,s') \\, V_k(s')\n",
    "$$\n",
    "\n",
    "### Steps\n",
    "1. Initialize the value function arbitrarily (often $V_0 = R$ or $V_0 = 0$).  \n",
    "2. Update all state values simultaneously using the Bellman update.  \n",
    "3. Repeat for a number of iterations, or until values converge.  \n",
    "\n",
    "In this exercise, we will:\n",
    "- Start with $V_0 = R$.  \n",
    "- Run the update for about 50 iterations.  \n",
    "- Observe how the values converge to the same result as the closed-form \n",
    "  solution from Lab 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae52c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your time to work on it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6619c08b",
   "metadata": {},
   "source": [
    "## Part 2: Monte Carlo Simulation\n",
    "\n",
    "In Part 1, we evaluated a fixed policy analytically and iteratively.  \n",
    "Now we take a different approach: **simulation**.\n",
    "\n",
    "The idea is to estimate the value of each state by running many episodes \n",
    "of the FrozenLake environment under the same policy, and then averaging \n",
    "the observed discounted returns.\n",
    "\n",
    "Formally, the value of a state is defined as:\n",
    "\n",
    "$$\n",
    "V^\\pi(s) = \\mathbb{E}_\\pi \\Big[ \\sum_{t=0}^{\\infty} \n",
    "    \\gamma^t R_{t+1} \\;\\Big|\\; S_0 = s \\Big]\n",
    "$$\n",
    "\n",
    "Monte Carlo methods approximate this expectation by repeated sampling:\n",
    "\n",
    "1. Start from a given state $s$.  \n",
    "2. Run an episode by following the policy $\\pi$, recording the sequence \n",
    "   of rewards.  \n",
    "3. Compute the discounted return $G = r_1 + \\gamma r_2 + \\gamma^2 r_3 + \\dots$.  \n",
    "4. Repeat many times and take the **average return** as an estimate of $V^\\pi(s)$.  \n",
    "\n",
    "### What to do\n",
    "- Run many episodes (e.g., 5,000) under your chosen policy.  \n",
    "- Collect the empirical state values.  \n",
    "- Compare them with the results from **Part 1** (iterative evaluation) \n",
    "  and **Lab 2** (closed-form solution).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b8f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your time to work on it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fbcca0",
   "metadata": {},
   "source": [
    "## Part 3: Finding the Optimal Policy (Value Iteration)\n",
    "\n",
    "So far, we have focused on **policy evaluation** â€” estimating the value of a \n",
    "given policy. In this part, we move to **control**, where the goal is to find \n",
    "the **optimal policy** that maximizes long-term returns.\n",
    "\n",
    "The key idea is to apply **value iteration**, which combines evaluation and \n",
    "improvement into a single update rule:\n",
    "\n",
    "$$\n",
    "V_{k+1}(s) \\;=\\; \\max_a \\Big[ R(s) + \\gamma \\sum_{s'} P(s'|s,a) \\, V_k(s') \\Big].\n",
    "$$\n",
    "\n",
    "Once the value function converges, we extract the optimal policy by choosing, \n",
    "in each state, the action that achieves the maximum:\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\arg\\max_a \\Big[ R(s) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s') \\Big].\n",
    "$$\n",
    "\n",
    "### Steps\n",
    "1. Initialize the value function $V_0(s)$ arbitrarily (e.g., all zeros).  \n",
    "2. Repeatedly update each stateâ€™s value using the **max over actions** rule.  \n",
    "3. Stop when values converge (the updates change very little).  \n",
    "4. Derive the optimal policy $\\pi^*$ by picking the greedy action at each state.  \n",
    "\n",
    "### What to do\n",
    "- Implement value iteration with your transition model `P_all_prob` and rewards.  \n",
    "- Print the **optimal state values**.  \n",
    "- Print the **optimal policy** as action indices (0 = LEFT, 1 = DOWN, 2 = RIGHT, 3 = UP).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97536865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your time to work on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd81ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec7f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
